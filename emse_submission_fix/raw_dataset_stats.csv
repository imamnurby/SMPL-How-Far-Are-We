folder,file,loc_before,loc_after,diff_full,loc_diff_full,num_tokens
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_export_btrfs_encode_fh,35,35," static int btrfs_encode_fh(struct inode *inode, u32 *fh, int *max_len,
 			   struct inode *parent)
 {
 	struct btrfs_fid *fid = (struct btrfs_fid *)fh;
 	int len = *max_len;
 	int type;
 	if (parent && (len < BTRFS_FID_SIZE_CONNECTABLE)) {
 		*max_len = BTRFS_FID_SIZE_CONNECTABLE;
 		return FILEID_INVALID;
 	} else if (len < BTRFS_FID_SIZE_NON_CONNECTABLE) {
 		*max_len = BTRFS_FID_SIZE_NON_CONNECTABLE;
 		return FILEID_INVALID;
 	}
 	len  = BTRFS_FID_SIZE_NON_CONNECTABLE;
 	type = FILEID_BTRFS_WITHOUT_PARENT;
 	fid->objectid = btrfs_ino(BTRFS_I(inode));
-	fid->root_objectid = BTRFS_I(inode)->root->objectid;
+	fid->root_objectid = BTRFS_I(inode)->root->root_key.objectid;
 	fid->gen = inode->i_generation;
 	if (parent) {
 		u64 parent_root_id;
 		fid->parent_objectid = BTRFS_I(parent)->location.objectid;
 		fid->parent_gen = parent->i_generation;
-		parent_root_id = BTRFS_I(parent)->root->objectid;
+		parent_root_id = BTRFS_I(parent)->root->root_key.objectid;
 		if (parent_root_id != fid->root_objectid) {
 			fid->parent_root_objectid = parent_root_id;
 			len = BTRFS_FID_SIZE_CONNECTABLE_ROOT;
 			type = FILEID_BTRFS_WITH_PARENT_ROOT;
 		} else {
 			len = BTRFS_FID_SIZE_CONNECTABLE;
 			type = FILEID_BTRFS_WITH_PARENT;
 		}
 	}
 	*max_len = len;
 	return type;
 }",37,239
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_qgroup_btrfs_qgroup_check_reserved_leak,23,23," void btrfs_qgroup_check_reserved_leak(struct inode *inode)
 {
 	struct extent_changeset changeset;
 	struct ulist_node *unode;
 	struct ulist_iterator iter;
 	int ret;
 	extent_changeset_init(&changeset);
 	ret = clear_record_extent_bits(&BTRFS_I(inode)->io_tree, 0, (u64)-1,
 			EXTENT_QGROUP_RESERVED, &changeset);
 	WARN_ON(ret < 0);
 	if (WARN_ON(changeset.bytes_changed)) {
 		ULIST_ITER_INIT(&iter);
 		while ((unode = ulist_next(&changeset.range_changed, &iter))) {
 			btrfs_warn(BTRFS_I(inode)->root->fs_info,
 				""leaking qgroup reserved space, ino: %lu, start: %llu, end: %llu"",
 				inode->i_ino, unode->val, unode->aux);
 		}
 		btrfs_qgroup_free_refroot(BTRFS_I(inode)->root->fs_info,
-				BTRFS_I(inode)->root->objectid,
+				BTRFS_I(inode)->root->root_key.objectid,
 				changeset.bytes_changed, BTRFS_QGROUP_RSV_DATA);
 	}
 	extent_changeset_release(&changeset);
 }",24,175
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_disk-io___setup_root,63,63," static void __setup_root(struct btrfs_root *root, struct btrfs_fs_info *fs_info,
 			 u64 objectid)
 {
 	bool dummy = test_bit(BTRFS_FS_STATE_DUMMY_FS_INFO, &fs_info->fs_state);
 	root->node = NULL;
 	root->commit_root = NULL;
 	root->state = 0;
 	root->orphan_cleanup_state = 0;
-	root->objectid = objectid;
+	root->root_key.objectid = objectid;
 	root->last_trans = 0;
 	root->highest_objectid = 0;
 	root->nr_delalloc_inodes = 0;
 	root->nr_ordered_extents = 0;
 	root->inode_tree = RB_ROOT;
 	INIT_RADIX_TREE(&root->delayed_nodes_tree, GFP_ATOMIC);
 	root->block_rsv = NULL;
 	INIT_LIST_HEAD(&root->dirty_list);
 	INIT_LIST_HEAD(&root->root_list);
 	INIT_LIST_HEAD(&root->delalloc_inodes);
 	INIT_LIST_HEAD(&root->delalloc_root);
 	INIT_LIST_HEAD(&root->ordered_extents);
 	INIT_LIST_HEAD(&root->ordered_root);
 	INIT_LIST_HEAD(&root->logged_list[0]);
 	INIT_LIST_HEAD(&root->logged_list[1]);
 	spin_lock_init(&root->inode_lock);
 	spin_lock_init(&root->delalloc_lock);
 	spin_lock_init(&root->ordered_extent_lock);
 	spin_lock_init(&root->accounting_lock);
 	spin_lock_init(&root->log_extents_lock[0]);
 	spin_lock_init(&root->log_extents_lock[1]);
 	spin_lock_init(&root->qgroup_meta_rsv_lock);
 	mutex_init(&root->objectid_mutex);
 	mutex_init(&root->log_mutex);
 	mutex_init(&root->ordered_extent_mutex);
 	mutex_init(&root->delalloc_mutex);
 	init_waitqueue_head(&root->log_writer_wait);
 	init_waitqueue_head(&root->log_commit_wait[0]);
 	init_waitqueue_head(&root->log_commit_wait[1]);
 	INIT_LIST_HEAD(&root->log_ctxs[0]);
 	INIT_LIST_HEAD(&root->log_ctxs[1]);
 	atomic_set(&root->log_commit[0], 0);
 	atomic_set(&root->log_commit[1], 0);
 	atomic_set(&root->log_writers, 0);
 	atomic_set(&root->log_batch, 0);
 	refcount_set(&root->refs, 1);
 	atomic_set(&root->will_be_snapshotted, 0);
 	atomic_set(&root->snapshot_force_cow, 0);
 	root->log_transid = 0;
 	root->log_transid_committed = -1;
 	root->last_log_commit = 0;
 	if (!dummy)
 		extent_io_tree_init(&root->dirty_log_pages, NULL);
 	memset(&root->root_key, 0, sizeof(root->root_key));
 	memset(&root->root_item, 0, sizeof(root->root_item));
 	memset(&root->defrag_progress, 0, sizeof(root->defrag_progress));
 	if (!dummy)
 		root->defrag_trans_start = fs_info->generation;
 	else
 		root->defrag_trans_start = 0;
 	root->root_key.objectid = objectid;
 	root->anon_dev = 0;
 	spin_lock_init(&root->root_item_lock);
 }",64,536
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_backref_btrfs_check_shared,58,58," int btrfs_check_shared(struct btrfs_root *root, u64 inum, u64 bytenr)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct btrfs_trans_handle *trans;
 	struct ulist *tmp = NULL;
 	struct ulist *roots = NULL;
 	struct ulist_iterator uiter;
 	struct ulist_node *node;
 	struct seq_list elem = SEQ_LIST_INIT(elem);
 	int ret = 0;
 	struct share_check shared = {
-		.root_objectid = root->objectid,
+		.root_objectid = root->root_key.objectid,
 		.inum = inum,
 		.share_count = 0,
 	};
 	tmp = ulist_alloc(GFP_NOFS);
 	roots = ulist_alloc(GFP_NOFS);
 	if (!tmp || !roots) {
 		ulist_free(tmp);
 		ulist_free(roots);
 		return -ENOMEM;
 	}
 	trans = btrfs_join_transaction(root);
 	if (IS_ERR(trans)) {
 		trans = NULL;
 		down_read(&fs_info->commit_root_sem);
 	} else {
 		btrfs_get_tree_mod_seq(fs_info, &elem);
 	}
 	ULIST_ITER_INIT(&uiter);
 	while (1) {
 		ret = find_parent_nodes(trans, fs_info, bytenr, elem.seq, tmp,
 					roots, NULL, &shared, false);
 		if (ret == BACKREF_FOUND_SHARED) {
 			
 			ret = 1;
 			break;
 		}
 		if (ret < 0 && ret != -ENOENT)
 			break;
 		ret = 0;
 		node = ulist_next(tmp, &uiter);
 		if (!node)
 			break;
 		bytenr = node->val;
 		shared.share_count = 0;
 		cond_resched();
 	}
 	if (trans) {
 		btrfs_put_tree_mod_seq(fs_info, &elem);
 		btrfs_end_transaction(trans);
 	} else {
 		up_read(&fs_info->commit_root_sem);
 	}
 	ulist_free(tmp);
 	ulist_free(roots);
 	return ret;
 }",59,324
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_qgroup___btrfs_qgroup_release_data,29,29," static int __btrfs_qgroup_release_data(struct inode *inode,
 			struct extent_changeset *reserved, u64 start, u64 len,
 			int free)
 {
 	struct extent_changeset changeset;
 	int trace_op = QGROUP_RELEASE;
 	int ret;
 	
 	WARN_ON(!free && reserved);
 	if (free && reserved)
 		return qgroup_free_reserved_data(inode, reserved, start, len);
 	extent_changeset_init(&changeset);
 	ret = clear_record_extent_bits(&BTRFS_I(inode)->io_tree, start, 
 			start + len -1, EXTENT_QGROUP_RESERVED, &changeset);
 	if (ret < 0)
 		goto out;
 	if (free)
 		trace_op = QGROUP_FREE;
 	trace_btrfs_qgroup_release_data(inode, start, len,
 					changeset.bytes_changed, trace_op);
 	if (free)
 		btrfs_qgroup_free_refroot(BTRFS_I(inode)->root->fs_info,
-				BTRFS_I(inode)->root->objectid,
+				BTRFS_I(inode)->root->root_key.objectid,
 				changeset.bytes_changed, BTRFS_QGROUP_RSV_DATA);
 	ret = changeset.bytes_changed;
 out:
 	extent_changeset_release(&changeset);
 	return ret;
 }",30,188
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_transaction_btrfs_clean_one_deleted_snapshot,22,22," int btrfs_clean_one_deleted_snapshot(struct btrfs_root *root)
 {
 	int ret;
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	spin_lock(&fs_info->trans_lock);
 	if (list_empty(&fs_info->dead_roots)) {
 		spin_unlock(&fs_info->trans_lock);
 		return 0;
 	}
 	root = list_first_entry(&fs_info->dead_roots,
 			struct btrfs_root, root_list);
 	list_del_init(&root->root_list);
 	spin_unlock(&fs_info->trans_lock);
-	btrfs_debug(fs_info, ""cleaner removing %llu"", root->objectid);
+	btrfs_debug(fs_info, ""cleaner removing %llu"", root->root_key.objectid);
 	btrfs_kill_all_delayed_nodes(root);
 	if (btrfs_header_backref_rev(root->node) <
 			BTRFS_MIXED_BACKREF_REV)
 		ret = btrfs_drop_snapshot(root, NULL, 0, 0);
 	else
 		ret = btrfs_drop_snapshot(root, NULL, 1, 0);
 	return (ret < 0) ? 0 : 1;
 }",23,168
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_super_btrfs_statfs,72,72," static int btrfs_statfs(struct dentry *dentry, struct kstatfs *buf)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(dentry->d_sb);
 	struct btrfs_super_block *disk_super = fs_info->super_copy;
 	struct list_head *head = &fs_info->space_info;
 	struct btrfs_space_info *found;
 	u64 total_used = 0;
 	u64 total_free_data = 0;
 	u64 total_free_meta = 0;
 	int bits = dentry->d_sb->s_blocksize_bits;
 	__be32 *fsid = (__be32 *)fs_info->fsid;
 	unsigned factor = 1;
 	struct btrfs_block_rsv *block_rsv = &fs_info->global_block_rsv;
 	int ret;
 	u64 thresh = 0;
 	int mixed = 0;
 	rcu_read_lock();
 	list_for_each_entry_rcu(found, head, list) {
 		if (found->flags & BTRFS_BLOCK_GROUP_DATA) {
 			int i;
 			total_free_data += found->disk_total - found->disk_used;
 			total_free_data -=
 				btrfs_account_ro_block_groups_free_space(found);
 			for (i = 0; i < BTRFS_NR_RAID_TYPES; i++) {
 				if (!list_empty(&found->block_groups[i]))
 					factor = btrfs_bg_type_to_factor(
 						btrfs_raid_array[i].bg_flag);
 			}
 		}
 		
 		if (!mixed && found->flags & BTRFS_BLOCK_GROUP_METADATA) {
 			if (found->flags & BTRFS_BLOCK_GROUP_DATA)
 				mixed = 1;
 			else
 				total_free_meta += found->disk_total -
 					found->disk_used;
 		}
 		total_used += found->disk_used;
 	}
 	rcu_read_unlock();
 	buf->f_blocks = div_u64(btrfs_super_total_bytes(disk_super), factor);
 	buf->f_blocks >>= bits;
 	buf->f_bfree = buf->f_blocks - (div_u64(total_used, factor) >> bits);
 	
 	spin_lock(&block_rsv->lock);
 	
 	if (buf->f_bfree >= block_rsv->size >> bits)
 		buf->f_bfree -= block_rsv->size >> bits;
 	else
 		buf->f_bfree = 0;
 	spin_unlock(&block_rsv->lock);
 	buf->f_bavail = div_u64(total_free_data, factor);
 	ret = btrfs_calc_avail_data_space(fs_info, &total_free_data);
 	if (ret)
 		return ret;
 	buf->f_bavail += div_u64(total_free_data, factor);
 	buf->f_bavail = buf->f_bavail >> bits;
 	
 	thresh = SZ_4M;
 	if (!mixed && total_free_meta - thresh < block_rsv->size)
 		buf->f_bavail = 0;
 	buf->f_type = BTRFS_SUPER_MAGIC;
 	buf->f_bsize = dentry->d_sb->s_blocksize;
 	buf->f_namelen = BTRFS_NAME_LEN;
 	
 	buf->f_fsid.val[0] = be32_to_cpu(fsid[0]) ^ be32_to_cpu(fsid[2]);
 	buf->f_fsid.val[1] = be32_to_cpu(fsid[1]) ^ be32_to_cpu(fsid[3]);
 	
-	buf->f_fsid.val[0] ^= BTRFS_I(d_inode(dentry))->root->objectid >> 32;
-	buf->f_fsid.val[1] ^= BTRFS_I(d_inode(dentry))->root->objectid;
+	buf->f_fsid.val[0] ^= BTRFS_I(d_inode(dentry))->root->root_key.objectid >> 32;
+	buf->f_fsid.val[1] ^= BTRFS_I(d_inode(dentry))->root->root_key.objectid;
 	return 0;
 }",74,575
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_qgroup___btrfs_qgroup_reserve_meta,17,17," int __btrfs_qgroup_reserve_meta(struct btrfs_root *root, int num_bytes,
 				enum btrfs_qgroup_rsv_type type, bool enforce)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	int ret;
 	if (!test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags) ||
-	    !is_fstree(root->objectid) || num_bytes == 0)
+	    !is_fstree(root->root_key.objectid) || num_bytes == 0)
 		return 0;
 	BUG_ON(num_bytes != round_down(num_bytes, fs_info->nodesize));
 	trace_qgroup_meta_reserve(root, type, (s64)num_bytes);
 	ret = qgroup_reserve(root, num_bytes, enforce, type);
 	if (ret < 0)
 		return ret;
 	
 	add_root_meta_rsv(root, num_bytes, type);
 	return ret;
 }",18,136
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_send_send_subvol_begin,76,76," static int send_subvol_begin(struct send_ctx *sctx)
 {
 	int ret;
 	struct btrfs_root *send_root = sctx->send_root;
 	struct btrfs_root *parent_root = sctx->parent_root;
 	struct btrfs_path *path;
 	struct btrfs_key key;
 	struct btrfs_root_ref *ref;
 	struct extent_buffer *leaf;
 	char *name = NULL;
 	int namelen;
 	path = btrfs_alloc_path();
 	if (!path)
 		return -ENOMEM;
 	name = kmalloc(BTRFS_PATH_NAME_MAX, GFP_KERNEL);
 	if (!name) {
 		btrfs_free_path(path);
 		return -ENOMEM;
 	}
-	key.objectid = send_root->objectid;
+	key.objectid = send_root->root_key.objectid;
 	key.type = BTRFS_ROOT_BACKREF_KEY;
 	key.offset = 0;
 	ret = btrfs_search_slot_for_read(send_root->fs_info->tree_root,
 				&key, path, 1, 0);
 	if (ret < 0)
 		goto out;
 	if (ret) {
 		ret = -ENOENT;
 		goto out;
 	}
 	leaf = path->nodes[0];
 	btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
 	if (key.type != BTRFS_ROOT_BACKREF_KEY ||
-	    key.objectid != send_root->objectid) {
+	    key.objectid != send_root->root_key.objectid) {
 		ret = -ENOENT;
 		goto out;
 	}
 	ref = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_root_ref);
 	namelen = btrfs_root_ref_name_len(leaf, ref);
 	read_extent_buffer(leaf, name, (unsigned long)(ref + 1), namelen);
 	btrfs_release_path(path);
 	if (parent_root) {
 		ret = begin_cmd(sctx, BTRFS_SEND_C_SNAPSHOT);
 		if (ret < 0)
 			goto out;
 	} else {
 		ret = begin_cmd(sctx, BTRFS_SEND_C_SUBVOL);
 		if (ret < 0)
 			goto out;
 	}
 	TLV_PUT_STRING(sctx, BTRFS_SEND_A_PATH, name, namelen);
 	if (!btrfs_is_empty_uuid(sctx->send_root->root_item.received_uuid))
 		TLV_PUT_UUID(sctx, BTRFS_SEND_A_UUID,
 			    sctx->send_root->root_item.received_uuid);
 	else
 		TLV_PUT_UUID(sctx, BTRFS_SEND_A_UUID,
 			    sctx->send_root->root_item.uuid);
 	TLV_PUT_U64(sctx, BTRFS_SEND_A_CTRANSID,
 		    le64_to_cpu(sctx->send_root->root_item.ctransid));
 	if (parent_root) {
 		if (!btrfs_is_empty_uuid(parent_root->root_item.received_uuid))
 			TLV_PUT_UUID(sctx, BTRFS_SEND_A_CLONE_UUID,
 				     parent_root->root_item.received_uuid);
 		else
 			TLV_PUT_UUID(sctx, BTRFS_SEND_A_CLONE_UUID,
 				     parent_root->root_item.uuid);
 		TLV_PUT_U64(sctx, BTRFS_SEND_A_CLONE_CTRANSID,
 			    le64_to_cpu(sctx->parent_root->root_item.ctransid));
 	}
 	ret = send_cmd(sctx);
 tlv_put_failure:
 out:
 	btrfs_free_path(path);
 	kfree(name);
 	return ret;
 }",78,495
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_relocation_build_backref_tree,423,424," backref_node *build_backref_tree(struct reloc_control *rc,
 					struct btrfs_key *node_key,
 					int level, u64 bytenr)
 {
 	struct backref_cache *cache = &rc->backref_cache;
 	struct btrfs_path *path1;
 	struct btrfs_path *path2;
 	struct extent_buffer *eb;
 	struct btrfs_root *root;
 	struct backref_node *cur;
 	struct backref_node *upper;
 	struct backref_node *lower;
 	struct backref_node *node = NULL;
 	struct backref_node *exist = NULL;
 	struct backref_edge *edge;
 	struct rb_node *rb_node;
 	struct btrfs_key key;
 	unsigned long end;
 	unsigned long ptr;
 	LIST_HEAD(list);
 	LIST_HEAD(useless);
 	int cowonly;
 	int ret;
 	int err = 0;
 	bool need_check = true;
 	path1 = btrfs_alloc_path();
 	path2 = btrfs_alloc_path();
 	if (!path1 || !path2) {
 		err = -ENOMEM;
 		goto out;
 	}
 	path1->reada = READA_FORWARD;
 	path2->reada = READA_FORWARD;
 	node = alloc_backref_node(cache);
 	if (!node) {
 		err = -ENOMEM;
 		goto out;
 	}
 	node->bytenr = bytenr;
 	node->level = level;
 	node->lowest = 1;
 	cur = node;
 again:
 	end = 0;
 	ptr = 0;
 	key.objectid = cur->bytenr;
 	key.type = BTRFS_METADATA_ITEM_KEY;
 	key.offset = (u64)-1;
 	path1->search_commit_root = 1;
 	path1->skip_locking = 1;
 	ret = btrfs_search_slot(NULL, rc->extent_root, &key, path1,
 				0, 0);
 	if (ret < 0) {
 		err = ret;
 		goto out;
 	}
 	ASSERT(ret);
 	ASSERT(path1->slots[0]);
 	path1->slots[0]--;
 	WARN_ON(cur->checked);
 	if (!list_empty(&cur->upper)) {
 		
 		ASSERT(list_is_singular(&cur->upper));
 		edge = list_entry(cur->upper.next, struct backref_edge,
 				  list[LOWER]);
 		ASSERT(list_empty(&edge->list[UPPER]));
 		exist = edge->node[UPPER];
 		
 		if (!exist->checked)
 			list_add_tail(&edge->list[UPPER], &list);
 	} else {
 		exist = NULL;
 	}
 	while (1) {
 		cond_resched();
 		eb = path1->nodes[0];
 		if (ptr >= end) {
 			if (path1->slots[0] >= btrfs_header_nritems(eb)) {
 				ret = btrfs_next_leaf(rc->extent_root, path1);
 				if (ret < 0) {
 					err = ret;
 					goto out;
 				}
 				if (ret > 0)
 					break;
 				eb = path1->nodes[0];
 			}
 			btrfs_item_key_to_cpu(eb, &key, path1->slots[0]);
 			if (key.objectid != cur->bytenr) {
 				WARN_ON(exist);
 				break;
 			}
 			if (key.type == BTRFS_EXTENT_ITEM_KEY ||
 			    key.type == BTRFS_METADATA_ITEM_KEY) {
 				ret = find_inline_backref(eb, path1->slots[0],
 							  &ptr, &end);
 				if (ret)
 					goto next;
 			}
 		}
 		if (ptr < end) {
 			
 			struct btrfs_extent_inline_ref *iref;
 			int type;
 			iref = (struct btrfs_extent_inline_ref *)ptr;
 			type = btrfs_get_extent_inline_ref_type(eb, iref,
 							BTRFS_REF_TYPE_BLOCK);
 			if (type == BTRFS_REF_TYPE_INVALID) {
 				err = -EUCLEAN;
 				goto out;
 			}
 			key.type = type;
 			key.offset = btrfs_extent_inline_ref_offset(eb, iref);
 			WARN_ON(key.type != BTRFS_TREE_BLOCK_REF_KEY &&
 				key.type != BTRFS_SHARED_BLOCK_REF_KEY);
 		}
 		if (exist &&
 		    ((key.type == BTRFS_TREE_BLOCK_REF_KEY &&
 		      exist->owner == key.offset) ||
 		     (key.type == BTRFS_SHARED_BLOCK_REF_KEY &&
 		      exist->bytenr == key.offset))) {
 			exist = NULL;
 			goto next;
 		}
 		if (key.type == BTRFS_SHARED_BLOCK_REF_KEY) {
 			if (key.objectid == key.offset) {
 				
 				root = find_reloc_root(rc, cur->bytenr);
 				ASSERT(root);
 				cur->root = root;
 				break;
 			}
 			edge = alloc_backref_edge(cache);
 			if (!edge) {
 				err = -ENOMEM;
 				goto out;
 			}
 			rb_node = tree_search(&cache->rb_root, key.offset);
 			if (!rb_node) {
 				upper = alloc_backref_node(cache);
 				if (!upper) {
 					free_backref_edge(cache, edge);
 					err = -ENOMEM;
 					goto out;
 				}
 				upper->bytenr = key.offset;
 				upper->level = cur->level + 1;
 				
 				list_add_tail(&edge->list[UPPER], &list);
 			} else {
 				upper = rb_entry(rb_node, struct backref_node,
 						 rb_node);
 				ASSERT(upper->checked);
 				INIT_LIST_HEAD(&edge->list[UPPER]);
 			}
 			list_add_tail(&edge->list[LOWER], &cur->upper);
 			edge->node[LOWER] = cur;
 			edge->node[UPPER] = upper;
 			goto next;
 		} else if (unlikely(key.type == BTRFS_EXTENT_REF_V0_KEY)) {
 			err = -EINVAL;
 			btrfs_print_v0_err(rc->extent_root->fs_info);
 			btrfs_handle_fs_error(rc->extent_root->fs_info, err,
 					      NULL);
 			goto out;
 		} else if (key.type != BTRFS_TREE_BLOCK_REF_KEY) {
 			goto next;
 		}
 		
 		root = read_fs_root(rc->extent_root->fs_info, key.offset);
 		if (IS_ERR(root)) {
 			err = PTR_ERR(root);
 			goto out;
 		}
 		if (!test_bit(BTRFS_ROOT_REF_COWS, &root->state))
 			cur->cowonly = 1;
 		if (btrfs_root_level(&root->root_item) == cur->level) {
 			
 			ASSERT(btrfs_root_bytenr(&root->root_item) ==
 			       cur->bytenr);
 			if (should_ignore_root(root))
 				list_add(&cur->list, &useless);
 			else
 				cur->root = root;
 			break;
 		}
 		level = cur->level + 1;
 		
 		path2->search_commit_root = 1;
 		path2->skip_locking = 1;
 		path2->lowest_level = level;
 		ret = btrfs_search_slot(NULL, root, node_key, path2, 0, 0);
 		path2->lowest_level = 0;
 		if (ret < 0) {
 			err = ret;
 			goto out;
 		}
 		if (ret > 0 && path2->slots[level] > 0)
 			path2->slots[level]--;
 		eb = path2->nodes[level];
 		if (btrfs_node_blockptr(eb, path2->slots[level]) !=
 		    cur->bytenr) {
 			btrfs_err(root->fs_info,
 	""couldn't find block (%llu) (level %d) in tree (%llu) with key (%llu %u %llu)"",
-				  cur->bytenr, level - 1, root->objectid,
+				  cur->bytenr, level - 1,
+				  root->root_key.objectid,
 				  node_key->objectid, node_key->type,
 				  node_key->offset);
 			err = -ENOENT;
 			goto out;
 		}
 		lower = cur;
 		need_check = true;
 		for (; level < BTRFS_MAX_LEVEL; level++) {
 			if (!path2->nodes[level]) {
 				ASSERT(btrfs_root_bytenr(&root->root_item) ==
 				       lower->bytenr);
 				if (should_ignore_root(root))
 					list_add(&lower->list, &useless);
 				else
 					lower->root = root;
 				break;
 			}
 			edge = alloc_backref_edge(cache);
 			if (!edge) {
 				err = -ENOMEM;
 				goto out;
 			}
 			eb = path2->nodes[level];
 			rb_node = tree_search(&cache->rb_root, eb->start);
 			if (!rb_node) {
 				upper = alloc_backref_node(cache);
 				if (!upper) {
 					free_backref_edge(cache, edge);
 					err = -ENOMEM;
 					goto out;
 				}
 				upper->bytenr = eb->start;
 				upper->owner = btrfs_header_owner(eb);
 				upper->level = lower->level + 1;
 				if (!test_bit(BTRFS_ROOT_REF_COWS,
 					      &root->state))
 					upper->cowonly = 1;
 				
 				if (btrfs_block_can_be_shared(root, eb))
 					upper->checked = 0;
 				else
 					upper->checked = 1;
 				
 				if (!upper->checked && need_check) {
 					need_check = false;
 					list_add_tail(&edge->list[UPPER],
 						      &list);
 				} else {
 					if (upper->checked)
 						need_check = true;
 					INIT_LIST_HEAD(&edge->list[UPPER]);
 				}
 			} else {
 				upper = rb_entry(rb_node, struct backref_node,
 						 rb_node);
 				ASSERT(upper->checked);
 				INIT_LIST_HEAD(&edge->list[UPPER]);
 				if (!upper->owner)
 					upper->owner = btrfs_header_owner(eb);
 			}
 			list_add_tail(&edge->list[LOWER], &lower->upper);
 			edge->node[LOWER] = lower;
 			edge->node[UPPER] = upper;
 			if (rb_node)
 				break;
 			lower = upper;
 			upper = NULL;
 		}
 		btrfs_release_path(path2);
 next:
 		if (ptr < end) {
 			ptr += btrfs_extent_inline_ref_size(key.type);
 			if (ptr >= end) {
 				WARN_ON(ptr > end);
 				ptr = 0;
 				end = 0;
 			}
 		}
 		if (ptr >= end)
 			path1->slots[0]++;
 	}
 	btrfs_release_path(path1);
 	cur->checked = 1;
 	WARN_ON(exist);
 	
 	if (!list_empty(&list)) {
 		edge = list_entry(list.next, struct backref_edge, list[UPPER]);
 		list_del_init(&edge->list[UPPER]);
 		cur = edge->node[UPPER];
 		goto again;
 	}
 	
 	ASSERT(node->checked);
 	cowonly = node->cowonly;
 	if (!cowonly) {
 		rb_node = tree_insert(&cache->rb_root, node->bytenr,
 				      &node->rb_node);
 		if (rb_node)
 			backref_tree_panic(rb_node, -EEXIST, node->bytenr);
 		list_add_tail(&node->lower, &cache->leaves);
 	}
 	list_for_each_entry(edge, &node->upper, list[LOWER])
 		list_add_tail(&edge->list[UPPER], &list);
 	while (!list_empty(&list)) {
 		edge = list_entry(list.next, struct backref_edge, list[UPPER]);
 		list_del_init(&edge->list[UPPER]);
 		upper = edge->node[UPPER];
 		if (upper->detached) {
 			list_del(&edge->list[LOWER]);
 			lower = edge->node[LOWER];
 			free_backref_edge(cache, edge);
 			if (list_empty(&lower->upper))
 				list_add(&lower->list, &useless);
 			continue;
 		}
 		if (!RB_EMPTY_NODE(&upper->rb_node)) {
 			if (upper->lowest) {
 				list_del_init(&upper->lower);
 				upper->lowest = 0;
 			}
 			list_add_tail(&edge->list[UPPER], &upper->lower);
 			continue;
 		}
 		if (!upper->checked) {
 			
 			ASSERT(0);
 			err = -EINVAL;
 			goto out;
 		}
 		if (cowonly != upper->cowonly) {
 			ASSERT(0);
 			err = -EINVAL;
 			goto out;
 		}
 		if (!cowonly) {
 			rb_node = tree_insert(&cache->rb_root, upper->bytenr,
 					      &upper->rb_node);
 			if (rb_node)
 				backref_tree_panic(rb_node, -EEXIST,
 						   upper->bytenr);
 		}
 		list_add_tail(&edge->list[UPPER], &upper->lower);
 		list_for_each_entry(edge, &upper->upper, list[LOWER])
 			list_add_tail(&edge->list[UPPER], &list);
 	}
 	
 	while (!list_empty(&useless)) {
 		upper = list_entry(useless.next, struct backref_node, list);
 		list_del_init(&upper->list);
 		ASSERT(list_empty(&upper->upper));
 		if (upper == node)
 			node = NULL;
 		if (upper->lowest) {
 			list_del_init(&upper->lower);
 			upper->lowest = 0;
 		}
 		while (!list_empty(&upper->lower)) {
 			edge = list_entry(upper->lower.next,
 					  struct backref_edge, list[UPPER]);
 			list_del(&edge->list[UPPER]);
 			list_del(&edge->list[LOWER]);
 			lower = edge->node[LOWER];
 			free_backref_edge(cache, edge);
 			if (list_empty(&lower->upper))
 				list_add(&lower->list, &useless);
 		}
 		__mark_block_processed(rc, upper);
 		if (upper->level > 0) {
 			list_add(&upper->list, &cache->detached);
 			upper->detached = 1;
 		} else {
 			rb_erase(&upper->rb_node, &cache->rb_root);
 			free_backref_node(cache, upper);
 		}
 	}
 out:
 	btrfs_free_path(path1);
 	btrfs_free_path(path2);
 	if (err) {
 		while (!list_empty(&useless)) {
 			lower = list_entry(useless.next,
 					   struct backref_node, list);
 			list_del_init(&lower->list);
 		}
 		while (!list_empty(&list)) {
 			edge = list_first_entry(&list, struct backref_edge,
 						list[UPPER]);
 			list_del(&edge->list[UPPER]);
 			list_del(&edge->list[LOWER]);
 			lower = edge->node[LOWER];
 			upper = edge->node[UPPER];
 			free_backref_edge(cache, edge);
 			
 			if (list_empty(&lower->upper) &&
 			    RB_EMPTY_NODE(&lower->rb_node))
 				list_add(&lower->list, &useless);
 			if (!RB_EMPTY_NODE(&upper->rb_node))
 				continue;
 			
 			list_for_each_entry(edge, &upper->upper, list[LOWER])
 				list_add_tail(&edge->list[UPPER], &list);
 			if (list_empty(&upper->upper))
 				list_add(&upper->list, &useless);
 		}
 		while (!list_empty(&useless)) {
 			lower = list_entry(useless.next,
 					   struct backref_node, list);
 			list_del_init(&lower->list);
 			if (lower == node)
 				node = NULL;
 			free_backref_node(cache, lower);
 		}
 		free_backref_node(cache, node);
 		return ERR_PTR(err);
 	}
 	ASSERT(!node || !node->detached);
 	return node;
 }",425,2688
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_qgroup_btrfs_qgroup_free_meta_all_pertrans,12,12," void btrfs_qgroup_free_meta_all_pertrans(struct btrfs_root *root)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	if (!test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags) ||
-	    !is_fstree(root->objectid))
+	    !is_fstree(root->root_key.objectid))
 		return;
 	
 	trace_qgroup_meta_free_all_pertrans(root);
 	
-	btrfs_qgroup_free_refroot(fs_info, root->objectid, (u64)-1,
+	btrfs_qgroup_free_refroot(fs_info, root->root_key.objectid, (u64)-1,
 				  BTRFS_QGROUP_RSV_META_PERTRANS);
 }",14,95
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_qgroup_qgroup_free_reserved_data,42,43," static int qgroup_free_reserved_data(struct inode *inode,
 			struct extent_changeset *reserved, u64 start, u64 len)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct ulist_node *unode;
 	struct ulist_iterator uiter;
 	struct extent_changeset changeset;
 	int freed = 0;
 	int ret;
 	extent_changeset_init(&changeset);
 	len = round_up(start + len, root->fs_info->sectorsize);
 	start = round_down(start, root->fs_info->sectorsize);
 	ULIST_ITER_INIT(&uiter);
 	while ((unode = ulist_next(&reserved->range_changed, &uiter))) {
 		u64 range_start = unode->val;
 		
 		u64 range_len = unode->aux - range_start + 1;
 		u64 free_start;
 		u64 free_len;
 		extent_changeset_release(&changeset);
 		
 		if (range_start >= start + len ||
 		    range_start + range_len <= start)
 			continue;
 		free_start = max(range_start, start);
 		free_len = min(start + len, range_start + range_len) -
 			   free_start;
 		
 		ret = clear_record_extent_bits(&BTRFS_I(inode)->io_failure_tree,
 				free_start, free_start + free_len - 1,
 				EXTENT_QGROUP_RESERVED, &changeset);
 		if (ret < 0)
 			goto out;
 		freed += changeset.bytes_changed;
 	}
-	btrfs_qgroup_free_refroot(root->fs_info, root->objectid, freed,
+	btrfs_qgroup_free_refroot(root->fs_info, root->root_key.objectid,
+				  freed,
 				  BTRFS_QGROUP_RSV_DATA);
 	ret = freed;
 out:
 	extent_changeset_release(&changeset);
 	return ret;
 }",44,273
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_qgroup_btrfs_qgroup_convert_reserved_meta,12,12," void btrfs_qgroup_convert_reserved_meta(struct btrfs_root *root, int num_bytes)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	if (!test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags) ||
-	    !is_fstree(root->objectid))
+	    !is_fstree(root->root_key.objectid))
 		return;
 	
 	num_bytes = sub_root_meta_rsv(root, num_bytes,
 				      BTRFS_QGROUP_RSV_META_PREALLOC);
 	trace_qgroup_meta_convert(root, num_bytes);
-	qgroup_convert_meta(fs_info, root->objectid, num_bytes);
+	qgroup_convert_meta(fs_info, root->root_key.objectid, num_bytes);
 }",14,101
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_ioctl_btrfs_ioctl_default_subvol,70,70," static long btrfs_ioctl_default_subvol(struct file *file, void __user *argp)
 {
 	struct inode *inode = file_inode(file);
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_root *new_root;
 	struct btrfs_dir_item *di;
 	struct btrfs_trans_handle *trans;
 	struct btrfs_path *path;
 	struct btrfs_key location;
 	struct btrfs_disk_key disk_key;
 	u64 objectid = 0;
 	u64 dir_id;
 	int ret;
 	if (!capable(CAP_SYS_ADMIN))
 		return -EPERM;
 	ret = mnt_want_write_file(file);
 	if (ret)
 		return ret;
 	if (copy_from_user(&objectid, argp, sizeof(objectid))) {
 		ret = -EFAULT;
 		goto out;
 	}
 	if (!objectid)
 		objectid = BTRFS_FS_TREE_OBJECTID;
 	location.objectid = objectid;
 	location.type = BTRFS_ROOT_ITEM_KEY;
 	location.offset = (u64)-1;
 	new_root = btrfs_read_fs_root_no_name(fs_info, &location);
 	if (IS_ERR(new_root)) {
 		ret = PTR_ERR(new_root);
 		goto out;
 	}
-	if (!is_fstree(new_root->objectid)) {
+	if (!is_fstree(new_root->root_key.objectid)) {
 		ret = -ENOENT;
 		goto out;
 	}
 	path = btrfs_alloc_path();
 	if (!path) {
 		ret = -ENOMEM;
 		goto out;
 	}
 	path->leave_spinning = 1;
 	trans = btrfs_start_transaction(root, 1);
 	if (IS_ERR(trans)) {
 		btrfs_free_path(path);
 		ret = PTR_ERR(trans);
 		goto out;
 	}
 	dir_id = btrfs_super_root_dir(fs_info->super_copy);
 	di = btrfs_lookup_dir_item(trans, fs_info->tree_root, path,
 				   dir_id, ""default"", 7, 1);
 	if (IS_ERR_OR_NULL(di)) {
 		btrfs_free_path(path);
 		btrfs_end_transaction(trans);
 		btrfs_err(fs_info,
 			  ""Umm, you don't have the default diritem, this isn't going to work"");
 		ret = -ENOENT;
 		goto out;
 	}
 	btrfs_cpu_key_to_disk(&disk_key, &new_root->root_key);
 	btrfs_set_dir_item_key(path->nodes[0], di, &disk_key);
 	btrfs_mark_buffer_dirty(path->nodes[0]);
 	btrfs_free_path(path);
 	btrfs_set_fs_incompat(fs_info, DEFAULT_SUBVOL);
 	btrfs_end_transaction(trans);
 out:
 	mnt_drop_write_file(file);
 	return ret;
 }",71,427
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_ref-verify_btrfs_ref_tree_mod,167,167," int btrfs_ref_tree_mod(struct btrfs_root *root, u64 bytenr, u64 num_bytes,
 		       u64 parent, u64 ref_root, u64 owner, u64 offset,
 		       int action)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct ref_entry *ref = NULL, *exist;
 	struct ref_action *ra = NULL;
 	struct block_entry *be = NULL;
 	struct root_entry *re = NULL;
 	int ret = 0;
 	bool metadata = owner < BTRFS_FIRST_FREE_OBJECTID;
 	if (!btrfs_test_opt(root->fs_info, REF_VERIFY))
 		return 0;
 	ref = kzalloc(sizeof(struct ref_entry), GFP_NOFS);
 	ra = kmalloc(sizeof(struct ref_action), GFP_NOFS);
 	if (!ra || !ref) {
 		kfree(ref);
 		kfree(ra);
 		ret = -ENOMEM;
 		goto out;
 	}
 	if (parent) {
 		ref->parent = parent;
 	} else {
 		ref->root_objectid = ref_root;
 		ref->owner = owner;
 		ref->offset = offset;
 	}
 	ref->num_refs = (action == BTRFS_DROP_DELAYED_REF) ? -1 : 1;
 	memcpy(&ra->ref, ref, sizeof(struct ref_entry));
 	
 	ra->ref.owner = owner;
 	ra->ref.offset = offset;
 	ra->ref.root_objectid = ref_root;
 	__save_stack_trace(ra);
 	INIT_LIST_HEAD(&ra->list);
 	ra->action = action;
-	ra->root = root->objectid;
+	ra->root = root->root_key.objectid;
 	
 	ret = -EINVAL;
 	if (action == BTRFS_ADD_DELAYED_EXTENT) {
 		
 		be = add_block_entry(root->fs_info, bytenr, num_bytes, ref_root);
 		if (IS_ERR(be)) {
 			kfree(ra);
 			ret = PTR_ERR(be);
 			goto out;
 		}
 		be->num_refs++;
 		if (metadata)
 			be->metadata = 1;
 		if (be->num_refs != 1) {
 			btrfs_err(fs_info,
 			""re-allocated a block that still has references to it!"");
 			dump_block_entry(fs_info, be);
 			dump_ref_action(fs_info, ra);
 			goto out_unlock;
 		}
 		while (!list_empty(&be->actions)) {
 			struct ref_action *tmp;
 			tmp = list_first_entry(&be->actions, struct ref_action,
 					       list);
 			list_del(&tmp->list);
 			kfree(tmp);
 		}
 	} else {
 		struct root_entry *tmp;
 		if (!parent) {
 			re = kmalloc(sizeof(struct root_entry), GFP_NOFS);
 			if (!re) {
 				kfree(ref);
 				kfree(ra);
 				ret = -ENOMEM;
 				goto out;
 			}
 			
-			ref_root = root->objectid;
-			re->root_objectid = root->objectid;
+			ref_root = root->root_key.objectid;
+			re->root_objectid = root->root_key.objectid;
 			re->num_refs = 0;
 		}
 		spin_lock(&root->fs_info->ref_verify_lock);
 		be = lookup_block_entry(&root->fs_info->block_tree, bytenr);
 		if (!be) {
 			btrfs_err(fs_info,
 ""trying to do action %d to bytenr %llu num_bytes %llu but there is no existing entry!"",
 				  action, (unsigned long long)bytenr,
 				  (unsigned long long)num_bytes);
 			dump_ref_action(fs_info, ra);
 			kfree(ref);
 			kfree(ra);
 			goto out_unlock;
 		}
 		if (!parent) {
 			tmp = insert_root_entry(&be->roots, re);
 			if (tmp) {
 				kfree(re);
 				re = tmp;
 			}
 		}
 	}
 	exist = insert_ref_entry(&be->refs, ref);
 	if (exist) {
 		if (action == BTRFS_DROP_DELAYED_REF) {
 			if (exist->num_refs == 0) {
 				btrfs_err(fs_info,
 ""dropping a ref for a existing root that doesn't have a ref on the block"");
 				dump_block_entry(fs_info, be);
 				dump_ref_action(fs_info, ra);
 				kfree(ra);
 				goto out_unlock;
 			}
 			exist->num_refs--;
 			if (exist->num_refs == 0) {
 				rb_erase(&exist->node, &be->refs);
 				kfree(exist);
 			}
 		} else if (!be->metadata) {
 			exist->num_refs++;
 		} else {
 			btrfs_err(fs_info,
 ""attempting to add another ref for an existing ref on a tree block"");
 			dump_block_entry(fs_info, be);
 			dump_ref_action(fs_info, ra);
 			kfree(ra);
 			goto out_unlock;
 		}
 		kfree(ref);
 	} else {
 		if (action == BTRFS_DROP_DELAYED_REF) {
 			btrfs_err(fs_info,
 ""dropping a ref for a root that doesn't have a ref on the block"");
 			dump_block_entry(fs_info, be);
 			dump_ref_action(fs_info, ra);
 			kfree(ra);
 			goto out_unlock;
 		}
 	}
 	if (!parent && !re) {
 		re = lookup_root_entry(&be->roots, ref_root);
 		if (!re) {
 			
 			btrfs_err(fs_info, ""failed to find root %llu for %llu"",
-				  root->objectid, be->bytenr);
+				  root->root_key.objectid, be->bytenr);
 			dump_block_entry(fs_info, be);
 			dump_ref_action(fs_info, ra);
 			kfree(ra);
 			goto out_unlock;
 		}
 	}
 	if (action == BTRFS_DROP_DELAYED_REF) {
 		if (re)
 			re->num_refs--;
 		be->num_refs--;
 	} else if (action == BTRFS_ADD_DELAYED_REF) {
 		be->num_refs++;
 		if (re)
 			re->num_refs++;
 	}
 	list_add_tail(&ra->list, &be->actions);
 	ret = 0;
 out_unlock:
 	spin_unlock(&root->fs_info->ref_verify_lock);
 out:
 	if (ret)
 		btrfs_clear_opt(fs_info->mount_opt, REF_VERIFY);
 	return ret;
 }",171,1001
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_extent-tree_btrfs_drop_snapshot,178,178," int btrfs_drop_snapshot(struct btrfs_root *root,
 			 struct btrfs_block_rsv *block_rsv, int update_ref,
 			 int for_reloc)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct btrfs_path *path;
 	struct btrfs_trans_handle *trans;
 	struct btrfs_root *tree_root = fs_info->tree_root;
 	struct btrfs_root_item *root_item = &root->root_item;
 	struct walk_control *wc;
 	struct btrfs_key key;
 	int err = 0;
 	int ret;
 	int level;
 	bool root_dropped = false;
-	btrfs_debug(fs_info, ""Drop subvolume %llu"", root->objectid);
+	btrfs_debug(fs_info, ""Drop subvolume %llu"", root->root_key.objectid);
 	path = btrfs_alloc_path();
 	if (!path) {
 		err = -ENOMEM;
 		goto out;
 	}
 	wc = kzalloc(sizeof(*wc), GFP_NOFS);
 	if (!wc) {
 		btrfs_free_path(path);
 		err = -ENOMEM;
 		goto out;
 	}
 	trans = btrfs_start_transaction(tree_root, 0);
 	if (IS_ERR(trans)) {
 		err = PTR_ERR(trans);
 		goto out_free;
 	}
 	if (block_rsv)
 		trans->block_rsv = block_rsv;
 	if (btrfs_disk_key_objectid(&root_item->drop_progress) == 0) {
 		level = btrfs_header_level(root->node);
 		path->nodes[level] = btrfs_lock_root_node(root);
 		btrfs_set_lock_blocking(path->nodes[level]);
 		path->slots[level] = 0;
 		path->locks[level] = BTRFS_WRITE_LOCK_BLOCKING;
 		memset(&wc->update_progress, 0,
 		       sizeof(wc->update_progress));
 	} else {
 		btrfs_disk_key_to_cpu(&key, &root_item->drop_progress);
 		memcpy(&wc->update_progress, &key,
 		       sizeof(wc->update_progress));
 		level = root_item->drop_level;
 		BUG_ON(level == 0);
 		path->lowest_level = level;
 		ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
 		path->lowest_level = 0;
 		if (ret < 0) {
 			err = ret;
 			goto out_end_trans;
 		}
 		WARN_ON(ret > 0);
 		
 		btrfs_unlock_up_safe(path, 0);
 		level = btrfs_header_level(root->node);
 		while (1) {
 			btrfs_tree_lock(path->nodes[level]);
 			btrfs_set_lock_blocking(path->nodes[level]);
 			path->locks[level] = BTRFS_WRITE_LOCK_BLOCKING;
 			ret = btrfs_lookup_extent_info(trans, fs_info,
 						path->nodes[level]->start,
 						level, 1, &wc->refs[level],
 						&wc->flags[level]);
 			if (ret < 0) {
 				err = ret;
 				goto out_end_trans;
 			}
 			BUG_ON(wc->refs[level] == 0);
 			if (level == root_item->drop_level)
 				break;
 			btrfs_tree_unlock(path->nodes[level]);
 			path->locks[level] = 0;
 			WARN_ON(wc->refs[level] != 1);
 			level--;
 		}
 	}
 	wc->level = level;
 	wc->shared_level = -1;
 	wc->stage = DROP_REFERENCE;
 	wc->update_ref = update_ref;
 	wc->keep_locks = 0;
 	wc->reada_count = BTRFS_NODEPTRS_PER_BLOCK(fs_info);
 	while (1) {
 		ret = walk_down_tree(trans, root, path, wc);
 		if (ret < 0) {
 			err = ret;
 			break;
 		}
 		ret = walk_up_tree(trans, root, path, wc, BTRFS_MAX_LEVEL);
 		if (ret < 0) {
 			err = ret;
 			break;
 		}
 		if (ret > 0) {
 			BUG_ON(wc->stage != DROP_REFERENCE);
 			break;
 		}
 		if (wc->stage == DROP_REFERENCE) {
 			level = wc->level;
 			btrfs_node_key(path->nodes[level],
 				       &root_item->drop_progress,
 				       path->slots[level]);
 			root_item->drop_level = level;
 		}
 		BUG_ON(wc->level == 0);
 		if (btrfs_should_end_transaction(trans) ||
 		    (!for_reloc && btrfs_need_cleaner_sleep(fs_info))) {
 			ret = btrfs_update_root(trans, tree_root,
 						&root->root_key,
 						root_item);
 			if (ret) {
 				btrfs_abort_transaction(trans, ret);
 				err = ret;
 				goto out_end_trans;
 			}
 			btrfs_end_transaction_throttle(trans);
 			if (!for_reloc && btrfs_need_cleaner_sleep(fs_info)) {
 				btrfs_debug(fs_info,
 					    ""drop snapshot early exit"");
 				err = -EAGAIN;
 				goto out_free;
 			}
 			trans = btrfs_start_transaction(tree_root, 0);
 			if (IS_ERR(trans)) {
 				err = PTR_ERR(trans);
 				goto out_free;
 			}
 			if (block_rsv)
 				trans->block_rsv = block_rsv;
 		}
 	}
 	btrfs_release_path(path);
 	if (err)
 		goto out_end_trans;
 	ret = btrfs_del_root(trans, &root->root_key);
 	if (ret) {
 		btrfs_abort_transaction(trans, ret);
 		err = ret;
 		goto out_end_trans;
 	}
 	if (root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID) {
 		ret = btrfs_find_root(tree_root, &root->root_key, path,
 				      NULL, NULL);
 		if (ret < 0) {
 			btrfs_abort_transaction(trans, ret);
 			err = ret;
 			goto out_end_trans;
 		} else if (ret > 0) {
 			
 			btrfs_del_orphan_item(trans, tree_root,
 					      root->root_key.objectid);
 		}
 	}
 	if (test_bit(BTRFS_ROOT_IN_RADIX, &root->state)) {
 		btrfs_add_dropped_root(trans, root);
 	} else {
 		free_extent_buffer(root->node);
 		free_extent_buffer(root->commit_root);
 		btrfs_put_fs_root(root);
 	}
 	root_dropped = true;
 out_end_trans:
 	btrfs_end_transaction_throttle(trans);
 out_free:
 	kfree(wc);
 	btrfs_free_path(path);
 out:
 	
 	if (!for_reloc && !root_dropped)
 		btrfs_add_dead_root(root);
 	if (err && err != -EAGAIN)
 		btrfs_handle_fs_error(fs_info, err, NULL);
 	return err;
 }",179,1063
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_backref_iterate_inode_refs,58,59," static int iterate_inode_refs(u64 inum, struct btrfs_root *fs_root,
 			      struct btrfs_path *path,
 			      iterate_irefs_t *iterate, void *ctx)
 {
 	int ret = 0;
 	int slot;
 	u32 cur;
 	u32 len;
 	u32 name_len;
 	u64 parent = 0;
 	int found = 0;
 	struct extent_buffer *eb;
 	struct btrfs_item *item;
 	struct btrfs_inode_ref *iref;
 	struct btrfs_key found_key;
 	while (!ret) {
 		ret = btrfs_find_item(fs_root, path, inum,
 				parent ? parent + 1 : 0, BTRFS_INODE_REF_KEY,
 				&found_key);
 		if (ret < 0)
 			break;
 		if (ret) {
 			ret = found ? 0 : -ENOENT;
 			break;
 		}
 		++found;
 		parent = found_key.offset;
 		slot = path->slots[0];
 		eb = btrfs_clone_extent_buffer(path->nodes[0]);
 		if (!eb) {
 			ret = -ENOMEM;
 			break;
 		}
 		extent_buffer_get(eb);
 		btrfs_tree_read_lock(eb);
 		btrfs_set_lock_blocking_rw(eb, BTRFS_READ_LOCK);
 		btrfs_release_path(path);
 		item = btrfs_item_nr(slot);
 		iref = btrfs_item_ptr(eb, slot, struct btrfs_inode_ref);
 		for (cur = 0; cur < btrfs_item_size(eb, item); cur += len) {
 			name_len = btrfs_inode_ref_name_len(eb, iref);
 			
 			btrfs_debug(fs_root->fs_info,
 				""following ref at offset %u for inode %llu in tree %llu"",
-				cur, found_key.objectid, fs_root->objectid);
+				cur, found_key.objectid,
+				fs_root->root_key.objectid);
 			ret = iterate(parent, name_len,
 				      (unsigned long)(iref + 1), eb, ctx);
 			if (ret)
 				break;
 			len = sizeof(*iref) + name_len;
 			iref = (struct btrfs_inode_ref *)((char *)iref + len);
 		}
 		btrfs_tree_read_unlock_blocking(eb);
 		free_extent_buffer(eb);
 	}
 	btrfs_release_path(path);
 	return ret;
 }",60,355
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_ctree_add_root_to_dirty_list,18,18," static void add_root_to_dirty_list(struct btrfs_root *root)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	if (test_bit(BTRFS_ROOT_DIRTY, &root->state) ||
 	    !test_bit(BTRFS_ROOT_TRACK_DIRTY, &root->state))
 		return;
 	spin_lock(&fs_info->trans_lock);
 	if (!test_and_set_bit(BTRFS_ROOT_DIRTY, &root->state)) {
 		
-		if (root->objectid == BTRFS_EXTENT_TREE_OBJECTID)
+		if (root->root_key.objectid == BTRFS_EXTENT_TREE_OBJECTID)
 			list_move_tail(&root->dirty_list,
 				       &fs_info->dirty_cowonly_roots);
 		else
 			list_move(&root->dirty_list,
 				  &fs_info->dirty_cowonly_roots);
 	}
 	spin_unlock(&fs_info->trans_lock);
 }",19,124
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_qgroup_btrfs_qgroup_reserve_data,46,46," int btrfs_qgroup_reserve_data(struct inode *inode,
 			struct extent_changeset **reserved_ret, u64 start,
 			u64 len)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct ulist_node *unode;
 	struct ulist_iterator uiter;
 	struct extent_changeset *reserved;
 	u64 orig_reserved;
 	u64 to_reserve;
 	int ret;
 	if (!test_bit(BTRFS_FS_QUOTA_ENABLED, &root->fs_info->flags) ||
-	    !is_fstree(root->objectid) || len == 0)
+	    !is_fstree(root->root_key.objectid) || len == 0)
 		return 0;
 	
 	if (WARN_ON(!reserved_ret))
 		return -EINVAL;
 	if (!*reserved_ret) {
 		*reserved_ret = extent_changeset_alloc();
 		if (!*reserved_ret)
 			return -ENOMEM;
 	}
 	reserved = *reserved_ret;
 	
 	orig_reserved = reserved->bytes_changed;
 	ret = set_record_extent_bits(&BTRFS_I(inode)->io_tree, start,
 			start + len -1, EXTENT_QGROUP_RESERVED, reserved);
 	
 	to_reserve = reserved->bytes_changed - orig_reserved;
 	trace_btrfs_qgroup_reserve_data(inode, start, len,
 					to_reserve, QGROUP_RESERVE);
 	if (ret < 0)
 		goto cleanup;
 	ret = qgroup_reserve(root, to_reserve, true, BTRFS_QGROUP_RSV_DATA);
 	if (ret < 0)
 		goto cleanup;
 	return ret;
 cleanup:
 	
 	ULIST_ITER_INIT(&uiter);
 	while ((unode = ulist_next(&reserved->range_changed, &uiter)))
 		clear_extent_bit(&BTRFS_I(inode)->io_tree, unode->val,
 				 unode->aux, EXTENT_QGROUP_RESERVED, 0, 0, NULL);
 	extent_changeset_release(reserved);
 	return ret;
 }",47,291
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_transaction_switch_commit_roots,27,27," static noinline void switch_commit_roots(struct btrfs_transaction *trans)
 {
 	struct btrfs_fs_info *fs_info = trans->fs_info;
 	struct btrfs_root *root, *tmp;
 	down_write(&fs_info->commit_root_sem);
 	list_for_each_entry_safe(root, tmp, &trans->switch_commits,
 				 dirty_list) {
 		list_del_init(&root->dirty_list);
 		free_extent_buffer(root->commit_root);
 		root->commit_root = btrfs_root_node(root);
-		if (is_fstree(root->objectid))
+		if (is_fstree(root->root_key.objectid))
 			btrfs_unpin_free_ino(root);
 		clear_btree_io_tree(&root->dirty_log_pages);
 	}
 	
 	spin_lock(&trans->dropped_roots_lock);
 	while (!list_empty(&trans->dropped_roots)) {
 		root = list_first_entry(&trans->dropped_roots,
 					struct btrfs_root, root_list);
 		list_del_init(&root->root_list);
 		spin_unlock(&trans->dropped_roots_lock);
 		btrfs_drop_and_free_fs_root(fs_info, root);
 		spin_lock(&trans->dropped_roots_lock);
 	}
 	spin_unlock(&trans->dropped_roots_lock);
 	up_write(&fs_info->commit_root_sem);
 }",28,196
EXP0-7,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/EXP0-7/1533533124_2018-08-06_4fd786e6c3d6_qgroup___btrfs_qgroup_free_meta,13,14," void __btrfs_qgroup_free_meta(struct btrfs_root *root, int num_bytes,
 			      enum btrfs_qgroup_rsv_type type)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	if (!test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags) ||
-	    !is_fstree(root->objectid))
+	    !is_fstree(root->root_key.objectid))
 		return;
 	
 	num_bytes = sub_root_meta_rsv(root, num_bytes, type);
 	BUG_ON(num_bytes != round_down(num_bytes, fs_info->nodesize));
 	trace_qgroup_meta_reserve(root, type, -(s64)num_bytes);
-	btrfs_qgroup_free_refroot(fs_info, root->objectid, num_bytes, type);
+	btrfs_qgroup_free_refroot(fs_info, root->root_key.objectid, num_bytes,
+				  type);
 }",16,130
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_drr_drr_init_qdisc,13,13," static int drr_init_qdisc(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct drr_sched *q = qdisc_priv(sch);
 	int err;
-	err = tcf_block_get(&q->block, &q->filter_list);
+	err = tcf_block_get(&q->block, &q->filter_list, sch);
 	if (err)
 		return err;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		return err;
 	INIT_LIST_HEAD(&q->active);
 	return 0;
 }",14,100
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_sfq_sfq_init,48,48," static int sfq_init(struct Qdisc *sch, struct nlattr *opt,
 		    struct netlink_ext_ack *extack)
 {
 	struct sfq_sched_data *q = qdisc_priv(sch);
 	int i;
 	int err;
 	q->sch = sch;
 	timer_setup(&q->perturb_timer, sfq_perturbation, TIMER_DEFERRABLE);
-	err = tcf_block_get(&q->block, &q->filter_list, sch);
+	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
 	if (err)
 		return err;
 	for (i = 0; i < SFQ_MAX_DEPTH + 1; i++) {
 		q->dep[i].next = i + SFQ_MAX_FLOWS;
 		q->dep[i].prev = i + SFQ_MAX_FLOWS;
 	}
 	q->limit = SFQ_MAX_DEPTH;
 	q->maxdepth = SFQ_MAX_DEPTH;
 	q->cur_depth = 0;
 	q->tail = NULL;
 	q->divisor = SFQ_DEFAULT_HASH_DIVISOR;
 	q->maxflows = SFQ_DEFAULT_FLOWS;
 	q->quantum = psched_mtu(qdisc_dev(sch));
 	q->scaled_quantum = SFQ_ALLOT_SIZE(q->quantum);
 	q->perturb_period = 0;
 	q->perturbation = prandom_u32();
 	if (opt) {
 		int err = sfq_change(sch, opt);
 		if (err)
 			return err;
 	}
 	q->ht = sfq_alloc(sizeof(q->ht[0]) * q->divisor);
 	q->slots = sfq_alloc(sizeof(q->slots[0]) * q->maxflows);
 	if (!q->ht || !q->slots) {
 		
 		return -ENOMEM;
 	}
 	for (i = 0; i < q->divisor; i++)
 		q->ht[i] = SFQ_EMPTY_SLOT;
 	for (i = 0; i < q->maxflows; i++) {
 		slot_queue_init(&q->slots[i]);
 		sfq_link(q, i);
 	}
 	if (q->limit >= 1)
 		sch->flags |= TCQ_F_CAN_BYPASS;
 	else
 		sch->flags &= ~TCQ_F_CAN_BYPASS;
 	return 0;
 }",49,383
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_sfb_sfb_init,10,10," static int sfb_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct sfb_sched_data *q = qdisc_priv(sch);
 	int err;
-	err = tcf_block_get(&q->block, &q->filter_list);
+	err = tcf_block_get(&q->block, &q->filter_list, sch);
 	if (err)
 		return err;
 	q->qdisc = &noop_qdisc;
 	return sfb_change(sch, opt);
 }",11,85
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_htb_htb_init,38,38," static int htb_init(struct Qdisc *sch, struct nlattr *opt,
 		    struct netlink_ext_ack *extack)
 {
 	struct htb_sched *q = qdisc_priv(sch);
 	struct nlattr *tb[TCA_HTB_MAX + 1];
 	struct tc_htb_glob *gopt;
 	int err;
 	int i;
 	qdisc_watchdog_init(&q->watchdog, sch);
 	INIT_WORK(&q->work, htb_work_func);
 	if (!opt)
 		return -EINVAL;
-	err = tcf_block_get(&q->block, &q->filter_list, sch);
+	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
 	if (err)
 		return err;
 	err = nla_parse_nested(tb, TCA_HTB_MAX, opt, htb_policy, NULL);
 	if (err < 0)
 		return err;
 	if (!tb[TCA_HTB_INIT])
 		return -EINVAL;
 	gopt = nla_data(tb[TCA_HTB_INIT]);
 	if (gopt->version != HTB_VER >> 16)
 		return -EINVAL;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		return err;
 	for (i = 0; i < TC_HTB_NUMPRIO; i++)
 		INIT_LIST_HEAD(q->drops + i);
 	qdisc_skb_head_init(&q->direct_queue);
 	if (tb[TCA_HTB_DIRECT_QLEN])
 		q->direct_qlen = nla_get_u32(tb[TCA_HTB_DIRECT_QLEN]);
 	else
 		q->direct_qlen = qdisc_dev(sch)->tx_queue_len;
 	if ((q->rate2quantum = gopt->rate2quantum) < 1)
 		q->rate2quantum = 1;
 	q->defcls = gopt->defcls;
 	return 0;
 }",39,297
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_atm_atm_tc_change,108,108," static int atm_tc_change(struct Qdisc *sch, u32 classid, u32 parent,
 			 struct nlattr **tca, unsigned long *arg)
 {
 	struct atm_qdisc_data *p = qdisc_priv(sch);
 	struct atm_flow_data *flow = (struct atm_flow_data *)*arg;
 	struct atm_flow_data *excess = NULL;
 	struct nlattr *opt = tca[TCA_OPTIONS];
 	struct nlattr *tb[TCA_ATM_MAX + 1];
 	struct socket *sock;
 	int fd, error, hdr_len;
 	void *hdr;
 	pr_debug(""atm_tc_change(sch %p,[qdisc %p],classid %x,parent %x,""
 		""flow %p,opt %p)\n"", sch, p, classid, parent, flow, opt);
 	
 	if (parent && parent != TC_H_ROOT && parent != sch->handle)
 		return -EINVAL;
 	
 	if (flow)
 		return -EBUSY;
 	if (opt == NULL)
 		return -EINVAL;
 	error = nla_parse_nested(tb, TCA_ATM_MAX, opt, atm_policy, NULL);
 	if (error < 0)
 		return error;
 	if (!tb[TCA_ATM_FD])
 		return -EINVAL;
 	fd = nla_get_u32(tb[TCA_ATM_FD]);
 	pr_debug(""atm_tc_change: fd %d\n"", fd);
 	if (tb[TCA_ATM_HDR]) {
 		hdr_len = nla_len(tb[TCA_ATM_HDR]);
 		hdr = nla_data(tb[TCA_ATM_HDR]);
 	} else {
 		hdr_len = RFC1483LLC_LEN;
 		hdr = NULL;	
 	}
 	if (!tb[TCA_ATM_EXCESS])
 		excess = NULL;
 	else {
 		excess = (struct atm_flow_data *)
 			atm_tc_find(sch, nla_get_u32(tb[TCA_ATM_EXCESS]));
 		if (!excess)
 			return -ENOENT;
 	}
 	pr_debug(""atm_tc_change: type %d, payload %d, hdr_len %d\n"",
 		 opt->nla_type, nla_len(opt), hdr_len);
 	sock = sockfd_lookup(fd, &error);
 	if (!sock)
 		return error;	
 	pr_debug(""atm_tc_change: f_count %ld\n"", file_count(sock->file));
 	if (sock->ops->family != PF_ATMSVC && sock->ops->family != PF_ATMPVC) {
 		error = -EPROTOTYPE;
 		goto err_out;
 	}
 	
 	if (classid) {
 		if (TC_H_MAJ(classid ^ sch->handle)) {
 			pr_debug(""atm_tc_change: classid mismatch\n"");
 			error = -EINVAL;
 			goto err_out;
 		}
 	} else {
 		int i;
 		unsigned long cl;
 		for (i = 1; i < 0x8000; i++) {
 			classid = TC_H_MAKE(sch->handle, 0x8000 | i);
 			cl = atm_tc_find(sch, classid);
 			if (!cl)
 				break;
 		}
 	}
 	pr_debug(""atm_tc_change: new id %x\n"", classid);
 	flow = kzalloc(sizeof(struct atm_flow_data) + hdr_len, GFP_KERNEL);
 	pr_debug(""atm_tc_change: flow %p\n"", flow);
 	if (!flow) {
 		error = -ENOBUFS;
 		goto err_out;
 	}
-	error = tcf_block_get(&flow->block, &flow->filter_list);
+	error = tcf_block_get(&flow->block, &flow->filter_list, sch);
 	if (error) {
 		kfree(flow);
 		goto err_out;
 	}
 	flow->q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops, classid);
 	if (!flow->q)
 		flow->q = &noop_qdisc;
 	pr_debug(""atm_tc_change: qdisc %p\n"", flow->q);
 	flow->sock = sock;
 	flow->vcc = ATM_SD(sock);	
 	flow->vcc->user_back = flow;
 	pr_debug(""atm_tc_change: vcc %p\n"", flow->vcc);
 	flow->old_pop = flow->vcc->pop;
 	flow->parent = p;
 	flow->vcc->pop = sch_atm_pop;
 	flow->common.classid = classid;
 	flow->ref = 1;
 	flow->excess = excess;
 	list_add(&flow->list, &p->link.list);
 	flow->hdr_len = hdr_len;
 	if (hdr)
 		memcpy(flow->hdr, hdr, hdr_len);
 	else
 		memcpy(flow->hdr, llc_oui_ip, sizeof(llc_oui_ip));
 	*arg = (unsigned long)flow;
 	return 0;
 err_out:
 	sockfd_put(sock);
 	return error;
 }",109,754
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_drr_drr_init_qdisc,14,14," static int drr_init_qdisc(struct Qdisc *sch, struct nlattr *opt,
 			  struct netlink_ext_ack *extack)
 {
 	struct drr_sched *q = qdisc_priv(sch);
 	int err;
-	err = tcf_block_get(&q->block, &q->filter_list, sch);
+	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
 	if (err)
 		return err;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		return err;
 	INIT_LIST_HEAD(&q->active);
 	return 0;
 }",15,109
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_qfq_qfq_init_qdisc,32,32," static int qfq_init_qdisc(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct qfq_sched *q = qdisc_priv(sch);
 	struct qfq_group *grp;
 	int i, j, err;
 	u32 max_cl_shift, maxbudg_shift, max_classes;
-	err = tcf_block_get(&q->block, &q->filter_list);
+	err = tcf_block_get(&q->block, &q->filter_list, sch);
 	if (err)
 		return err;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		return err;
 	if (qdisc_dev(sch)->tx_queue_len + 1 > QFQ_MAX_AGG_CLASSES)
 		max_classes = QFQ_MAX_AGG_CLASSES;
 	else
 		max_classes = qdisc_dev(sch)->tx_queue_len + 1;
 	
 	max_cl_shift = __fls(max_classes);
 	q->max_agg_classes = 1<<max_cl_shift;
 	
 	maxbudg_shift = QFQ_MTU_SHIFT + max_cl_shift;
 	q->min_slot_shift = FRAC_BITS + maxbudg_shift - QFQ_MAX_INDEX;
 	for (i = 0; i <= QFQ_MAX_INDEX; i++) {
 		grp = &q->groups[i];
 		grp->index = i;
 		grp->slot_shift = q->min_slot_shift + i;
 		for (j = 0; j < QFQ_MAX_SLOTS; j++)
 			INIT_HLIST_HEAD(&grp->slots[j]);
 	}
 	INIT_HLIST_HEAD(&q->nonfull_aggs);
 	return 0;
 }",33,241
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_htb_htb_change_class,166,166," static int htb_change_class(struct Qdisc *sch, u32 classid,
 			    u32 parentid, struct nlattr **tca,
 			    unsigned long *arg, struct netlink_ext_ack *extack)
 {
 	int err = -EINVAL;
 	struct htb_sched *q = qdisc_priv(sch);
 	struct htb_class *cl = (struct htb_class *)*arg, *parent;
 	struct nlattr *opt = tca[TCA_OPTIONS];
 	struct nlattr *tb[TCA_HTB_MAX + 1];
 	struct tc_htb_opt *hopt;
 	u64 rate64, ceil64;
 	
 	if (!opt)
 		goto failure;
 	err = nla_parse_nested(tb, TCA_HTB_MAX, opt, htb_policy, NULL);
 	if (err < 0)
 		goto failure;
 	err = -EINVAL;
 	if (tb[TCA_HTB_PARMS] == NULL)
 		goto failure;
 	parent = parentid == TC_H_ROOT ? NULL : htb_find(parentid, sch);
 	hopt = nla_data(tb[TCA_HTB_PARMS]);
 	if (!hopt->rate.rate || !hopt->ceil.rate)
 		goto failure;
 	
 	if (hopt->rate.linklayer == TC_LINKLAYER_UNAWARE)
 		qdisc_put_rtab(qdisc_get_rtab(&hopt->rate, tb[TCA_HTB_RTAB],
 					      NULL));
 	if (hopt->ceil.linklayer == TC_LINKLAYER_UNAWARE)
 		qdisc_put_rtab(qdisc_get_rtab(&hopt->ceil, tb[TCA_HTB_CTAB],
 					      NULL));
 	if (!cl) {		
 		struct Qdisc *new_q;
 		int prio;
 		struct {
 			struct nlattr		nla;
 			struct gnet_estimator	opt;
 		} est = {
 			.nla = {
 				.nla_len	= nla_attr_size(sizeof(est.opt)),
 				.nla_type	= TCA_RATE,
 			},
 			.opt = {
 				
 				.interval	= 2,
 				.ewma_log	= 2,
 			},
 		};
 		
 		if (!classid || TC_H_MAJ(classid ^ sch->handle) ||
 		    htb_find(classid, sch))
 			goto failure;
 		
 		if (parent && parent->parent && parent->parent->level < 2) {
 			pr_err(""htb: tree is too deep\n"");
 			goto failure;
 		}
 		err = -ENOBUFS;
 		cl = kzalloc(sizeof(*cl), GFP_KERNEL);
 		if (!cl)
 			goto failure;
-		err = tcf_block_get(&cl->block, &cl->filter_list, sch);
+		err = tcf_block_get(&cl->block, &cl->filter_list, sch, extack);
 		if (err) {
 			kfree(cl);
 			goto failure;
 		}
 		if (htb_rate_est || tca[TCA_RATE]) {
 			err = gen_new_estimator(&cl->bstats, NULL,
 						&cl->rate_est,
 						NULL,
 						qdisc_root_sleeping_running(sch),
 						tca[TCA_RATE] ? : &est.nla);
 			if (err) {
 				tcf_block_put(cl->block);
 				kfree(cl);
 				goto failure;
 			}
 		}
 		cl->children = 0;
 		INIT_LIST_HEAD(&cl->un.leaf.drop_list);
 		RB_CLEAR_NODE(&cl->pq_node);
 		for (prio = 0; prio < TC_HTB_NUMPRIO; prio++)
 			RB_CLEAR_NODE(&cl->node[prio]);
 		
 		new_q = qdisc_create_dflt(sch->dev_queue,
 					  &pfifo_qdisc_ops, classid);
 		sch_tree_lock(sch);
 		if (parent && !parent->level) {
 			unsigned int qlen = parent->un.leaf.q->q.qlen;
 			unsigned int backlog = parent->un.leaf.q->qstats.backlog;
 			
 			qdisc_reset(parent->un.leaf.q);
 			qdisc_tree_reduce_backlog(parent->un.leaf.q, qlen, backlog);
 			qdisc_destroy(parent->un.leaf.q);
 			if (parent->prio_activity)
 				htb_deactivate(q, parent);
 			
 			if (parent->cmode != HTB_CAN_SEND) {
 				htb_safe_rb_erase(&parent->pq_node, &q->hlevel[0].wait_pq);
 				parent->cmode = HTB_CAN_SEND;
 			}
 			parent->level = (parent->parent ? parent->parent->level
 					 : TC_HTB_MAXDEPTH) - 1;
 			memset(&parent->un.inner, 0, sizeof(parent->un.inner));
 		}
 		
 		cl->un.leaf.q = new_q ? new_q : &noop_qdisc;
 		cl->common.classid = classid;
 		cl->parent = parent;
 		
 		cl->tokens = PSCHED_TICKS2NS(hopt->buffer);
 		cl->ctokens = PSCHED_TICKS2NS(hopt->cbuffer);
 		cl->mbuffer = 60ULL * NSEC_PER_SEC;	
 		cl->t_c = ktime_get_ns();
 		cl->cmode = HTB_CAN_SEND;
 		
 		qdisc_class_hash_insert(&q->clhash, &cl->common);
 		if (parent)
 			parent->children++;
 		if (cl->un.leaf.q != &noop_qdisc)
 			qdisc_hash_add(cl->un.leaf.q, true);
 	} else {
 		if (tca[TCA_RATE]) {
 			err = gen_replace_estimator(&cl->bstats, NULL,
 						    &cl->rate_est,
 						    NULL,
 						    qdisc_root_sleeping_running(sch),
 						    tca[TCA_RATE]);
 			if (err)
 				return err;
 		}
 		sch_tree_lock(sch);
 	}
 	rate64 = tb[TCA_HTB_RATE64] ? nla_get_u64(tb[TCA_HTB_RATE64]) : 0;
 	ceil64 = tb[TCA_HTB_CEIL64] ? nla_get_u64(tb[TCA_HTB_CEIL64]) : 0;
 	psched_ratecfg_precompute(&cl->rate, &hopt->rate, rate64);
 	psched_ratecfg_precompute(&cl->ceil, &hopt->ceil, ceil64);
 	
 	if (!cl->level) {
 		u64 quantum = cl->rate.rate_bytes_ps;
 		do_div(quantum, q->rate2quantum);
 		cl->quantum = min_t(u64, quantum, INT_MAX);
 		if (!hopt->quantum && cl->quantum < 1000) {
 			pr_warn(""HTB: quantum of class %X is small. Consider r2q change.\n"",
 				cl->common.classid);
 			cl->quantum = 1000;
 		}
 		if (!hopt->quantum && cl->quantum > 200000) {
 			pr_warn(""HTB: quantum of class %X is big. Consider r2q change.\n"",
 				cl->common.classid);
 			cl->quantum = 200000;
 		}
 		if (hopt->quantum)
 			cl->quantum = hopt->quantum;
 		if ((cl->prio = hopt->prio) >= TC_HTB_NUMPRIO)
 			cl->prio = TC_HTB_NUMPRIO - 1;
 	}
 	cl->buffer = PSCHED_TICKS2NS(hopt->buffer);
 	cl->cbuffer = PSCHED_TICKS2NS(hopt->cbuffer);
 	sch_tree_unlock(sch);
 	qdisc_class_hash_grow(sch, &q->clhash);
 	*arg = (unsigned long)cl;
 	return 0;
 failure:
 	return err;
 }",167,1188
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_cbq_cbq_change_class,157,157," static int
 cbq_change_class(struct Qdisc *sch, u32 classid, u32 parentid, struct nlattr **tca,
 		 unsigned long *arg)
 {
 	int err;
 	struct cbq_sched_data *q = qdisc_priv(sch);
 	struct cbq_class *cl = (struct cbq_class *)*arg;
 	struct nlattr *opt = tca[TCA_OPTIONS];
 	struct nlattr *tb[TCA_CBQ_MAX + 1];
 	struct cbq_class *parent;
 	struct qdisc_rate_table *rtab = NULL;
 	if (opt == NULL)
 		return -EINVAL;
 	err = nla_parse_nested(tb, TCA_CBQ_MAX, opt, cbq_policy, NULL);
 	if (err < 0)
 		return err;
 	if (tb[TCA_CBQ_OVL_STRATEGY] || tb[TCA_CBQ_POLICE])
 		return -EOPNOTSUPP;
 	if (cl) {
 		
 		if (parentid) {
 			if (cl->tparent &&
 			    cl->tparent->common.classid != parentid)
 				return -EINVAL;
 			if (!cl->tparent && parentid != TC_H_ROOT)
 				return -EINVAL;
 		}
 		if (tb[TCA_CBQ_RATE]) {
 			rtab = qdisc_get_rtab(nla_data(tb[TCA_CBQ_RATE]),
 					      tb[TCA_CBQ_RTAB]);
 			if (rtab == NULL)
 				return -EINVAL;
 		}
 		if (tca[TCA_RATE]) {
 			err = gen_replace_estimator(&cl->bstats, NULL,
 						    &cl->rate_est,
 						    NULL,
 						    qdisc_root_sleeping_running(sch),
 						    tca[TCA_RATE]);
 			if (err) {
 				qdisc_put_rtab(rtab);
 				return err;
 			}
 		}
 		
 		sch_tree_lock(sch);
 		if (cl->next_alive != NULL)
 			cbq_deactivate_class(cl);
 		if (rtab) {
 			qdisc_put_rtab(cl->R_tab);
 			cl->R_tab = rtab;
 		}
 		if (tb[TCA_CBQ_LSSOPT])
 			cbq_set_lss(cl, nla_data(tb[TCA_CBQ_LSSOPT]));
 		if (tb[TCA_CBQ_WRROPT]) {
 			cbq_rmprio(q, cl);
 			cbq_set_wrr(cl, nla_data(tb[TCA_CBQ_WRROPT]));
 		}
 		if (tb[TCA_CBQ_FOPT])
 			cbq_set_fopt(cl, nla_data(tb[TCA_CBQ_FOPT]));
 		if (cl->q->q.qlen)
 			cbq_activate_class(cl);
 		sch_tree_unlock(sch);
 		return 0;
 	}
 	if (parentid == TC_H_ROOT)
 		return -EINVAL;
 	if (tb[TCA_CBQ_WRROPT] == NULL || tb[TCA_CBQ_RATE] == NULL ||
 	    tb[TCA_CBQ_LSSOPT] == NULL)
 		return -EINVAL;
 	rtab = qdisc_get_rtab(nla_data(tb[TCA_CBQ_RATE]), tb[TCA_CBQ_RTAB]);
 	if (rtab == NULL)
 		return -EINVAL;
 	if (classid) {
 		err = -EINVAL;
 		if (TC_H_MAJ(classid ^ sch->handle) ||
 		    cbq_class_lookup(q, classid))
 			goto failure;
 	} else {
 		int i;
 		classid = TC_H_MAKE(sch->handle, 0x8000);
 		for (i = 0; i < 0x8000; i++) {
 			if (++q->hgenerator >= 0x8000)
 				q->hgenerator = 1;
 			if (cbq_class_lookup(q, classid|q->hgenerator) == NULL)
 				break;
 		}
 		err = -ENOSR;
 		if (i >= 0x8000)
 			goto failure;
 		classid = classid|q->hgenerator;
 	}
 	parent = &q->link;
 	if (parentid) {
 		parent = cbq_class_lookup(q, parentid);
 		err = -EINVAL;
 		if (parent == NULL)
 			goto failure;
 	}
 	err = -ENOBUFS;
 	cl = kzalloc(sizeof(*cl), GFP_KERNEL);
 	if (cl == NULL)
 		goto failure;
-	err = tcf_block_get(&cl->block, &cl->filter_list);
+	err = tcf_block_get(&cl->block, &cl->filter_list, sch);
 	if (err) {
 		kfree(cl);
 		return err;
 	}
 	if (tca[TCA_RATE]) {
 		err = gen_new_estimator(&cl->bstats, NULL, &cl->rate_est,
 					NULL,
 					qdisc_root_sleeping_running(sch),
 					tca[TCA_RATE]);
 		if (err) {
 			tcf_block_put(cl->block);
 			kfree(cl);
 			goto failure;
 		}
 	}
 	cl->R_tab = rtab;
 	rtab = NULL;
 	cl->q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops, classid);
 	if (!cl->q)
 		cl->q = &noop_qdisc;
 	else
 		qdisc_hash_add(cl->q, true);
 	cl->common.classid = classid;
 	cl->tparent = parent;
 	cl->qdisc = sch;
 	cl->allot = parent->allot;
 	cl->quantum = cl->allot;
 	cl->weight = cl->R_tab->rate.rate;
 	sch_tree_lock(sch);
 	cbq_link_class(cl);
 	cl->borrow = cl->tparent;
 	if (cl->tparent != &q->link)
 		cl->share = cl->tparent;
 	cbq_adjust_levels(parent);
 	cl->minidle = -0x7FFFFFFF;
 	cbq_set_lss(cl, nla_data(tb[TCA_CBQ_LSSOPT]));
 	cbq_set_wrr(cl, nla_data(tb[TCA_CBQ_WRROPT]));
 	if (cl->ewma_log == 0)
 		cl->ewma_log = q->link.ewma_log;
 	if (cl->maxidle == 0)
 		cl->maxidle = q->link.maxidle;
 	if (cl->avpkt == 0)
 		cl->avpkt = q->link.avpkt;
 	if (tb[TCA_CBQ_FOPT])
 		cbq_set_fopt(cl, nla_data(tb[TCA_CBQ_FOPT]));
 	sch_tree_unlock(sch);
 	qdisc_class_hash_grow(sch, &q->clhash);
 	*arg = (unsigned long)cl;
 	return 0;
 failure:
 	qdisc_put_rtab(rtab);
 	return err;
 }",158,1037
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_atm_atm_tc_init,24,24," static int atm_tc_init(struct Qdisc *sch, struct nlattr *opt,
 		       struct netlink_ext_ack *extack)
 {
 	struct atm_qdisc_data *p = qdisc_priv(sch);
 	int err;
 	pr_debug(""atm_tc_init(sch %p,[qdisc %p],opt %p)\n"", sch, p, opt);
 	INIT_LIST_HEAD(&p->flows);
 	INIT_LIST_HEAD(&p->link.list);
 	list_add(&p->link.list, &p->flows);
 	p->link.q = qdisc_create_dflt(sch->dev_queue,
 				      &pfifo_qdisc_ops, sch->handle);
 	if (!p->link.q)
 		p->link.q = &noop_qdisc;
 	pr_debug(""atm_tc_init: link (%p) qdisc %p\n"", &p->link, p->link.q);
-	err = tcf_block_get(&p->link.block, &p->link.filter_list, sch);
+	err = tcf_block_get(&p->link.block, &p->link.filter_list, sch, extack);
 	if (err)
 		return err;
 	p->link.vcc = NULL;
 	p->link.sock = NULL;
 	p->link.common.classid = sch->handle;
 	p->link.ref = 1;
 	tasklet_init(&p->task, sch_atm_dequeue, (unsigned long)sch);
 	return 0;
 }",25,246
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_hfsc_hfsc_init_qdisc,33,33," static int
 hfsc_init_qdisc(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct hfsc_sched *q = qdisc_priv(sch);
 	struct tc_hfsc_qopt *qopt;
 	int err;
 	qdisc_watchdog_init(&q->watchdog, sch);
 	if (opt == NULL || nla_len(opt) < sizeof(*qopt))
 		return -EINVAL;
 	qopt = nla_data(opt);
 	q->defcls = qopt->defcls;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		return err;
 	q->eligible = RB_ROOT;
-	err = tcf_block_get(&q->root.block, &q->root.filter_list);
+	err = tcf_block_get(&q->root.block, &q->root.filter_list, sch);
 	if (err)
 		return err;
 	q->root.cl_common.classid = sch->handle;
 	q->root.sched   = q;
 	q->root.qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,
 					  sch->handle);
 	if (q->root.qdisc == NULL)
 		q->root.qdisc = &noop_qdisc;
 	else
 		qdisc_hash_add(q->root.qdisc, true);
 	INIT_LIST_HEAD(&q->root.children);
 	q->root.vt_tree = RB_ROOT;
 	q->root.cf_tree = RB_ROOT;
 	qdisc_class_hash_insert(&q->clhash, &q->root.cl_common);
 	qdisc_class_hash_grow(sch, &q->clhash);
 	return 0;
 }",34,279
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_hfsc_hfsc_change_class,140,140," static int
 hfsc_change_class(struct Qdisc *sch, u32 classid, u32 parentid,
 		  struct nlattr **tca, unsigned long *arg,
 		  struct netlink_ext_ack *extack)
 {
 	struct hfsc_sched *q = qdisc_priv(sch);
 	struct hfsc_class *cl = (struct hfsc_class *)*arg;
 	struct hfsc_class *parent = NULL;
 	struct nlattr *opt = tca[TCA_OPTIONS];
 	struct nlattr *tb[TCA_HFSC_MAX + 1];
 	struct tc_service_curve *rsc = NULL, *fsc = NULL, *usc = NULL;
 	u64 cur_time;
 	int err;
 	if (opt == NULL)
 		return -EINVAL;
 	err = nla_parse_nested(tb, TCA_HFSC_MAX, opt, hfsc_policy, NULL);
 	if (err < 0)
 		return err;
 	if (tb[TCA_HFSC_RSC]) {
 		rsc = nla_data(tb[TCA_HFSC_RSC]);
 		if (rsc->m1 == 0 && rsc->m2 == 0)
 			rsc = NULL;
 	}
 	if (tb[TCA_HFSC_FSC]) {
 		fsc = nla_data(tb[TCA_HFSC_FSC]);
 		if (fsc->m1 == 0 && fsc->m2 == 0)
 			fsc = NULL;
 	}
 	if (tb[TCA_HFSC_USC]) {
 		usc = nla_data(tb[TCA_HFSC_USC]);
 		if (usc->m1 == 0 && usc->m2 == 0)
 			usc = NULL;
 	}
 	if (cl != NULL) {
 		int old_flags;
 		if (parentid) {
 			if (cl->cl_parent &&
 			    cl->cl_parent->cl_common.classid != parentid)
 				return -EINVAL;
 			if (cl->cl_parent == NULL && parentid != TC_H_ROOT)
 				return -EINVAL;
 		}
 		cur_time = psched_get_time();
 		if (tca[TCA_RATE]) {
 			err = gen_replace_estimator(&cl->bstats, NULL,
 						    &cl->rate_est,
 						    NULL,
 						    qdisc_root_sleeping_running(sch),
 						    tca[TCA_RATE]);
 			if (err)
 				return err;
 		}
 		sch_tree_lock(sch);
 		old_flags = cl->cl_flags;
 		if (rsc != NULL)
 			hfsc_change_rsc(cl, rsc, cur_time);
 		if (fsc != NULL)
 			hfsc_change_fsc(cl, fsc);
 		if (usc != NULL)
 			hfsc_change_usc(cl, usc, cur_time);
 		if (cl->qdisc->q.qlen != 0) {
 			int len = qdisc_peek_len(cl->qdisc);
 			if (cl->cl_flags & HFSC_RSC) {
 				if (old_flags & HFSC_RSC)
 					update_ed(cl, len);
 				else
 					init_ed(cl, len);
 			}
 			if (cl->cl_flags & HFSC_FSC) {
 				if (old_flags & HFSC_FSC)
 					update_vf(cl, 0, cur_time);
 				else
 					init_vf(cl, len);
 			}
 		}
 		sch_tree_unlock(sch);
 		return 0;
 	}
 	if (parentid == TC_H_ROOT)
 		return -EEXIST;
 	parent = &q->root;
 	if (parentid) {
 		parent = hfsc_find_class(parentid, sch);
 		if (parent == NULL)
 			return -ENOENT;
 	}
 	if (classid == 0 || TC_H_MAJ(classid ^ sch->handle) != 0)
 		return -EINVAL;
 	if (hfsc_find_class(classid, sch))
 		return -EEXIST;
 	if (rsc == NULL && fsc == NULL)
 		return -EINVAL;
 	cl = kzalloc(sizeof(struct hfsc_class), GFP_KERNEL);
 	if (cl == NULL)
 		return -ENOBUFS;
-	err = tcf_block_get(&cl->block, &cl->filter_list, sch);
+	err = tcf_block_get(&cl->block, &cl->filter_list, sch, extack);
 	if (err) {
 		kfree(cl);
 		return err;
 	}
 	if (tca[TCA_RATE]) {
 		err = gen_new_estimator(&cl->bstats, NULL, &cl->rate_est,
 					NULL,
 					qdisc_root_sleeping_running(sch),
 					tca[TCA_RATE]);
 		if (err) {
 			tcf_block_put(cl->block);
 			kfree(cl);
 			return err;
 		}
 	}
 	if (rsc != NULL)
 		hfsc_change_rsc(cl, rsc, 0);
 	if (fsc != NULL)
 		hfsc_change_fsc(cl, fsc);
 	if (usc != NULL)
 		hfsc_change_usc(cl, usc, 0);
 	cl->cl_common.classid = classid;
 	cl->sched     = q;
 	cl->cl_parent = parent;
 	cl->qdisc = qdisc_create_dflt(sch->dev_queue,
 				      &pfifo_qdisc_ops, classid);
 	if (cl->qdisc == NULL)
 		cl->qdisc = &noop_qdisc;
 	else
 		qdisc_hash_add(cl->qdisc, true);
 	INIT_LIST_HEAD(&cl->children);
 	cl->vt_tree = RB_ROOT;
 	cl->cf_tree = RB_ROOT;
 	sch_tree_lock(sch);
 	qdisc_class_hash_insert(&q->clhash, &cl->cl_common);
 	list_add_tail(&cl->siblings, &parent->children);
 	if (parent->level == 0)
 		hfsc_purge_queue(sch, parent);
 	hfsc_adjust_levels(parent);
 	sch_tree_unlock(sch);
 	qdisc_class_hash_grow(sch, &q->clhash);
 	*arg = (unsigned long)cl;
 	return 0;
 }",141,931
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_prio_prio_init,11,11," static int prio_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct prio_sched_data *q = qdisc_priv(sch);
 	int err;
 	if (!opt)
 		return -EINVAL;
-	err = tcf_block_get(&q->block, &q->filter_list);
+	err = tcf_block_get(&q->block, &q->filter_list, sch);
 	if (err)
 		return err;
 	return prio_tune(sch, opt);
 }",12,87
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_cbq_cbq_change_class,157,157," static int
 cbq_change_class(struct Qdisc *sch, u32 classid, u32 parentid, struct nlattr **tca,
 		 unsigned long *arg, struct netlink_ext_ack *extack)
 {
 	int err;
 	struct cbq_sched_data *q = qdisc_priv(sch);
 	struct cbq_class *cl = (struct cbq_class *)*arg;
 	struct nlattr *opt = tca[TCA_OPTIONS];
 	struct nlattr *tb[TCA_CBQ_MAX + 1];
 	struct cbq_class *parent;
 	struct qdisc_rate_table *rtab = NULL;
 	if (!opt)
 		return -EINVAL;
 	err = nla_parse_nested(tb, TCA_CBQ_MAX, opt, cbq_policy, NULL);
 	if (err < 0)
 		return err;
 	if (tb[TCA_CBQ_OVL_STRATEGY] || tb[TCA_CBQ_POLICE])
 		return -EOPNOTSUPP;
 	if (cl) {
 		
 		if (parentid) {
 			if (cl->tparent &&
 			    cl->tparent->common.classid != parentid)
 				return -EINVAL;
 			if (!cl->tparent && parentid != TC_H_ROOT)
 				return -EINVAL;
 		}
 		if (tb[TCA_CBQ_RATE]) {
 			rtab = qdisc_get_rtab(nla_data(tb[TCA_CBQ_RATE]),
 					      tb[TCA_CBQ_RTAB], extack);
 			if (rtab == NULL)
 				return -EINVAL;
 		}
 		if (tca[TCA_RATE]) {
 			err = gen_replace_estimator(&cl->bstats, NULL,
 						    &cl->rate_est,
 						    NULL,
 						    qdisc_root_sleeping_running(sch),
 						    tca[TCA_RATE]);
 			if (err) {
 				qdisc_put_rtab(rtab);
 				return err;
 			}
 		}
 		
 		sch_tree_lock(sch);
 		if (cl->next_alive != NULL)
 			cbq_deactivate_class(cl);
 		if (rtab) {
 			qdisc_put_rtab(cl->R_tab);
 			cl->R_tab = rtab;
 		}
 		if (tb[TCA_CBQ_LSSOPT])
 			cbq_set_lss(cl, nla_data(tb[TCA_CBQ_LSSOPT]));
 		if (tb[TCA_CBQ_WRROPT]) {
 			cbq_rmprio(q, cl);
 			cbq_set_wrr(cl, nla_data(tb[TCA_CBQ_WRROPT]));
 		}
 		if (tb[TCA_CBQ_FOPT])
 			cbq_set_fopt(cl, nla_data(tb[TCA_CBQ_FOPT]));
 		if (cl->q->q.qlen)
 			cbq_activate_class(cl);
 		sch_tree_unlock(sch);
 		return 0;
 	}
 	if (parentid == TC_H_ROOT)
 		return -EINVAL;
 	if (!tb[TCA_CBQ_WRROPT] || !tb[TCA_CBQ_RATE] || !tb[TCA_CBQ_LSSOPT])
 		return -EINVAL;
 	rtab = qdisc_get_rtab(nla_data(tb[TCA_CBQ_RATE]), tb[TCA_CBQ_RTAB],
 			      extack);
 	if (rtab == NULL)
 		return -EINVAL;
 	if (classid) {
 		err = -EINVAL;
 		if (TC_H_MAJ(classid ^ sch->handle) ||
 		    cbq_class_lookup(q, classid))
 			goto failure;
 	} else {
 		int i;
 		classid = TC_H_MAKE(sch->handle, 0x8000);
 		for (i = 0; i < 0x8000; i++) {
 			if (++q->hgenerator >= 0x8000)
 				q->hgenerator = 1;
 			if (cbq_class_lookup(q, classid|q->hgenerator) == NULL)
 				break;
 		}
 		err = -ENOSR;
 		if (i >= 0x8000)
 			goto failure;
 		classid = classid|q->hgenerator;
 	}
 	parent = &q->link;
 	if (parentid) {
 		parent = cbq_class_lookup(q, parentid);
 		err = -EINVAL;
 		if (!parent)
 			goto failure;
 	}
 	err = -ENOBUFS;
 	cl = kzalloc(sizeof(*cl), GFP_KERNEL);
 	if (cl == NULL)
 		goto failure;
-	err = tcf_block_get(&cl->block, &cl->filter_list, sch);
+	err = tcf_block_get(&cl->block, &cl->filter_list, sch, extack);
 	if (err) {
 		kfree(cl);
 		return err;
 	}
 	if (tca[TCA_RATE]) {
 		err = gen_new_estimator(&cl->bstats, NULL, &cl->rate_est,
 					NULL,
 					qdisc_root_sleeping_running(sch),
 					tca[TCA_RATE]);
 		if (err) {
 			tcf_block_put(cl->block);
 			kfree(cl);
 			goto failure;
 		}
 	}
 	cl->R_tab = rtab;
 	rtab = NULL;
 	cl->q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops, classid);
 	if (!cl->q)
 		cl->q = &noop_qdisc;
 	else
 		qdisc_hash_add(cl->q, true);
 	cl->common.classid = classid;
 	cl->tparent = parent;
 	cl->qdisc = sch;
 	cl->allot = parent->allot;
 	cl->quantum = cl->allot;
 	cl->weight = cl->R_tab->rate.rate;
 	sch_tree_lock(sch);
 	cbq_link_class(cl);
 	cl->borrow = cl->tparent;
 	if (cl->tparent != &q->link)
 		cl->share = cl->tparent;
 	cbq_adjust_levels(parent);
 	cl->minidle = -0x7FFFFFFF;
 	cbq_set_lss(cl, nla_data(tb[TCA_CBQ_LSSOPT]));
 	cbq_set_wrr(cl, nla_data(tb[TCA_CBQ_WRROPT]));
 	if (cl->ewma_log == 0)
 		cl->ewma_log = q->link.ewma_log;
 	if (cl->maxidle == 0)
 		cl->maxidle = q->link.maxidle;
 	if (cl->avpkt == 0)
 		cl->avpkt = q->link.avpkt;
 	if (tb[TCA_CBQ_FOPT])
 		cbq_set_fopt(cl, nla_data(tb[TCA_CBQ_FOPT]));
 	sch_tree_unlock(sch);
 	qdisc_class_hash_grow(sch, &q->clhash);
 	*arg = (unsigned long)cl;
 	return 0;
 failure:
 	qdisc_put_rtab(rtab);
 	return err;
 }",158,1045
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_htb_htb_change_class,164,164," static int htb_change_class(struct Qdisc *sch, u32 classid,
 			    u32 parentid, struct nlattr **tca,
 			    unsigned long *arg)
 {
 	int err = -EINVAL;
 	struct htb_sched *q = qdisc_priv(sch);
 	struct htb_class *cl = (struct htb_class *)*arg, *parent;
 	struct nlattr *opt = tca[TCA_OPTIONS];
 	struct nlattr *tb[TCA_HTB_MAX + 1];
 	struct tc_htb_opt *hopt;
 	u64 rate64, ceil64;
 	
 	if (!opt)
 		goto failure;
 	err = nla_parse_nested(tb, TCA_HTB_MAX, opt, htb_policy, NULL);
 	if (err < 0)
 		goto failure;
 	err = -EINVAL;
 	if (tb[TCA_HTB_PARMS] == NULL)
 		goto failure;
 	parent = parentid == TC_H_ROOT ? NULL : htb_find(parentid, sch);
 	hopt = nla_data(tb[TCA_HTB_PARMS]);
 	if (!hopt->rate.rate || !hopt->ceil.rate)
 		goto failure;
 	
 	if (hopt->rate.linklayer == TC_LINKLAYER_UNAWARE)
 		qdisc_put_rtab(qdisc_get_rtab(&hopt->rate, tb[TCA_HTB_RTAB]));
 	if (hopt->ceil.linklayer == TC_LINKLAYER_UNAWARE)
 		qdisc_put_rtab(qdisc_get_rtab(&hopt->ceil, tb[TCA_HTB_CTAB]));
 	if (!cl) {		
 		struct Qdisc *new_q;
 		int prio;
 		struct {
 			struct nlattr		nla;
 			struct gnet_estimator	opt;
 		} est = {
 			.nla = {
 				.nla_len	= nla_attr_size(sizeof(est.opt)),
 				.nla_type	= TCA_RATE,
 			},
 			.opt = {
 				
 				.interval	= 2,
 				.ewma_log	= 2,
 			},
 		};
 		
 		if (!classid || TC_H_MAJ(classid ^ sch->handle) ||
 		    htb_find(classid, sch))
 			goto failure;
 		
 		if (parent && parent->parent && parent->parent->level < 2) {
 			pr_err(""htb: tree is too deep\n"");
 			goto failure;
 		}
 		err = -ENOBUFS;
 		cl = kzalloc(sizeof(*cl), GFP_KERNEL);
 		if (!cl)
 			goto failure;
-		err = tcf_block_get(&cl->block, &cl->filter_list);
+		err = tcf_block_get(&cl->block, &cl->filter_list, sch);
 		if (err) {
 			kfree(cl);
 			goto failure;
 		}
 		if (htb_rate_est || tca[TCA_RATE]) {
 			err = gen_new_estimator(&cl->bstats, NULL,
 						&cl->rate_est,
 						NULL,
 						qdisc_root_sleeping_running(sch),
 						tca[TCA_RATE] ? : &est.nla);
 			if (err) {
 				tcf_block_put(cl->block);
 				kfree(cl);
 				goto failure;
 			}
 		}
 		cl->children = 0;
 		INIT_LIST_HEAD(&cl->un.leaf.drop_list);
 		RB_CLEAR_NODE(&cl->pq_node);
 		for (prio = 0; prio < TC_HTB_NUMPRIO; prio++)
 			RB_CLEAR_NODE(&cl->node[prio]);
 		
 		new_q = qdisc_create_dflt(sch->dev_queue,
 					  &pfifo_qdisc_ops, classid);
 		sch_tree_lock(sch);
 		if (parent && !parent->level) {
 			unsigned int qlen = parent->un.leaf.q->q.qlen;
 			unsigned int backlog = parent->un.leaf.q->qstats.backlog;
 			
 			qdisc_reset(parent->un.leaf.q);
 			qdisc_tree_reduce_backlog(parent->un.leaf.q, qlen, backlog);
 			qdisc_destroy(parent->un.leaf.q);
 			if (parent->prio_activity)
 				htb_deactivate(q, parent);
 			
 			if (parent->cmode != HTB_CAN_SEND) {
 				htb_safe_rb_erase(&parent->pq_node, &q->hlevel[0].wait_pq);
 				parent->cmode = HTB_CAN_SEND;
 			}
 			parent->level = (parent->parent ? parent->parent->level
 					 : TC_HTB_MAXDEPTH) - 1;
 			memset(&parent->un.inner, 0, sizeof(parent->un.inner));
 		}
 		
 		cl->un.leaf.q = new_q ? new_q : &noop_qdisc;
 		cl->common.classid = classid;
 		cl->parent = parent;
 		
 		cl->tokens = PSCHED_TICKS2NS(hopt->buffer);
 		cl->ctokens = PSCHED_TICKS2NS(hopt->cbuffer);
 		cl->mbuffer = 60ULL * NSEC_PER_SEC;	
 		cl->t_c = ktime_get_ns();
 		cl->cmode = HTB_CAN_SEND;
 		
 		qdisc_class_hash_insert(&q->clhash, &cl->common);
 		if (parent)
 			parent->children++;
 		if (cl->un.leaf.q != &noop_qdisc)
 			qdisc_hash_add(cl->un.leaf.q, true);
 	} else {
 		if (tca[TCA_RATE]) {
 			err = gen_replace_estimator(&cl->bstats, NULL,
 						    &cl->rate_est,
 						    NULL,
 						    qdisc_root_sleeping_running(sch),
 						    tca[TCA_RATE]);
 			if (err)
 				return err;
 		}
 		sch_tree_lock(sch);
 	}
 	rate64 = tb[TCA_HTB_RATE64] ? nla_get_u64(tb[TCA_HTB_RATE64]) : 0;
 	ceil64 = tb[TCA_HTB_CEIL64] ? nla_get_u64(tb[TCA_HTB_CEIL64]) : 0;
 	psched_ratecfg_precompute(&cl->rate, &hopt->rate, rate64);
 	psched_ratecfg_precompute(&cl->ceil, &hopt->ceil, ceil64);
 	
 	if (!cl->level) {
 		u64 quantum = cl->rate.rate_bytes_ps;
 		do_div(quantum, q->rate2quantum);
 		cl->quantum = min_t(u64, quantum, INT_MAX);
 		if (!hopt->quantum && cl->quantum < 1000) {
 			pr_warn(""HTB: quantum of class %X is small. Consider r2q change.\n"",
 				cl->common.classid);
 			cl->quantum = 1000;
 		}
 		if (!hopt->quantum && cl->quantum > 200000) {
 			pr_warn(""HTB: quantum of class %X is big. Consider r2q change.\n"",
 				cl->common.classid);
 			cl->quantum = 200000;
 		}
 		if (hopt->quantum)
 			cl->quantum = hopt->quantum;
 		if ((cl->prio = hopt->prio) >= TC_HTB_NUMPRIO)
 			cl->prio = TC_HTB_NUMPRIO - 1;
 	}
 	cl->buffer = PSCHED_TICKS2NS(hopt->buffer);
 	cl->cbuffer = PSCHED_TICKS2NS(hopt->cbuffer);
 	sch_tree_unlock(sch);
 	qdisc_class_hash_grow(sch, &q->clhash);
 	*arg = (unsigned long)cl;
 	return 0;
 failure:
 	return err;
 }",165,1175
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_multiq_multiq_init,19,19," static int multiq_init(struct Qdisc *sch, struct nlattr *opt,
 		       struct netlink_ext_ack *extack)
 {
 	struct multiq_sched_data *q = qdisc_priv(sch);
 	int i, err;
 	q->queues = NULL;
 	if (!opt)
 		return -EINVAL;
-	err = tcf_block_get(&q->block, &q->filter_list, sch);
+	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
 	if (err)
 		return err;
 	q->max_bands = qdisc_dev(sch)->num_tx_queues;
 	q->queues = kcalloc(q->max_bands, sizeof(struct Qdisc *), GFP_KERNEL);
 	if (!q->queues)
 		return -ENOBUFS;
 	for (i = 0; i < q->max_bands; i++)
 		q->queues[i] = &noop_qdisc;
 	return multiq_tune(sch, opt, extack);
 }",20,173
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_qfq_qfq_init_qdisc,33,33," static int qfq_init_qdisc(struct Qdisc *sch, struct nlattr *opt,
 			  struct netlink_ext_ack *extack)
 {
 	struct qfq_sched *q = qdisc_priv(sch);
 	struct qfq_group *grp;
 	int i, j, err;
 	u32 max_cl_shift, maxbudg_shift, max_classes;
-	err = tcf_block_get(&q->block, &q->filter_list, sch);
+	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
 	if (err)
 		return err;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		return err;
 	if (qdisc_dev(sch)->tx_queue_len + 1 > QFQ_MAX_AGG_CLASSES)
 		max_classes = QFQ_MAX_AGG_CLASSES;
 	else
 		max_classes = qdisc_dev(sch)->tx_queue_len + 1;
 	
 	max_cl_shift = __fls(max_classes);
 	q->max_agg_classes = 1<<max_cl_shift;
 	
 	maxbudg_shift = QFQ_MTU_SHIFT + max_cl_shift;
 	q->min_slot_shift = FRAC_BITS + maxbudg_shift - QFQ_MAX_INDEX;
 	for (i = 0; i <= QFQ_MAX_INDEX; i++) {
 		grp = &q->groups[i];
 		grp->index = i;
 		grp->slot_shift = q->min_slot_shift + i;
 		for (j = 0; j < QFQ_MAX_SLOTS; j++)
 			INIT_HLIST_HEAD(&grp->slots[j]);
 	}
 	INIT_HLIST_HEAD(&q->nonfull_aggs);
 	return 0;
 }",34,250
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_fq_codel_fq_codel_init,44,44," static int fq_codel_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct fq_codel_sched_data *q = qdisc_priv(sch);
 	int i;
 	int err;
 	sch->limit = 10*1024;
 	q->flows_cnt = 1024;
 	q->memory_limit = 32 << 20; 
 	q->drop_batch_size = 64;
 	q->quantum = psched_mtu(qdisc_dev(sch));
 	INIT_LIST_HEAD(&q->new_flows);
 	INIT_LIST_HEAD(&q->old_flows);
 	codel_params_init(&q->cparams);
 	codel_stats_init(&q->cstats);
 	q->cparams.ecn = true;
 	q->cparams.mtu = psched_mtu(qdisc_dev(sch));
 	if (opt) {
 		int err = fq_codel_change(sch, opt);
 		if (err)
 			return err;
 	}
-	err = tcf_block_get(&q->block, &q->filter_list);
+	err = tcf_block_get(&q->block, &q->filter_list, sch);
 	if (err)
 		return err;
 	if (!q->flows) {
 		q->flows = kvzalloc(q->flows_cnt *
 					   sizeof(struct fq_codel_flow), GFP_KERNEL);
 		if (!q->flows)
 			return -ENOMEM;
 		q->backlogs = kvzalloc(q->flows_cnt * sizeof(u32), GFP_KERNEL);
 		if (!q->backlogs)
 			return -ENOMEM;
 		for (i = 0; i < q->flows_cnt; i++) {
 			struct fq_codel_flow *flow = q->flows + i;
 			INIT_LIST_HEAD(&flow->flowchain);
 			codel_vars_init(&flow->cvars);
 		}
 	}
 	if (sch->limit >= 1)
 		sch->flags |= TCQ_F_CAN_BYPASS;
 	else
 		sch->flags &= ~TCQ_F_CAN_BYPASS;
 	return 0;
 }",45,327
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_htb_htb_init,37,37," static int htb_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct htb_sched *q = qdisc_priv(sch);
 	struct nlattr *tb[TCA_HTB_MAX + 1];
 	struct tc_htb_glob *gopt;
 	int err;
 	int i;
 	qdisc_watchdog_init(&q->watchdog, sch);
 	INIT_WORK(&q->work, htb_work_func);
 	if (!opt)
 		return -EINVAL;
-	err = tcf_block_get(&q->block, &q->filter_list);
+	err = tcf_block_get(&q->block, &q->filter_list, sch);
 	if (err)
 		return err;
 	err = nla_parse_nested(tb, TCA_HTB_MAX, opt, htb_policy, NULL);
 	if (err < 0)
 		return err;
 	if (!tb[TCA_HTB_INIT])
 		return -EINVAL;
 	gopt = nla_data(tb[TCA_HTB_INIT]);
 	if (gopt->version != HTB_VER >> 16)
 		return -EINVAL;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		return err;
 	for (i = 0; i < TC_HTB_NUMPRIO; i++)
 		INIT_LIST_HEAD(q->drops + i);
 	qdisc_skb_head_init(&q->direct_queue);
 	if (tb[TCA_HTB_DIRECT_QLEN])
 		q->direct_qlen = nla_get_u32(tb[TCA_HTB_DIRECT_QLEN]);
 	else
 		q->direct_qlen = qdisc_dev(sch)->tx_queue_len;
 	if ((q->rate2quantum = gopt->rate2quantum) < 1)
 		q->rate2quantum = 1;
 	q->defcls = gopt->defcls;
 	return 0;
 }",38,288
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_ingress_clsact_init,16,16," static int clsact_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct clsact_sched_data *q = qdisc_priv(sch);
 	struct net_device *dev = qdisc_dev(sch);
 	int err;
-	err = tcf_block_get(&q->ingress_block, &dev->ingress_cl_list);
+	err = tcf_block_get(&q->ingress_block, &dev->ingress_cl_list, sch);
 	if (err)
 		return err;
-	err = tcf_block_get(&q->egress_block, &dev->egress_cl_list);
+	err = tcf_block_get(&q->egress_block, &dev->egress_cl_list, sch);
 	if (err)
 		return err;
 	net_inc_ingress_queue();
 	net_inc_egress_queue();
 	sch->flags |= TCQ_F_CPUSTATS;
 	return 0;
 }",18,138
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_dsmark_dsmark_init,49,49," static int dsmark_init(struct Qdisc *sch, struct nlattr *opt,
 		       struct netlink_ext_ack *extack)
 {
 	struct dsmark_qdisc_data *p = qdisc_priv(sch);
 	struct nlattr *tb[TCA_DSMARK_MAX + 1];
 	int err = -EINVAL;
 	u32 default_index = NO_DEFAULT_INDEX;
 	u16 indices;
 	int i;
 	pr_debug(""%s(sch %p,[qdisc %p],opt %p)\n"", __func__, sch, p, opt);
 	if (!opt)
 		goto errout;
-	err = tcf_block_get(&p->block, &p->filter_list, sch);
+	err = tcf_block_get(&p->block, &p->filter_list, sch, extack);
 	if (err)
 		return err;
 	err = nla_parse_nested(tb, TCA_DSMARK_MAX, opt, dsmark_policy, NULL);
 	if (err < 0)
 		goto errout;
 	err = -EINVAL;
 	indices = nla_get_u16(tb[TCA_DSMARK_INDICES]);
 	if (hweight32(indices) != 1)
 		goto errout;
 	if (tb[TCA_DSMARK_DEFAULT_INDEX])
 		default_index = nla_get_u16(tb[TCA_DSMARK_DEFAULT_INDEX]);
 	if (indices <= DSMARK_EMBEDDED_SZ)
 		p->mv = p->embedded;
 	else
 		p->mv = kmalloc_array(indices, sizeof(*p->mv), GFP_KERNEL);
 	if (!p->mv) {
 		err = -ENOMEM;
 		goto errout;
 	}
 	for (i = 0; i < indices; i++) {
 		p->mv[i].mask = 0xff;
 		p->mv[i].value = 0;
 	}
 	p->indices = indices;
 	p->default_index = default_index;
 	p->set_tc_index = nla_get_flag(tb[TCA_DSMARK_SET_TC_INDEX]);
 	p->q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops, sch->handle);
 	if (p->q == NULL)
 		p->q = &noop_qdisc;
 	else
 		qdisc_hash_add(p->q, true);
 	pr_debug(""%s: qdisc %p\n"", __func__, p->q);
 	err = 0;
 errout:
 	return err;
 }",50,373
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_hfsc_hfsc_init_qdisc,34,34," static int
 hfsc_init_qdisc(struct Qdisc *sch, struct nlattr *opt,
 		struct netlink_ext_ack *extack)
 {
 	struct hfsc_sched *q = qdisc_priv(sch);
 	struct tc_hfsc_qopt *qopt;
 	int err;
 	qdisc_watchdog_init(&q->watchdog, sch);
 	if (!opt || nla_len(opt) < sizeof(*qopt))
 		return -EINVAL;
 	qopt = nla_data(opt);
 	q->defcls = qopt->defcls;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		return err;
 	q->eligible = RB_ROOT;
-	err = tcf_block_get(&q->root.block, &q->root.filter_list, sch);
+	err = tcf_block_get(&q->root.block, &q->root.filter_list, sch, extack);
 	if (err)
 		return err;
 	q->root.cl_common.classid = sch->handle;
 	q->root.sched   = q;
 	q->root.qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,
 					  sch->handle);
 	if (q->root.qdisc == NULL)
 		q->root.qdisc = &noop_qdisc;
 	else
 		qdisc_hash_add(q->root.qdisc, true);
 	INIT_LIST_HEAD(&q->root.children);
 	q->root.vt_tree = RB_ROOT;
 	q->root.cf_tree = RB_ROOT;
 	qdisc_class_hash_insert(&q->clhash, &q->root.cl_common);
 	qdisc_class_hash_grow(sch, &q->clhash);
 	return 0;
 }",35,287
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_cbq_cbq_init,58,58," static int cbq_init(struct Qdisc *sch, struct nlattr *opt,
 		    struct netlink_ext_ack *extack)
 {
 	struct cbq_sched_data *q = qdisc_priv(sch);
 	struct nlattr *tb[TCA_CBQ_MAX + 1];
 	struct tc_ratespec *r;
 	int err;
 	qdisc_watchdog_init(&q->watchdog, sch);
 	hrtimer_init(&q->delay_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
 	q->delay_timer.function = cbq_undelay;
 	if (!opt)
 		return -EINVAL;
 	err = nla_parse_nested(tb, TCA_CBQ_MAX, opt, cbq_policy, NULL);
 	if (err < 0)
 		return err;
 	if (!tb[TCA_CBQ_RTAB] || !tb[TCA_CBQ_RATE])
 		return -EINVAL;
 	r = nla_data(tb[TCA_CBQ_RATE]);
 	q->link.R_tab = qdisc_get_rtab(r, tb[TCA_CBQ_RTAB], extack);
 	if (!q->link.R_tab)
 		return -EINVAL;
-	err = tcf_block_get(&q->link.block, &q->link.filter_list, sch);
+	err = tcf_block_get(&q->link.block, &q->link.filter_list, sch, extack);
 	if (err)
 		goto put_rtab;
 	err = qdisc_class_hash_init(&q->clhash);
 	if (err < 0)
 		goto put_block;
 	q->link.sibling = &q->link;
 	q->link.common.classid = sch->handle;
 	q->link.qdisc = sch;
 	q->link.q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,
 				      sch->handle);
 	if (!q->link.q)
 		q->link.q = &noop_qdisc;
 	else
 		qdisc_hash_add(q->link.q, true);
 	q->link.priority = TC_CBQ_MAXPRIO - 1;
 	q->link.priority2 = TC_CBQ_MAXPRIO - 1;
 	q->link.cpriority = TC_CBQ_MAXPRIO - 1;
 	q->link.allot = psched_mtu(qdisc_dev(sch));
 	q->link.quantum = q->link.allot;
 	q->link.weight = q->link.R_tab->rate.rate;
 	q->link.ewma_log = TC_CBQ_DEF_EWMA;
 	q->link.avpkt = q->link.allot/2;
 	q->link.minidle = -0x7FFFFFFF;
 	q->toplevel = TC_CBQ_MAXLEVEL;
 	q->now = psched_get_time();
 	cbq_link_class(&q->link);
 	if (tb[TCA_CBQ_LSSOPT])
 		cbq_set_lss(&q->link, nla_data(tb[TCA_CBQ_LSSOPT]));
 	cbq_addprio(q, &q->link);
 	return 0;
 put_block:
 	tcf_block_put(q->link.block);
 put_rtab:
 	qdisc_put_rtab(q->link.R_tab);
 	return err;
 }",59,509
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_atm_atm_tc_change,109,109," static int atm_tc_change(struct Qdisc *sch, u32 classid, u32 parent,
 			 struct nlattr **tca, unsigned long *arg,
 			 struct netlink_ext_ack *extack)
 {
 	struct atm_qdisc_data *p = qdisc_priv(sch);
 	struct atm_flow_data *flow = (struct atm_flow_data *)*arg;
 	struct atm_flow_data *excess = NULL;
 	struct nlattr *opt = tca[TCA_OPTIONS];
 	struct nlattr *tb[TCA_ATM_MAX + 1];
 	struct socket *sock;
 	int fd, error, hdr_len;
 	void *hdr;
 	pr_debug(""atm_tc_change(sch %p,[qdisc %p],classid %x,parent %x,""
 		""flow %p,opt %p)\n"", sch, p, classid, parent, flow, opt);
 	
 	if (parent && parent != TC_H_ROOT && parent != sch->handle)
 		return -EINVAL;
 	
 	if (flow)
 		return -EBUSY;
 	if (opt == NULL)
 		return -EINVAL;
 	error = nla_parse_nested(tb, TCA_ATM_MAX, opt, atm_policy, NULL);
 	if (error < 0)
 		return error;
 	if (!tb[TCA_ATM_FD])
 		return -EINVAL;
 	fd = nla_get_u32(tb[TCA_ATM_FD]);
 	pr_debug(""atm_tc_change: fd %d\n"", fd);
 	if (tb[TCA_ATM_HDR]) {
 		hdr_len = nla_len(tb[TCA_ATM_HDR]);
 		hdr = nla_data(tb[TCA_ATM_HDR]);
 	} else {
 		hdr_len = RFC1483LLC_LEN;
 		hdr = NULL;	
 	}
 	if (!tb[TCA_ATM_EXCESS])
 		excess = NULL;
 	else {
 		excess = (struct atm_flow_data *)
 			atm_tc_find(sch, nla_get_u32(tb[TCA_ATM_EXCESS]));
 		if (!excess)
 			return -ENOENT;
 	}
 	pr_debug(""atm_tc_change: type %d, payload %d, hdr_len %d\n"",
 		 opt->nla_type, nla_len(opt), hdr_len);
 	sock = sockfd_lookup(fd, &error);
 	if (!sock)
 		return error;	
 	pr_debug(""atm_tc_change: f_count %ld\n"", file_count(sock->file));
 	if (sock->ops->family != PF_ATMSVC && sock->ops->family != PF_ATMPVC) {
 		error = -EPROTOTYPE;
 		goto err_out;
 	}
 	
 	if (classid) {
 		if (TC_H_MAJ(classid ^ sch->handle)) {
 			pr_debug(""atm_tc_change: classid mismatch\n"");
 			error = -EINVAL;
 			goto err_out;
 		}
 	} else {
 		int i;
 		unsigned long cl;
 		for (i = 1; i < 0x8000; i++) {
 			classid = TC_H_MAKE(sch->handle, 0x8000 | i);
 			cl = atm_tc_find(sch, classid);
 			if (!cl)
 				break;
 		}
 	}
 	pr_debug(""atm_tc_change: new id %x\n"", classid);
 	flow = kzalloc(sizeof(struct atm_flow_data) + hdr_len, GFP_KERNEL);
 	pr_debug(""atm_tc_change: flow %p\n"", flow);
 	if (!flow) {
 		error = -ENOBUFS;
 		goto err_out;
 	}
-	error = tcf_block_get(&flow->block, &flow->filter_list, sch);
+	error = tcf_block_get(&flow->block, &flow->filter_list, sch, extack);
 	if (error) {
 		kfree(flow);
 		goto err_out;
 	}
 	flow->q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops, classid);
 	if (!flow->q)
 		flow->q = &noop_qdisc;
 	pr_debug(""atm_tc_change: qdisc %p\n"", flow->q);
 	flow->sock = sock;
 	flow->vcc = ATM_SD(sock);	
 	flow->vcc->user_back = flow;
 	pr_debug(""atm_tc_change: vcc %p\n"", flow->vcc);
 	flow->old_pop = flow->vcc->pop;
 	flow->parent = p;
 	flow->vcc->pop = sch_atm_pop;
 	flow->common.classid = classid;
 	flow->ref = 1;
 	flow->excess = excess;
 	list_add(&flow->list, &p->link.list);
 	flow->hdr_len = hdr_len;
 	if (hdr)
 		memcpy(flow->hdr, hdr, hdr_len);
 	else
 		memcpy(flow->hdr, llc_oui_ip, sizeof(llc_oui_ip));
 	*arg = (unsigned long)flow;
 	return 0;
 err_out:
 	sockfd_put(sock);
 	return error;
 }",110,763
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_prio_prio_init,12,12," static int prio_init(struct Qdisc *sch, struct nlattr *opt,
 		     struct netlink_ext_ack *extack)
 {
 	struct prio_sched_data *q = qdisc_priv(sch);
 	int err;
 	if (!opt)
 		return -EINVAL;
-	err = tcf_block_get(&q->block, &q->filter_list, sch);
+	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
 	if (err)
 		return err;
 	return prio_tune(sch, opt, extack);
 }",13,98
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_atm_atm_tc_init,23,23," static int atm_tc_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct atm_qdisc_data *p = qdisc_priv(sch);
 	int err;
 	pr_debug(""atm_tc_init(sch %p,[qdisc %p],opt %p)\n"", sch, p, opt);
 	INIT_LIST_HEAD(&p->flows);
 	INIT_LIST_HEAD(&p->link.list);
 	list_add(&p->link.list, &p->flows);
 	p->link.q = qdisc_create_dflt(sch->dev_queue,
 				      &pfifo_qdisc_ops, sch->handle);
 	if (!p->link.q)
 		p->link.q = &noop_qdisc;
 	pr_debug(""atm_tc_init: link (%p) qdisc %p\n"", &p->link, p->link.q);
-	err = tcf_block_get(&p->link.block, &p->link.filter_list);
+	err = tcf_block_get(&p->link.block, &p->link.filter_list, sch);
 	if (err)
 		return err;
 	p->link.vcc = NULL;
 	p->link.sock = NULL;
 	p->link.common.classid = sch->handle;
 	p->link.ref = 1;
 	tasklet_init(&p->task, sch_atm_dequeue, (unsigned long)sch);
 	return 0;
 }",24,237
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_ingress_ingress_init,12,12," static int ingress_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct ingress_sched_data *q = qdisc_priv(sch);
 	struct net_device *dev = qdisc_dev(sch);
 	int err;
-	err = tcf_block_get(&q->block, &dev->ingress_cl_list);
+	err = tcf_block_get(&q->block, &dev->ingress_cl_list, sch);
 	if (err)
 		return err;
 	net_inc_ingress_queue();
 	sch->flags |= TCQ_F_CPUSTATS;
 	return 0;
 }",13,93
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_sfq_sfq_init,47,47," static int sfq_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct sfq_sched_data *q = qdisc_priv(sch);
 	int i;
 	int err;
 	setup_deferrable_timer(&q->perturb_timer, sfq_perturbation,
 			       (unsigned long)sch);
-	err = tcf_block_get(&q->block, &q->filter_list);
+	err = tcf_block_get(&q->block, &q->filter_list, sch);
 	if (err)
 		return err;
 	for (i = 0; i < SFQ_MAX_DEPTH + 1; i++) {
 		q->dep[i].next = i + SFQ_MAX_FLOWS;
 		q->dep[i].prev = i + SFQ_MAX_FLOWS;
 	}
 	q->limit = SFQ_MAX_DEPTH;
 	q->maxdepth = SFQ_MAX_DEPTH;
 	q->cur_depth = 0;
 	q->tail = NULL;
 	q->divisor = SFQ_DEFAULT_HASH_DIVISOR;
 	q->maxflows = SFQ_DEFAULT_FLOWS;
 	q->quantum = psched_mtu(qdisc_dev(sch));
 	q->scaled_quantum = SFQ_ALLOT_SIZE(q->quantum);
 	q->perturb_period = 0;
 	q->perturbation = prandom_u32();
 	if (opt) {
 		int err = sfq_change(sch, opt);
 		if (err)
 			return err;
 	}
 	q->ht = sfq_alloc(sizeof(q->ht[0]) * q->divisor);
 	q->slots = sfq_alloc(sizeof(q->slots[0]) * q->maxflows);
 	if (!q->ht || !q->slots) {
 		
 		return -ENOMEM;
 	}
 	for (i = 0; i < q->divisor; i++)
 		q->ht[i] = SFQ_EMPTY_SLOT;
 	for (i = 0; i < q->maxflows; i++) {
 		slot_queue_init(&q->slots[i]);
 		sfq_link(q, i);
 	}
 	if (q->limit >= 1)
 		sch->flags |= TCQ_F_CAN_BYPASS;
 	else
 		sch->flags &= ~TCQ_F_CAN_BYPASS;
 	return 0;
 }",48,372
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_dsmark_dsmark_init,48,48," static int dsmark_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct dsmark_qdisc_data *p = qdisc_priv(sch);
 	struct nlattr *tb[TCA_DSMARK_MAX + 1];
 	int err = -EINVAL;
 	u32 default_index = NO_DEFAULT_INDEX;
 	u16 indices;
 	int i;
 	pr_debug(""%s(sch %p,[qdisc %p],opt %p)\n"", __func__, sch, p, opt);
 	if (!opt)
 		goto errout;
-	err = tcf_block_get(&p->block, &p->filter_list);
+	err = tcf_block_get(&p->block, &p->filter_list, sch);
 	if (err)
 		return err;
 	err = nla_parse_nested(tb, TCA_DSMARK_MAX, opt, dsmark_policy, NULL);
 	if (err < 0)
 		goto errout;
 	err = -EINVAL;
 	indices = nla_get_u16(tb[TCA_DSMARK_INDICES]);
 	if (hweight32(indices) != 1)
 		goto errout;
 	if (tb[TCA_DSMARK_DEFAULT_INDEX])
 		default_index = nla_get_u16(tb[TCA_DSMARK_DEFAULT_INDEX]);
 	if (indices <= DSMARK_EMBEDDED_SZ)
 		p->mv = p->embedded;
 	else
 		p->mv = kmalloc_array(indices, sizeof(*p->mv), GFP_KERNEL);
 	if (!p->mv) {
 		err = -ENOMEM;
 		goto errout;
 	}
 	for (i = 0; i < indices; i++) {
 		p->mv[i].mask = 0xff;
 		p->mv[i].value = 0;
 	}
 	p->indices = indices;
 	p->default_index = default_index;
 	p->set_tc_index = nla_get_flag(tb[TCA_DSMARK_SET_TC_INDEX]);
 	p->q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops, sch->handle);
 	if (p->q == NULL)
 		p->q = &noop_qdisc;
 	else
 		qdisc_hash_add(p->q, true);
 	pr_debug(""%s: qdisc %p\n"", __func__, p->q);
 	err = 0;
 errout:
 	return err;
 }",49,364
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_hfsc_hfsc_change_class,139,139," static int
 hfsc_change_class(struct Qdisc *sch, u32 classid, u32 parentid,
 		  struct nlattr **tca, unsigned long *arg)
 {
 	struct hfsc_sched *q = qdisc_priv(sch);
 	struct hfsc_class *cl = (struct hfsc_class *)*arg;
 	struct hfsc_class *parent = NULL;
 	struct nlattr *opt = tca[TCA_OPTIONS];
 	struct nlattr *tb[TCA_HFSC_MAX + 1];
 	struct tc_service_curve *rsc = NULL, *fsc = NULL, *usc = NULL;
 	u64 cur_time;
 	int err;
 	if (opt == NULL)
 		return -EINVAL;
 	err = nla_parse_nested(tb, TCA_HFSC_MAX, opt, hfsc_policy, NULL);
 	if (err < 0)
 		return err;
 	if (tb[TCA_HFSC_RSC]) {
 		rsc = nla_data(tb[TCA_HFSC_RSC]);
 		if (rsc->m1 == 0 && rsc->m2 == 0)
 			rsc = NULL;
 	}
 	if (tb[TCA_HFSC_FSC]) {
 		fsc = nla_data(tb[TCA_HFSC_FSC]);
 		if (fsc->m1 == 0 && fsc->m2 == 0)
 			fsc = NULL;
 	}
 	if (tb[TCA_HFSC_USC]) {
 		usc = nla_data(tb[TCA_HFSC_USC]);
 		if (usc->m1 == 0 && usc->m2 == 0)
 			usc = NULL;
 	}
 	if (cl != NULL) {
 		int old_flags;
 		if (parentid) {
 			if (cl->cl_parent &&
 			    cl->cl_parent->cl_common.classid != parentid)
 				return -EINVAL;
 			if (cl->cl_parent == NULL && parentid != TC_H_ROOT)
 				return -EINVAL;
 		}
 		cur_time = psched_get_time();
 		if (tca[TCA_RATE]) {
 			err = gen_replace_estimator(&cl->bstats, NULL,
 						    &cl->rate_est,
 						    NULL,
 						    qdisc_root_sleeping_running(sch),
 						    tca[TCA_RATE]);
 			if (err)
 				return err;
 		}
 		sch_tree_lock(sch);
 		old_flags = cl->cl_flags;
 		if (rsc != NULL)
 			hfsc_change_rsc(cl, rsc, cur_time);
 		if (fsc != NULL)
 			hfsc_change_fsc(cl, fsc);
 		if (usc != NULL)
 			hfsc_change_usc(cl, usc, cur_time);
 		if (cl->qdisc->q.qlen != 0) {
 			int len = qdisc_peek_len(cl->qdisc);
 			if (cl->cl_flags & HFSC_RSC) {
 				if (old_flags & HFSC_RSC)
 					update_ed(cl, len);
 				else
 					init_ed(cl, len);
 			}
 			if (cl->cl_flags & HFSC_FSC) {
 				if (old_flags & HFSC_FSC)
 					update_vf(cl, 0, cur_time);
 				else
 					init_vf(cl, len);
 			}
 		}
 		sch_tree_unlock(sch);
 		return 0;
 	}
 	if (parentid == TC_H_ROOT)
 		return -EEXIST;
 	parent = &q->root;
 	if (parentid) {
 		parent = hfsc_find_class(parentid, sch);
 		if (parent == NULL)
 			return -ENOENT;
 	}
 	if (classid == 0 || TC_H_MAJ(classid ^ sch->handle) != 0)
 		return -EINVAL;
 	if (hfsc_find_class(classid, sch))
 		return -EEXIST;
 	if (rsc == NULL && fsc == NULL)
 		return -EINVAL;
 	cl = kzalloc(sizeof(struct hfsc_class), GFP_KERNEL);
 	if (cl == NULL)
 		return -ENOBUFS;
-	err = tcf_block_get(&cl->block, &cl->filter_list);
+	err = tcf_block_get(&cl->block, &cl->filter_list, sch);
 	if (err) {
 		kfree(cl);
 		return err;
 	}
 	if (tca[TCA_RATE]) {
 		err = gen_new_estimator(&cl->bstats, NULL, &cl->rate_est,
 					NULL,
 					qdisc_root_sleeping_running(sch),
 					tca[TCA_RATE]);
 		if (err) {
 			tcf_block_put(cl->block);
 			kfree(cl);
 			return err;
 		}
 	}
 	if (rsc != NULL)
 		hfsc_change_rsc(cl, rsc, 0);
 	if (fsc != NULL)
 		hfsc_change_fsc(cl, fsc);
 	if (usc != NULL)
 		hfsc_change_usc(cl, usc, 0);
 	cl->cl_common.classid = classid;
 	cl->sched     = q;
 	cl->cl_parent = parent;
 	cl->qdisc = qdisc_create_dflt(sch->dev_queue,
 				      &pfifo_qdisc_ops, classid);
 	if (cl->qdisc == NULL)
 		cl->qdisc = &noop_qdisc;
 	else
 		qdisc_hash_add(cl->qdisc, true);
 	INIT_LIST_HEAD(&cl->children);
 	cl->vt_tree = RB_ROOT;
 	cl->cf_tree = RB_ROOT;
 	sch_tree_lock(sch);
 	qdisc_class_hash_insert(&q->clhash, &cl->cl_common);
 	list_add_tail(&cl->siblings, &parent->children);
 	if (parent->level == 0)
 		hfsc_purge_queue(sch, parent);
 	hfsc_adjust_levels(parent);
 	sch_tree_unlock(sch);
 	qdisc_class_hash_grow(sch, &q->clhash);
 	*arg = (unsigned long)cl;
 	return 0;
 }",140,922
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_fq_codel_fq_codel_init,45,45," static int fq_codel_init(struct Qdisc *sch, struct nlattr *opt,
 			 struct netlink_ext_ack *extack)
 {
 	struct fq_codel_sched_data *q = qdisc_priv(sch);
 	int i;
 	int err;
 	sch->limit = 10*1024;
 	q->flows_cnt = 1024;
 	q->memory_limit = 32 << 20; 
 	q->drop_batch_size = 64;
 	q->quantum = psched_mtu(qdisc_dev(sch));
 	INIT_LIST_HEAD(&q->new_flows);
 	INIT_LIST_HEAD(&q->old_flows);
 	codel_params_init(&q->cparams);
 	codel_stats_init(&q->cstats);
 	q->cparams.ecn = true;
 	q->cparams.mtu = psched_mtu(qdisc_dev(sch));
 	if (opt) {
 		int err = fq_codel_change(sch, opt, extack);
 		if (err)
 			return err;
 	}
-	err = tcf_block_get(&q->block, &q->filter_list, sch);
+	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
 	if (err)
 		return err;
 	if (!q->flows) {
 		q->flows = kvzalloc(q->flows_cnt *
 					   sizeof(struct fq_codel_flow), GFP_KERNEL);
 		if (!q->flows)
 			return -ENOMEM;
 		q->backlogs = kvzalloc(q->flows_cnt * sizeof(u32), GFP_KERNEL);
 		if (!q->backlogs)
 			return -ENOMEM;
 		for (i = 0; i < q->flows_cnt; i++) {
 			struct fq_codel_flow *flow = q->flows + i;
 			INIT_LIST_HEAD(&flow->flowchain);
 			codel_vars_init(&flow->cvars);
 		}
 	}
 	if (sch->limit >= 1)
 		sch->flags |= TCQ_F_CAN_BYPASS;
 	else
 		sch->flags &= ~TCQ_F_CAN_BYPASS;
 	return 0;
 }",46,338
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1513791319_2017-12-20_8d1a77f974ca_sch_sfb_sfb_init,11,11," static int sfb_init(struct Qdisc *sch, struct nlattr *opt,
 		    struct netlink_ext_ack *extack)
 {
 	struct sfb_sched_data *q = qdisc_priv(sch);
 	int err;
-	err = tcf_block_get(&q->block, &q->filter_list, sch);
+	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
 	if (err)
 		return err;
 	q->qdisc = &noop_qdisc;
 	return sfb_change(sch, opt, extack);
 }",12,96
tcf_block_get-61,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcf_block_get-61/1507896057_2017-10-13_69d78ef25c7b_sch_multiq_multiq_init,18,18," static int multiq_init(struct Qdisc *sch, struct nlattr *opt)
 {
 	struct multiq_sched_data *q = qdisc_priv(sch);
 	int i, err;
 	q->queues = NULL;
 	if (opt == NULL)
 		return -EINVAL;
-	err = tcf_block_get(&q->block, &q->filter_list);
+	err = tcf_block_get(&q->block, &q->filter_list, sch);
 	if (err)
 		return err;
 	q->max_bands = qdisc_dev(sch)->num_tx_queues;
 	q->queues = kcalloc(q->max_bands, sizeof(struct Qdisc *), GFP_KERNEL);
 	if (!q->queues)
 		return -ENOBUFS;
 	for (i = 0; i < q->max_bands; i++)
 		q->queues[i] = &noop_qdisc;
 	return multiq_tune(sch, opt);
 }",19,163
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_llcp_sock_llcp_sock_poll,27,27," static __poll_t llcp_sock_poll(struct file *file, struct socket *sock,
 				   poll_table *wait)
 {
 	struct sock *sk = sock->sk;
 	__poll_t mask = 0;
 	pr_debug(""%p\n"", sk);
-	sock_poll_wait(file, wait);
+	sock_poll_wait(file, sock, wait);
 	if (sk->sk_state == LLCP_LISTEN)
 		return llcp_accept_poll(sk);
 	if (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue))
 		mask |= EPOLLERR |
 			(sock_flag(sk, SOCK_SELECT_ERR_QUEUE) ? EPOLLPRI : 0);
 	if (!skb_queue_empty(&sk->sk_receive_queue))
 		mask |= EPOLLIN | EPOLLRDNORM;
 	if (sk->sk_state == LLCP_CLOSED)
 		mask |= EPOLLHUP;
 	if (sk->sk_shutdown & RCV_SHUTDOWN)
 		mask |= EPOLLRDHUP | EPOLLIN | EPOLLRDNORM;
 	if (sk->sk_shutdown == SHUTDOWN_MASK)
 		mask |= EPOLLHUP;
 	if (sock_writeable(sk) && sk->sk_state == LLCP_CONNECTED)
 		mask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;
 	else
 		sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
 	pr_debug(""mask 0x%x\n"", mask);
 	return mask;
 }",28,207
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_tcp_tcp_poll,40,40," __poll_t tcp_poll(struct file *file, struct socket *sock, poll_table *wait) {
   __poll_t mask;
   struct sock *sk = sock->sk;
   const struct tcp_sock *tp = tcp_sk(sk);
   int state;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   state = inet_sk_state_load(sk);
   if (state == TCP_LISTEN) return inet_csk_listen_poll(sk);
   
   mask = 0;
   
   if (sk->sk_shutdown == SHUTDOWN_MASK || state == TCP_CLOSE) mask |= EPOLLHUP;
   if (sk->sk_shutdown & RCV_SHUTDOWN) mask |= EPOLLIN | EPOLLRDNORM | EPOLLRDHUP;
   
   if (state != TCP_SYN_SENT && (state != TCP_SYN_RECV || tp->fastopen_rsk)) {
     int target = sock_rcvlowat(sk, 0, INT_MAX);
     if (tp->urg_seq == tp->copied_seq && !sock_flag(sk, SOCK_URGINLINE) && tp->urg_data) target++;
     if (tcp_stream_is_readable(tp, target, sk)) mask |= EPOLLIN | EPOLLRDNORM;
     if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
       if (sk_stream_is_writeable(sk)) {
         mask |= EPOLLOUT | EPOLLWRNORM;
       } else { 
         sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
         set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
         
         smp_mb__after_atomic();
         if (sk_stream_is_writeable(sk)) mask |= EPOLLOUT | EPOLLWRNORM;
       }
     } else
       mask |= EPOLLOUT | EPOLLWRNORM;
     if (tp->urg_data & TCP_URG_VALID) mask |= EPOLLPRI;
   } else if (state == TCP_SYN_SENT && inet_sk(sk)->defer_connect) {
     
     mask |= EPOLLOUT | EPOLLWRNORM;
   }
   
   smp_rmb();
   if (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue)) mask |= EPOLLERR;
   return mask;
 }",41,324
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_af_rxrpc_rxrpc_poll,16,16," static __poll_t rxrpc_poll(struct file *file, struct socket *sock,
 			       poll_table *wait)
 {
 	struct sock *sk = sock->sk;
 	struct rxrpc_sock *rx = rxrpc_sk(sk);
 	__poll_t mask;
-	sock_poll_wait(file, wait);
+	sock_poll_wait(file, sock, wait);
 	mask = 0;
 	
 	if (!list_empty(&rx->recvmsg_q))
 		mask |= EPOLLIN | EPOLLRDNORM;
 	
 	if (rxrpc_writable(sk))
 		mask |= EPOLLOUT | EPOLLWRNORM;
 	return mask;
 }",17,97
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_af_unix_unix_poll,17,17," static __poll_t unix_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk;
   __poll_t mask;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   mask = 0;
   
   if (sk->sk_err) mask |= EPOLLERR;
   if (sk->sk_shutdown == SHUTDOWN_MASK) mask |= EPOLLHUP;
   if (sk->sk_shutdown & RCV_SHUTDOWN) mask |= EPOLLRDHUP | EPOLLIN | EPOLLRDNORM;
   
   if (!skb_queue_empty(&sk->sk_receive_queue)) mask |= EPOLLIN | EPOLLRDNORM;
   
   if ((sk->sk_type == SOCK_STREAM || sk->sk_type == SOCK_SEQPACKET) && sk->sk_state == TCP_CLOSE) mask |= EPOLLHUP;
   
   if (unix_writable(sk)) mask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;
   return mask;
 }",18,153
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_datagram_datagram_poll,24,24," __poll_t datagram_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk;
   __poll_t mask;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   mask = 0;
   
   if (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue)) mask |= EPOLLERR | (sock_flag(sk, SOCK_SELECT_ERR_QUEUE) ? EPOLLPRI : 0);
   if (sk->sk_shutdown & RCV_SHUTDOWN) mask |= EPOLLRDHUP | EPOLLIN | EPOLLRDNORM;
   if (sk->sk_shutdown == SHUTDOWN_MASK) mask |= EPOLLHUP;
   
   if (!skb_queue_empty(&sk->sk_receive_queue)) mask |= EPOLLIN | EPOLLRDNORM;
   
   if (connection_based(sk)) {
     if (sk->sk_state == TCP_CLOSE) mask |= EPOLLHUP;
     
     if (sk->sk_state == TCP_SYN_SENT) return mask;
   }
   
   if (sock_writeable(sk))
     mask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;
   else
     sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
   return mask;
 }",25,188
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_af_alg_af_alg_poll,11,11," __poll_t af_alg_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk;
   struct alg_sock *ask = alg_sk(sk);
   struct af_alg_ctx *ctx = ask->private;
   __poll_t mask;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   mask = 0;
   if (!ctx->more || ctx->used) mask |= EPOLLIN | EPOLLRDNORM;
   if (af_alg_writable(sk)) mask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;
   return mask;
 }",12,107
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_proto_dccp_poll,26,26," __poll_t dccp_poll(struct file *file, struct socket *sock, poll_table *wait) {
   __poll_t mask;
   struct sock *sk = sock->sk;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   if (sk->sk_state == DCCP_LISTEN) return inet_csk_listen_poll(sk);
   
   mask = 0;
   if (sk->sk_err) mask = EPOLLERR;
   if (sk->sk_shutdown == SHUTDOWN_MASK || sk->sk_state == DCCP_CLOSED) mask |= EPOLLHUP;
   if (sk->sk_shutdown & RCV_SHUTDOWN) mask |= EPOLLIN | EPOLLRDNORM | EPOLLRDHUP;
   
   if ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_RESPOND)) {
     if (atomic_read(&sk->sk_rmem_alloc) > 0) mask |= EPOLLIN | EPOLLRDNORM;
     if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
       if (sk_stream_is_writeable(sk)) {
         mask |= EPOLLOUT | EPOLLWRNORM;
       } else { 
         sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
         set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
         
         if (sk_stream_is_writeable(sk)) mask |= EPOLLOUT | EPOLLWRNORM;
       }
     }
   }
   return mask;
 }",27,214
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_af_unix_unix_dgram_poll,33,33," static __poll_t unix_dgram_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk, *other;
   unsigned int writable;
   __poll_t mask;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   mask = 0;
   
   if (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue)) mask |= EPOLLERR | (sock_flag(sk, SOCK_SELECT_ERR_QUEUE) ? EPOLLPRI : 0);
   if (sk->sk_shutdown & RCV_SHUTDOWN) mask |= EPOLLRDHUP | EPOLLIN | EPOLLRDNORM;
   if (sk->sk_shutdown == SHUTDOWN_MASK) mask |= EPOLLHUP;
   
   if (!skb_queue_empty(&sk->sk_receive_queue)) mask |= EPOLLIN | EPOLLRDNORM;
   
   if (sk->sk_type == SOCK_SEQPACKET) {
     if (sk->sk_state == TCP_CLOSE) mask |= EPOLLHUP;
     
     if (sk->sk_state == TCP_SYN_SENT) return mask;
   }
   
   if (!(poll_requested_events(wait) & (EPOLLWRBAND | EPOLLWRNORM | EPOLLOUT))) return mask;
   writable = unix_writable(sk);
   if (writable) {
     unix_state_lock(sk);
     other = unix_peer(sk);
     if (other && unix_peer(other) != sk && unix_recvq_full(other) && unix_dgram_peer_wake_me(sk, other)) writable = 0;
     unix_state_unlock(sk);
   }
   if (writable)
     mask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;
   else
     sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
   return mask;
 }",34,272
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_socket_tipc_poll,25,25," static __poll_t tipc_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk;
   struct tipc_sock *tsk = tipc_sk(sk);
   __poll_t revents = 0;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   if (sk->sk_shutdown & RCV_SHUTDOWN) revents |= EPOLLRDHUP | EPOLLIN | EPOLLRDNORM;
   if (sk->sk_shutdown == SHUTDOWN_MASK) revents |= EPOLLHUP;
   switch (sk->sk_state) {
   case TIPC_ESTABLISHED:
   case TIPC_CONNECTING:
     if (!tsk->cong_link_cnt && !tsk_conn_cong(tsk)) revents |= EPOLLOUT;
     
   case TIPC_LISTEN:
     if (!skb_queue_empty(&sk->sk_receive_queue)) revents |= EPOLLIN | EPOLLRDNORM;
     break;
   case TIPC_OPEN:
     if (tsk->group_is_open && !tsk->cong_link_cnt) revents |= EPOLLOUT;
     if (!tipc_sk_type_connectionless(sk)) break;
     if (skb_queue_empty(&sk->sk_receive_queue)) break;
     revents |= EPOLLIN | EPOLLRDNORM;
     break;
   case TIPC_DISCONNECTING: revents = EPOLLIN | EPOLLRDNORM | EPOLLHUP; break;
   }
   return revents;
 }",26,207
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_caif_socket_caif_poll,16,16," static __poll_t caif_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk;
   __poll_t mask;
   struct caifsock *cf_sk = container_of(sk, struct caifsock, sk);
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   mask = 0;
   
   if (sk->sk_err) mask |= EPOLLERR;
   if (sk->sk_shutdown == SHUTDOWN_MASK) mask |= EPOLLHUP;
   if (sk->sk_shutdown & RCV_SHUTDOWN) mask |= EPOLLRDHUP;
   
   if (!skb_queue_empty(&sk->sk_receive_queue) || (sk->sk_shutdown & RCV_SHUTDOWN)) mask |= EPOLLIN | EPOLLRDNORM;
   
   if (sock_writeable(sk) && tx_flow_is_on(cf_sk)) mask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;
   return mask;
 }",17,151
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_af_smc_smc_poll,33,33," static __poll_t smc_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk;
   __poll_t mask = 0;
   struct smc_sock *smc;
   if (!sk) return EPOLLNVAL;
   smc = smc_sk(sock->sk);
   if (smc->use_fallback) {
     
     mask = smc->clcsock->ops->poll(file, smc->clcsock, wait);
     sk->sk_err = smc->clcsock->sk->sk_err;
     if (sk->sk_err) mask |= EPOLLERR;
   } else {
-    if (sk->sk_state != SMC_CLOSED) sock_poll_wait(file, wait);
+    if (sk->sk_state != SMC_CLOSED) sock_poll_wait(file, sock, wait);
     if (sk->sk_err) mask |= EPOLLERR;
     if ((sk->sk_shutdown == SHUTDOWN_MASK) || (sk->sk_state == SMC_CLOSED)) mask |= EPOLLHUP;
     if (sk->sk_state == SMC_LISTEN) {
       
       mask = smc_accept_poll(sk);
     } else {
       if (atomic_read(&smc->conn.sndbuf_space) || sk->sk_shutdown & SEND_SHUTDOWN) {
         mask |= EPOLLOUT | EPOLLWRNORM;
       } else {
         sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
         set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
       }
       if (atomic_read(&smc->conn.bytes_to_rcv)) mask |= EPOLLIN | EPOLLRDNORM;
       if (sk->sk_shutdown & RCV_SHUTDOWN) mask |= EPOLLIN | EPOLLRDNORM | EPOLLRDHUP;
       if (sk->sk_state == SMC_APPCLOSEWAIT1) mask |= EPOLLIN;
       if (smc->conn.urg_state == SMC_URG_VALID) mask |= EPOLLPRI;
     }
   }
   return mask;
 }",34,307
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_common_vcc_poll,17,17," __poll_t vcc_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk;
   struct atm_vcc *vcc;
   __poll_t mask;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   mask = 0;
   vcc = ATM_SD(sock);
   
   if (sk->sk_err) mask = EPOLLERR;
   if (test_bit(ATM_VF_RELEASED, &vcc->flags) || test_bit(ATM_VF_CLOSE, &vcc->flags)) mask |= EPOLLHUP;
   
   if (!skb_queue_empty(&sk->sk_receive_queue)) mask |= EPOLLIN | EPOLLRDNORM;
   
   if (sock->state == SS_CONNECTING && test_bit(ATM_VF_WAITING, &vcc->flags)) return mask;
   if (vcc->qos.txtp.traffic_class != ATM_NONE && vcc_writable(sk)) mask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;
   return mask;
 }",18,167
sock_poll_wait-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/sock_poll_wait-84/1540294839_2018-10-23_89ab066d4229_af_iucv_iucv_sock_poll,17,17," __poll_t iucv_sock_poll(struct file *file, struct socket *sock, poll_table *wait) {
   struct sock *sk = sock->sk;
   __poll_t mask = 0;
-  sock_poll_wait(file, wait);
+  sock_poll_wait(file, sock, wait);
   if (sk->sk_state == IUCV_LISTEN) return iucv_accept_poll(sk);
   if (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue)) mask |= EPOLLERR | (sock_flag(sk, SOCK_SELECT_ERR_QUEUE) ? EPOLLPRI : 0);
   if (sk->sk_shutdown & RCV_SHUTDOWN) mask |= EPOLLRDHUP;
   if (sk->sk_shutdown == SHUTDOWN_MASK) mask |= EPOLLHUP;
   if (!skb_queue_empty(&sk->sk_receive_queue) || (sk->sk_shutdown & RCV_SHUTDOWN)) mask |= EPOLLIN | EPOLLRDNORM;
   if (sk->sk_state == IUCV_CLOSED) mask |= EPOLLHUP;
   if (sk->sk_state == IUCV_DISCONN) mask |= EPOLLIN;
   if (sock_writeable(sk) && iucv_below_msglim(sk))
     mask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;
   else
     sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
   return mask;
 }",18,201
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_qlcnic_sriov_common_qlcnic_sriov_init,81,81," int qlcnic_sriov_init(struct qlcnic_adapter *adapter, int num_vfs)
 {
 	struct qlcnic_sriov *sriov;
 	struct qlcnic_back_channel *bc;
 	struct workqueue_struct *wq;
 	struct qlcnic_vport *vp;
 	struct qlcnic_vf_info *vf;
 	int err, i;
 	if (!qlcnic_sriov_enable_check(adapter))
 		return -EIO;
 	sriov  = kzalloc(sizeof(struct qlcnic_sriov), GFP_KERNEL);
 	if (!sriov)
 		return -ENOMEM;
 	adapter->ahw->sriov = sriov;
 	sriov->num_vfs = num_vfs;
 	bc = &sriov->bc;
 	sriov->vf_info = kcalloc(num_vfs, sizeof(struct qlcnic_vf_info),
 				 GFP_KERNEL);
 	if (!sriov->vf_info) {
 		err = -ENOMEM;
 		goto qlcnic_free_sriov;
 	}
 	wq = create_singlethread_workqueue(""bc-trans"");
 	if (wq == NULL) {
 		err = -ENOMEM;
 		dev_err(&adapter->pdev->dev,
 			""Cannot create bc-trans workqueue\n"");
 		goto qlcnic_free_vf_info;
 	}
 	bc->bc_trans_wq = wq;
 	wq = create_singlethread_workqueue(""async"");
 	if (wq == NULL) {
 		err = -ENOMEM;
 		dev_err(&adapter->pdev->dev, ""Cannot create async workqueue\n"");
 		goto qlcnic_destroy_trans_wq;
 	}
 	bc->bc_async_wq =  wq;
 	INIT_LIST_HEAD(&bc->async_cmd_list);
 	INIT_WORK(&bc->vf_async_work, qlcnic_sriov_handle_async_issue_cmd);
 	spin_lock_init(&bc->queue_lock);
 	bc->adapter = adapter;
 	for (i = 0; i < num_vfs; i++) {
 		vf = &sriov->vf_info[i];
 		vf->adapter = adapter;
 		vf->pci_func = qlcnic_sriov_virtid_fn(adapter, i);
 		mutex_init(&vf->send_cmd_lock);
 		spin_lock_init(&vf->vlan_list_lock);
 		INIT_LIST_HEAD(&vf->rcv_act.wait_list);
 		INIT_LIST_HEAD(&vf->rcv_pend.wait_list);
 		spin_lock_init(&vf->rcv_act.lock);
 		spin_lock_init(&vf->rcv_pend.lock);
 		init_completion(&vf->ch_free_cmpl);
 		INIT_WORK(&vf->trans_work, qlcnic_sriov_process_bc_cmd);
 		if (qlcnic_sriov_pf_check(adapter)) {
 			vp = kzalloc(sizeof(struct qlcnic_vport), GFP_KERNEL);
 			if (!vp) {
 				err = -ENOMEM;
 				goto qlcnic_destroy_async_wq;
 			}
 			sriov->vf_info[i].vp = vp;
 			vp->vlan_mode = QLC_GUEST_VLAN_MODE;
 			vp->max_tx_bw = MAX_BW;
 			vp->min_tx_bw = MIN_BW;
 			vp->spoofchk = false;
-			random_ether_addr(vp->mac);
+			eth_random_addr(vp->mac);
 			dev_info(&adapter->pdev->dev,
 				 ""MAC Address %pM is configured for VF %d\n"",
 				 vp->mac, i);
 		}
 	}
 	return 0;
 qlcnic_destroy_async_wq:
 	destroy_workqueue(bc->bc_async_wq);
 qlcnic_destroy_trans_wq:
 	destroy_workqueue(bc->bc_trans_wq);
 qlcnic_free_vf_info:
 	kfree(sriov->vf_info);
 qlcnic_free_sriov:
 	kfree(adapter->ahw->sriov);
 	return err;
 }",82,525
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_rmnet_vnd_rmnet_vnd_setup,18,18," void rmnet_vnd_setup(struct net_device *rmnet_dev)
 {
 	rmnet_dev->netdev_ops = &rmnet_vnd_ops;
 	rmnet_dev->mtu = RMNET_DFLT_PACKET_SIZE;
 	rmnet_dev->needed_headroom = RMNET_NEEDED_HEADROOM;
-	random_ether_addr(rmnet_dev->dev_addr);
+	eth_random_addr(rmnet_dev->dev_addr);
 	rmnet_dev->tx_queue_len = RMNET_TX_QUEUE_LEN;
 	
 	rmnet_dev->header_ops = NULL;  
 	rmnet_dev->type = ARPHRD_RAWIP;
 	rmnet_dev->hard_header_len = 0;
 	rmnet_dev->flags &= ~(IFF_BROADCAST | IFF_MULTICAST);
 	rmnet_dev->needs_free_netdev = true;
 	rmnet_dev->ethtool_ops = &rmnet_ethtool_ops;
 	
 	rmnet_dev->addr_assign_type = NET_ADDR_RANDOM;
 	eth_random_addr(rmnet_dev->perm_addr);
 }",19,106
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_gemini_gemini_ethernet_port_probe,130,130," static int gemini_ethernet_port_probe(struct platform_device *pdev)
 {
 	char *port_names[2] = { ""ethernet0"", ""ethernet1"" };
 	struct gemini_ethernet_port *port;
 	struct device *dev = &pdev->dev;
 	struct gemini_ethernet *geth;
 	struct net_device *netdev;
 	struct resource *gmacres;
 	struct resource *dmares;
 	struct device *parent;
 	unsigned int id;
 	int irq;
 	int ret;
 	parent = dev->parent;
 	geth = dev_get_drvdata(parent);
 	if (!strcmp(dev_name(dev), ""60008000.ethernet-port""))
 		id = 0;
 	else if (!strcmp(dev_name(dev), ""6000c000.ethernet-port""))
 		id = 1;
 	else
 		return -ENODEV;
 	dev_info(dev, ""probe %s ID %d\n"", dev_name(dev), id);
 	netdev = alloc_etherdev_mq(sizeof(*port), TX_QUEUE_NUM);
 	if (!netdev) {
 		dev_err(dev, ""Can't allocate ethernet device #%d\n"", id);
 		return -ENOMEM;
 	}
 	port = netdev_priv(netdev);
 	SET_NETDEV_DEV(netdev, dev);
 	port->netdev = netdev;
 	port->id = id;
 	port->geth = geth;
 	port->dev = dev;
 	
 	dmares = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	if (!dmares) {
 		dev_err(dev, ""no DMA resource\n"");
 		return -ENODEV;
 	}
 	port->dma_base = devm_ioremap_resource(dev, dmares);
 	if (IS_ERR(port->dma_base))
 		return PTR_ERR(port->dma_base);
 	
 	gmacres = platform_get_resource(pdev, IORESOURCE_MEM, 1);
 	if (!gmacres) {
 		dev_err(dev, ""no GMAC resource\n"");
 		return -ENODEV;
 	}
 	port->gmac_base = devm_ioremap_resource(dev, gmacres);
 	if (IS_ERR(port->gmac_base))
 		return PTR_ERR(port->gmac_base);
 	
 	irq = platform_get_irq(pdev, 0);
 	if (irq <= 0) {
 		dev_err(dev, ""no IRQ\n"");
 		return irq ? irq : -ENODEV;
 	}
 	port->irq = irq;
 	
 	port->pclk = devm_clk_get(dev, ""PCLK"");
 	if (IS_ERR(port->pclk)) {
 		dev_err(dev, ""no PCLK\n"");
 		return PTR_ERR(port->pclk);
 	}
 	ret = clk_prepare_enable(port->pclk);
 	if (ret)
 		return ret;
 	
 	gemini_port_save_mac_addr(port);
 	
 	port->reset = devm_reset_control_get_exclusive(dev, NULL);
 	if (IS_ERR(port->reset)) {
 		dev_err(dev, ""no reset\n"");
 		return PTR_ERR(port->reset);
 	}
 	reset_control_reset(port->reset);
 	usleep_range(100, 500);
 	
 	if (!id)
 		geth->port0 = port;
 	else
 		geth->port1 = port;
 	platform_set_drvdata(pdev, port);
 	
 	netdev->dev_id = port->id;
 	netdev->irq = irq;
 	netdev->netdev_ops = &gmac_351x_ops;
 	netdev->ethtool_ops = &gmac_351x_ethtool_ops;
 	spin_lock_init(&port->config_lock);
 	gmac_clear_hw_stats(netdev);
 	netdev->hw_features = GMAC_OFFLOAD_FEATURES;
 	netdev->features |= GMAC_OFFLOAD_FEATURES | NETIF_F_GRO;
 	port->freeq_refill = 0;
 	netif_napi_add(netdev, &port->napi, gmac_napi_poll,
 		       DEFAULT_NAPI_WEIGHT);
 	if (is_valid_ether_addr((void *)port->mac_addr)) {
 		memcpy(netdev->dev_addr, port->mac_addr, ETH_ALEN);
 	} else {
 		dev_dbg(dev, ""ethernet address 0x%08x%08x%08x invalid\n"",
 			port->mac_addr[0], port->mac_addr[1],
 			port->mac_addr[2]);
 		dev_info(dev, ""using a random ethernet address\n"");
-		random_ether_addr(netdev->dev_addr);
+		eth_random_addr(netdev->dev_addr);
 	}
 	gmac_write_mac_address(netdev);
 	ret = devm_request_threaded_irq(port->dev,
 					port->irq,
 					gemini_port_irq,
 					gemini_port_irq_thread,
 					IRQF_SHARED,
 					port_names[port->id],
 					port);
 	if (ret)
 		return ret;
 	ret = register_netdev(netdev);
 	if (!ret) {
 		netdev_info(netdev,
 			    ""irq %d, DMA @ 0x%pap, GMAC @ 0x%pap\n"",
 			    port->irq, &dmares->start,
 			    &gmacres->start);
 		ret = gmac_setup_phy(netdev);
 		if (ret)
 			netdev_info(netdev,
 				    ""PHY init failed, deferring to ifup time\n"");
 		return 0;
 	}
 	port->netdev = NULL;
 	free_netdev(netdev);
 	return ret;
 }",131,808
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_ef10_sriov_efx_ef10_sriov_alloc_vf_vswitching,24,24," static int efx_ef10_sriov_alloc_vf_vswitching(struct efx_nic *efx)
 {
 	struct efx_ef10_nic_data *nic_data = efx->nic_data;
 	unsigned int i;
 	int rc;
 	nic_data->vf = kcalloc(efx->vf_count, sizeof(struct ef10_vf),
 			       GFP_KERNEL);
 	if (!nic_data->vf)
 		return -ENOMEM;
 	for (i = 0; i < efx->vf_count; i++) {
-		random_ether_addr(nic_data->vf[i].mac);
+		eth_random_addr(nic_data->vf[i].mac);
 		nic_data->vf[i].efx = NULL;
 		nic_data->vf[i].vlan = EFX_EF10_NO_VLAN;
 		rc = efx_ef10_sriov_assign_vf_vport(efx, i);
 		if (rc)
 			goto fail;
 	}
 	return 0;
 fail:
 	efx_ef10_sriov_free_vf_vports(efx);
 	kfree(nic_data->vf);
 	nic_data->vf = NULL;
 	return rc;
 }",25,164
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_lan78xx_lan78xx_init_mac_address,42,42," static void lan78xx_init_mac_address(struct lan78xx_net *dev)
 {
 	u32 addr_lo, addr_hi;
 	int ret;
 	u8 addr[6];
 	ret = lan78xx_read_reg(dev, RX_ADDRL, &addr_lo);
 	ret = lan78xx_read_reg(dev, RX_ADDRH, &addr_hi);
 	addr[0] = addr_lo & 0xFF;
 	addr[1] = (addr_lo >> 8) & 0xFF;
 	addr[2] = (addr_lo >> 16) & 0xFF;
 	addr[3] = (addr_lo >> 24) & 0xFF;
 	addr[4] = addr_hi & 0xFF;
 	addr[5] = (addr_hi >> 8) & 0xFF;
 	if (!is_valid_ether_addr(addr)) {
 		if (!eth_platform_get_mac_address(&dev->udev->dev, addr)) {
 			
 			netif_dbg(dev, ifup, dev->net,
 				  ""MAC address read from Device Tree"");
 		} else if (((lan78xx_read_eeprom(dev, EEPROM_MAC_OFFSET,
 						 ETH_ALEN, addr) == 0) ||
 			    (lan78xx_read_otp(dev, EEPROM_MAC_OFFSET,
 					      ETH_ALEN, addr) == 0)) &&
 			   is_valid_ether_addr(addr)) {
 			
 			netif_dbg(dev, ifup, dev->net,
 				  ""MAC address read from EEPROM"");
 		} else {
 			
-			random_ether_addr(addr);
+			eth_random_addr(addr);
 			netif_dbg(dev, ifup, dev->net,
 				  ""MAC address set to random addr"");
 		}
 		addr_lo = addr[0] | (addr[1] << 8) |
 			  (addr[2] << 16) | (addr[3] << 24);
 		addr_hi = addr[4] | (addr[5] << 8);
 		ret = lan78xx_write_reg(dev, RX_ADDRL, addr_lo);
 		ret = lan78xx_write_reg(dev, RX_ADDRH, addr_hi);
 	}
 	ret = lan78xx_write_reg(dev, MAF_LO(0), addr_lo);
 	ret = lan78xx_write_reg(dev, MAF_HI(0), addr_hi | MAF_HI_VALID_);
 	ether_addr_copy(dev->net->dev_addr, addr);
 }",43,361
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_ntb_netdev_ntb_netdev_probe,47,47," static int ntb_netdev_probe(struct device *client_dev)
 {
 	struct ntb_dev *ntb;
 	struct net_device *ndev;
 	struct pci_dev *pdev;
 	struct ntb_netdev *dev;
 	int rc;
 	ntb = dev_ntb(client_dev->parent);
 	pdev = ntb->pdev;
 	if (!pdev)
 		return -ENODEV;
 	ndev = alloc_etherdev(sizeof(*dev));
 	if (!ndev)
 		return -ENOMEM;
 	SET_NETDEV_DEV(ndev, client_dev);
 	dev = netdev_priv(ndev);
 	dev->ndev = ndev;
 	dev->pdev = pdev;
 	ndev->features = NETIF_F_HIGHDMA;
 	ndev->priv_flags |= IFF_LIVE_ADDR_CHANGE;
 	ndev->hw_features = ndev->features;
 	ndev->watchdog_timeo = msecs_to_jiffies(NTB_TX_TIMEOUT_MS);
-	random_ether_addr(ndev->perm_addr);
+	eth_random_addr(ndev->perm_addr);
 	memcpy(ndev->dev_addr, ndev->perm_addr, ndev->addr_len);
 	ndev->netdev_ops = &ntb_netdev_ops;
 	ndev->ethtool_ops = &ntb_ethtool_ops;
 	ndev->min_mtu = 0;
 	ndev->max_mtu = ETH_MAX_MTU;
 	dev->qp = ntb_transport_create_queue(ndev, client_dev,
 					     &ntb_netdev_handlers);
 	if (!dev->qp) {
 		rc = -EIO;
 		goto err;
 	}
 	ndev->mtu = ntb_transport_max_size(dev->qp) - ETH_HLEN;
 	rc = register_netdev(ndev);
 	if (rc)
 		goto err1;
 	list_add(&dev->list, &dev_list);
 	dev_info(&pdev->dev, ""%s created\n"", ndev->name);
 	return 0;
 err1:
 	ntb_transport_free_queue(dev->qp);
 err:
 	free_netdev(ndev);
 	return rc;
 }",48,298
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529774878_2018-06-23_6d2b0f7b9c1e_ieee80211_softmac_ieee80211_start_ibss_wq,69,69," static void ieee80211_start_ibss_wq(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
 	struct ieee80211_device *ieee = container_of(dwork, struct ieee80211_device, start_ibss_wq);
 	
 	if (!ieee->proto_started) {
 		printk(""==========oh driver down return\n"");
 		return;
 	}
 	mutex_lock(&ieee->wx_mutex);
 	if (ieee->current_network.ssid_len == 0) {
 		strcpy(ieee->current_network.ssid, IEEE80211_DEFAULT_TX_ESSID);
 		ieee->current_network.ssid_len = strlen(IEEE80211_DEFAULT_TX_ESSID);
 		ieee->ssid_set = 1;
 	}
 	
 	ieee80211_softmac_check_all_nets(ieee);
 
 	if (ieee->state == IEEE80211_NOLINK)
 		ieee->current_network.channel = 6;
 	
 	if (ieee->state == IEEE80211_NOLINK)
 		ieee80211_start_scan_syncro(ieee);
 	
 	if (ieee->state == IEEE80211_NOLINK) {
 		printk(""creating new IBSS cell\n"");
 		if(!ieee->wap_set)
-			random_ether_addr(ieee->current_network.bssid);
+			eth_random_addr(ieee->current_network.bssid);
 		if(ieee->modulation & IEEE80211_CCK_MODULATION){
 			ieee->current_network.rates_len = 4;
 			ieee->current_network.rates[0] = IEEE80211_BASIC_RATE_MASK | IEEE80211_CCK_RATE_1MB;
 			ieee->current_network.rates[1] = IEEE80211_BASIC_RATE_MASK | IEEE80211_CCK_RATE_2MB;
 			ieee->current_network.rates[2] = IEEE80211_BASIC_RATE_MASK | IEEE80211_CCK_RATE_5MB;
 			ieee->current_network.rates[3] = IEEE80211_BASIC_RATE_MASK | IEEE80211_CCK_RATE_11MB;
 		}else
 			ieee->current_network.rates_len = 0;
 		if(ieee->modulation & IEEE80211_OFDM_MODULATION){
 			ieee->current_network.rates_ex_len = 8;
 			ieee->current_network.rates_ex[0] = IEEE80211_BASIC_RATE_MASK | IEEE80211_OFDM_RATE_6MB;
 			ieee->current_network.rates_ex[1] = IEEE80211_BASIC_RATE_MASK | IEEE80211_OFDM_RATE_9MB;
 			ieee->current_network.rates_ex[2] = IEEE80211_BASIC_RATE_MASK | IEEE80211_OFDM_RATE_12MB;
 			ieee->current_network.rates_ex[3] = IEEE80211_BASIC_RATE_MASK | IEEE80211_OFDM_RATE_18MB;
 			ieee->current_network.rates_ex[4] = IEEE80211_BASIC_RATE_MASK | IEEE80211_OFDM_RATE_24MB;
 			ieee->current_network.rates_ex[5] = IEEE80211_BASIC_RATE_MASK | IEEE80211_OFDM_RATE_36MB;
 			ieee->current_network.rates_ex[6] = IEEE80211_BASIC_RATE_MASK | IEEE80211_OFDM_RATE_48MB;
 			ieee->current_network.rates_ex[7] = IEEE80211_BASIC_RATE_MASK | IEEE80211_OFDM_RATE_54MB;
 			ieee->rate = 108;
 		}else{
 			ieee->current_network.rates_ex_len = 0;
 			ieee->rate = 22;
 		}
 		
 		ieee->current_network.QoS_Enable = 0;
 		ieee->SetWirelessMode(ieee->dev, IEEE_G);
 		ieee->current_network.atim_window = 0;
 		ieee->current_network.capability = WLAN_CAPABILITY_IBSS;
 		if(ieee->short_slot)
 			ieee->current_network.capability |= WLAN_CAPABILITY_SHORT_SLOT;
 	}
 	ieee->state = IEEE80211_LINKED;
 	ieee->set_chan(ieee->dev, ieee->current_network.channel);
 	ieee->link_change(ieee->dev);
 	notify_wx_assoc_event(ieee);
 	ieee80211_start_send_beacons(ieee);
 	if (ieee->data_hard_resume)
 		ieee->data_hard_resume(ieee->dev);
 	netif_carrier_on(ieee->dev);
 	mutex_unlock(&ieee->wx_mutex);
 }",70,525
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529735368_2018-06-22_8e5c88bfbb8c_board_ath25_find_config,69,69," int __init ath25_find_config(phys_addr_t base, unsigned long size)
 {
 	const void __iomem *flash_base, *flash_limit;
 	struct ath25_boarddata *config;
 	unsigned int rcfg_size;
 	int broken_boarddata = 0;
 	const void __iomem *bcfg, *rcfg;
 	u8 *board_data;
 	u8 *radio_data;
 	u8 *mac_addr;
 	u32 offset;
 	flash_base = ioremap_nocache(base, size);
 	flash_limit = flash_base + size;
 	ath25_board.config = NULL;
 	ath25_board.radio = NULL;
 	
 	
 	bcfg = find_board_config(flash_limit, false);
 	
 	if (!bcfg) {
 		bcfg = find_board_config(flash_limit, true);
 		broken_boarddata = 1;
 	}
 	if (!bcfg) {
 		pr_warn(""WARNING: No board configuration data found!\n"");
 		goto error;
 	}
 	board_data = kzalloc(BOARD_CONFIG_BUFSZ, GFP_KERNEL);
 	if (!board_data)
 		goto error;
 	ath25_board.config = (struct ath25_boarddata *)board_data;
 	memcpy_fromio(board_data, bcfg, 0x100);
 	if (broken_boarddata) {
 		pr_warn(""WARNING: broken board data detected\n"");
 		config = ath25_board.config;
 		if (is_zero_ether_addr(config->enet0_mac)) {
 			pr_info(""Fixing up empty mac addresses\n"");
 			config->reset_config_gpio = 0xffff;
 			config->sys_led_gpio = 0xffff;
-			random_ether_addr(config->wlan0_mac);
+			eth_random_addr(config->wlan0_mac);
 			config->wlan0_mac[0] &= ~0x06;
-			random_ether_addr(config->enet0_mac);
-			random_ether_addr(config->enet1_mac);
+			eth_random_addr(config->enet0_mac);
+			eth_random_addr(config->enet1_mac);
 		}
 	}
 	
 	rcfg = find_radio_config(flash_limit, bcfg);
 	if (!rcfg) {
 		pr_warn(""WARNING: Could not find Radio Configuration data\n"");
 		goto error;
 	}
 	radio_data = board_data + 0x100 + ((rcfg - bcfg) & 0xfff);
 	ath25_board.radio = radio_data;
 	offset = radio_data - board_data;
 	pr_info(""Radio config found at offset 0x%x (0x%x)\n"", rcfg - bcfg,
 		offset);
 	rcfg_size = BOARD_CONFIG_BUFSZ - offset;
 	memcpy_fromio(radio_data, rcfg, rcfg_size);
 	mac_addr = &radio_data[0x1d * 2];
 	if (is_broadcast_ether_addr(mac_addr)) {
 		pr_info(""Radio MAC is blank; using board-data\n"");
 		ether_addr_copy(mac_addr, ath25_board.config->wlan0_mac);
 	}
 	iounmap(flash_base);
 	return 0;
 error:
 	iounmap(flash_base);
 	return -ENODEV;
 }",72,421
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_i40e_main_i40e_config_netdev,86,86," static int i40e_config_netdev(struct i40e_vsi *vsi)
 {
 	struct i40e_pf *pf = vsi->back;
 	struct i40e_hw *hw = &pf->hw;
 	struct i40e_netdev_priv *np;
 	struct net_device *netdev;
 	u8 broadcast[ETH_ALEN];
 	u8 mac_addr[ETH_ALEN];
 	int etherdev_size;
 	netdev_features_t hw_enc_features;
 	netdev_features_t hw_features;
 	etherdev_size = sizeof(struct i40e_netdev_priv);
 	netdev = alloc_etherdev_mq(etherdev_size, vsi->alloc_queue_pairs);
 	if (!netdev)
 		return -ENOMEM;
 	vsi->netdev = netdev;
 	np = netdev_priv(netdev);
 	np->vsi = vsi;
 	hw_enc_features = NETIF_F_SG			|
 			  NETIF_F_IP_CSUM		|
 			  NETIF_F_IPV6_CSUM		|
 			  NETIF_F_HIGHDMA		|
 			  NETIF_F_SOFT_FEATURES		|
 			  NETIF_F_TSO			|
 			  NETIF_F_TSO_ECN		|
 			  NETIF_F_TSO6			|
 			  NETIF_F_GSO_GRE		|
 			  NETIF_F_GSO_GRE_CSUM		|
 			  NETIF_F_GSO_PARTIAL		|
 			  NETIF_F_GSO_UDP_TUNNEL	|
 			  NETIF_F_GSO_UDP_TUNNEL_CSUM	|
 			  NETIF_F_SCTP_CRC		|
 			  NETIF_F_RXHASH		|
 			  NETIF_F_RXCSUM		|
 			  0;
 	if (!(pf->hw_features & I40E_HW_OUTER_UDP_CSUM_CAPABLE))
 		netdev->gso_partial_features |= NETIF_F_GSO_UDP_TUNNEL_CSUM;
 	netdev->gso_partial_features |= NETIF_F_GSO_GRE_CSUM;
 	netdev->hw_enc_features |= hw_enc_features;
 	
 	netdev->vlan_features |= hw_enc_features | NETIF_F_TSO_MANGLEID;
 	if (!(pf->flags & I40E_FLAG_MFP_ENABLED))
 		netdev->hw_features |= NETIF_F_NTUPLE | NETIF_F_HW_TC;
 	hw_features = hw_enc_features		|
 		      NETIF_F_HW_VLAN_CTAG_TX	|
 		      NETIF_F_HW_VLAN_CTAG_RX;
 	netdev->hw_features |= hw_features;
 	netdev->features |= hw_features | NETIF_F_HW_VLAN_CTAG_FILTER;
 	netdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;
 	if (vsi->type == I40E_VSI_MAIN) {
 		SET_NETDEV_DEV(netdev, &pf->pdev->dev);
 		ether_addr_copy(mac_addr, hw->mac.perm_addr);
 		
 		i40e_rm_default_mac_filter(vsi, mac_addr);
 		spin_lock_bh(&vsi->mac_filter_hash_lock);
 		i40e_add_mac_filter(vsi, mac_addr);
 		spin_unlock_bh(&vsi->mac_filter_hash_lock);
 	} else {
 		
 		snprintf(netdev->name, IFNAMSIZ, ""%.*sv%%d"",
 			 IFNAMSIZ - 4,
 			 pf->vsi[pf->lan_vsi]->netdev->name);
-		random_ether_addr(mac_addr);
+		eth_random_addr(mac_addr);
 		spin_lock_bh(&vsi->mac_filter_hash_lock);
 		i40e_add_mac_filter(vsi, mac_addr);
 		spin_unlock_bh(&vsi->mac_filter_hash_lock);
 	}
 	
 	eth_broadcast_addr(broadcast);
 	spin_lock_bh(&vsi->mac_filter_hash_lock);
 	i40e_add_mac_filter(vsi, broadcast);
 	spin_unlock_bh(&vsi->mac_filter_hash_lock);
 	ether_addr_copy(netdev->dev_addr, mac_addr);
 	ether_addr_copy(netdev->perm_addr, mac_addr);
 	netdev->priv_flags |= IFF_UNICAST_FLT;
 	netdev->priv_flags |= IFF_SUPP_NOFCS;
 	
 	i40e_vsi_config_netdev_tc(vsi, vsi->tc_config.enabled_tc);
 	netdev->netdev_ops = &i40e_netdev_ops;
 	netdev->watchdog_timeo = 5 * HZ;
 	i40e_set_ethtool_ops(netdev);
 	
 	netdev->min_mtu = ETH_MIN_MTU;
 	netdev->max_mtu = I40E_MAX_RXBUFFER - I40E_PACKET_HDR_PAD;
 	return 0;
 }",87,465
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_bridge_loop_avoidance_batadv_bla_periodic_work,63,63," static void batadv_bla_periodic_work(struct work_struct *work)
 {
 	struct delayed_work *delayed_work;
 	struct batadv_priv *bat_priv;
 	struct batadv_priv_bla *priv_bla;
 	struct hlist_head *head;
 	struct batadv_bla_backbone_gw *backbone_gw;
 	struct batadv_hashtable *hash;
 	struct batadv_hard_iface *primary_if;
 	bool send_loopdetect = false;
 	int i;
 	delayed_work = to_delayed_work(work);
 	priv_bla = container_of(delayed_work, struct batadv_priv_bla, work);
 	bat_priv = container_of(priv_bla, struct batadv_priv, bla);
 	primary_if = batadv_primary_if_get_selected(bat_priv);
 	if (!primary_if)
 		goto out;
 	batadv_bla_purge_claims(bat_priv, primary_if, 0);
 	batadv_bla_purge_backbone_gw(bat_priv, 0);
 	if (!atomic_read(&bat_priv->bridge_loop_avoidance))
 		goto out;
 	if (atomic_dec_and_test(&bat_priv->bla.loopdetect_next)) {
 		
-		random_ether_addr(bat_priv->bla.loopdetect_addr);
+		eth_random_addr(bat_priv->bla.loopdetect_addr);
 		bat_priv->bla.loopdetect_addr[0] = 0xba;
 		bat_priv->bla.loopdetect_addr[1] = 0xbe;
 		bat_priv->bla.loopdetect_lasttime = jiffies;
 		atomic_set(&bat_priv->bla.loopdetect_next,
 			   BATADV_BLA_LOOPDETECT_PERIODS);
 		
 		send_loopdetect = true;
 	}
 	hash = bat_priv->bla.backbone_hash;
 	if (!hash)
 		goto out;
 	for (i = 0; i < hash->size; i++) {
 		head = &hash->table[i];
 		rcu_read_lock();
 		hlist_for_each_entry_rcu(backbone_gw, head, hash_entry) {
 			if (!batadv_compare_eth(backbone_gw->orig,
 						primary_if->net_dev->dev_addr))
 				continue;
 			backbone_gw->lasttime = jiffies;
 			batadv_bla_send_announce(bat_priv, backbone_gw);
 			if (send_loopdetect)
 				batadv_bla_send_loopdetect(bat_priv,
 							   backbone_gw);
 			
 			if (atomic_read(&backbone_gw->request_sent) == 0)
 				continue;
 			if (!atomic_dec_and_test(&backbone_gw->wait_periods))
 				continue;
 			atomic_dec(&backbone_gw->bat_priv->bla.num_requests);
 			atomic_set(&backbone_gw->request_sent, 0);
 		}
 		rcu_read_unlock();
 	}
 out:
 	if (primary_if)
 		batadv_hardif_put(primary_if);
 	queue_delayed_work(batadv_event_workqueue, &bat_priv->bla.work,
 			   msecs_to_jiffies(BATADV_BLA_PERIOD_LENGTH));
 }",64,391
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_cpsw_cpsw_probe_dual_emac,43,43," static int cpsw_probe_dual_emac(struct cpsw_priv *priv)
 {
 	struct cpsw_common		*cpsw = priv->cpsw;
 	struct cpsw_platform_data	*data = &cpsw->data;
 	struct net_device		*ndev;
 	struct cpsw_priv		*priv_sl2;
 	int ret = 0;
 	ndev = alloc_etherdev_mq(sizeof(struct cpsw_priv), CPSW_MAX_QUEUES);
 	if (!ndev) {
 		dev_err(cpsw->dev, ""cpsw: error allocating net_device\n"");
 		return -ENOMEM;
 	}
 	priv_sl2 = netdev_priv(ndev);
 	priv_sl2->cpsw = cpsw;
 	priv_sl2->ndev = ndev;
 	priv_sl2->dev  = &ndev->dev;
 	priv_sl2->msg_enable = netif_msg_init(debug_level, CPSW_DEBUG);
 	if (is_valid_ether_addr(data->slave_data[1].mac_addr)) {
 		memcpy(priv_sl2->mac_addr, data->slave_data[1].mac_addr,
 			ETH_ALEN);
 		dev_info(cpsw->dev, ""cpsw: Detected MACID = %pM\n"",
 			 priv_sl2->mac_addr);
 	} else {
-		random_ether_addr(priv_sl2->mac_addr);
+		eth_random_addr(priv_sl2->mac_addr);
 		dev_info(cpsw->dev, ""cpsw: Random MACID = %pM\n"",
 			 priv_sl2->mac_addr);
 	}
 	memcpy(ndev->dev_addr, priv_sl2->mac_addr, ETH_ALEN);
 	priv_sl2->emac_port = 1;
 	cpsw->slaves[1].ndev = ndev;
 	ndev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
 	ndev->netdev_ops = &cpsw_netdev_ops;
 	ndev->ethtool_ops = &cpsw_ethtool_ops;
 	
 	SET_NETDEV_DEV(ndev, cpsw->dev);
 	ret = register_netdev(ndev);
 	if (ret) {
 		dev_err(cpsw->dev, ""cpsw: error registering net device\n"");
 		free_netdev(ndev);
 		ret = -ENODEV;
 	}
 	return ret;
 }",44,302
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_hw_ath9k_hw_init_macaddr,23,23," static void ath9k_hw_init_macaddr(struct ath_hw *ah)
 {
 	struct ath_common *common = ath9k_hw_common(ah);
 	int i;
 	u16 eeval;
 	static const u32 EEP_MAC[] = { EEP_MAC_LSW, EEP_MAC_MID, EEP_MAC_MSW };
 	
 	if (is_valid_ether_addr(common->macaddr))
 		return;
 	for (i = 0; i < 3; i++) {
 		eeval = ah->eep_ops->get_eeprom(ah, EEP_MAC[i]);
 		common->macaddr[2 * i] = eeval >> 8;
 		common->macaddr[2 * i + 1] = eeval & 0xff;
 	}
 	if (is_valid_ether_addr(common->macaddr))
 		return;
 	ath_err(common, ""eeprom contains invalid mac address: %pM\n"",
 		common->macaddr);
-	random_ether_addr(common->macaddr);
+	eth_random_addr(common->macaddr);
 	ath_err(common, ""random mac address will be used: %pM\n"",
 		common->macaddr);
 	return;
 }",24,169
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_netcp_core_netcp_create_interface,140,140," static int netcp_create_interface(struct netcp_device *netcp_device,
 				  struct device_node *node_interface)
 {
 	struct device *dev = netcp_device->device;
 	struct device_node *node = dev->of_node;
 	struct netcp_intf *netcp;
 	struct net_device *ndev;
 	resource_size_t size;
 	struct resource res;
 	void __iomem *efuse = NULL;
 	u32 efuse_mac = 0;
 	const void *mac_addr;
 	u8 efuse_mac_addr[6];
 	u32 temp[2];
 	int ret = 0;
 	ndev = alloc_etherdev_mqs(sizeof(*netcp), 1, 1);
 	if (!ndev) {
 		dev_err(dev, ""Error allocating netdev\n"");
 		return -ENOMEM;
 	}
 	ndev->features |= NETIF_F_SG;
 	ndev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
 	ndev->hw_features = ndev->features;
 	ndev->vlan_features |=  NETIF_F_SG;
 	
 	ndev->min_mtu = ETH_MIN_MTU;
 	ndev->max_mtu = NETCP_MAX_FRAME_SIZE - (ETH_HLEN + ETH_FCS_LEN);
 	netcp = netdev_priv(ndev);
 	spin_lock_init(&netcp->lock);
 	INIT_LIST_HEAD(&netcp->module_head);
 	INIT_LIST_HEAD(&netcp->txhook_list_head);
 	INIT_LIST_HEAD(&netcp->rxhook_list_head);
 	INIT_LIST_HEAD(&netcp->addr_list);
 	u64_stats_init(&netcp->stats.syncp_rx);
 	u64_stats_init(&netcp->stats.syncp_tx);
 	netcp->netcp_device = netcp_device;
 	netcp->dev = netcp_device->device;
 	netcp->ndev = ndev;
 	netcp->ndev_dev  = &ndev->dev;
 	netcp->msg_enable = netif_msg_init(netcp_debug_level, NETCP_DEBUG);
 	netcp->tx_pause_threshold = MAX_SKB_FRAGS;
 	netcp->tx_resume_threshold = netcp->tx_pause_threshold;
 	netcp->node_interface = node_interface;
 	ret = of_property_read_u32(node_interface, ""efuse-mac"", &efuse_mac);
 	if (efuse_mac) {
 		if (of_address_to_resource(node, NETCP_EFUSE_REG_INDEX, &res)) {
 			dev_err(dev, ""could not find efuse-mac reg resource\n"");
 			ret = -ENODEV;
 			goto quit;
 		}
 		size = resource_size(&res);
 		if (!devm_request_mem_region(dev, res.start, size,
 					     dev_name(dev))) {
 			dev_err(dev, ""could not reserve resource\n"");
 			ret = -ENOMEM;
 			goto quit;
 		}
 		efuse = devm_ioremap_nocache(dev, res.start, size);
 		if (!efuse) {
 			dev_err(dev, ""could not map resource\n"");
 			devm_release_mem_region(dev, res.start, size);
 			ret = -ENOMEM;
 			goto quit;
 		}
 		emac_arch_get_mac_addr(efuse_mac_addr, efuse, efuse_mac);
 		if (is_valid_ether_addr(efuse_mac_addr))
 			ether_addr_copy(ndev->dev_addr, efuse_mac_addr);
 		else
-			random_ether_addr(ndev->dev_addr);
+			eth_random_addr(ndev->dev_addr);
 		devm_iounmap(dev, efuse);
 		devm_release_mem_region(dev, res.start, size);
 	} else {
 		mac_addr = of_get_mac_address(node_interface);
 		if (mac_addr)
 			ether_addr_copy(ndev->dev_addr, mac_addr);
 		else
-			random_ether_addr(ndev->dev_addr);
+			eth_random_addr(ndev->dev_addr);
 	}
 	ret = of_property_read_string(node_interface, ""rx-channel"",
 				      &netcp->dma_chan_name);
 	if (ret < 0) {
 		dev_err(dev, ""missing \""rx-channel\"" parameter\n"");
 		ret = -ENODEV;
 		goto quit;
 	}
 	ret = of_property_read_u32(node_interface, ""rx-queue"",
 				   &netcp->rx_queue_id);
 	if (ret < 0) {
 		dev_warn(dev, ""missing \""rx-queue\"" parameter\n"");
 		netcp->rx_queue_id = KNAV_QUEUE_QPEND;
 	}
 	ret = of_property_read_u32_array(node_interface, ""rx-queue-depth"",
 					 netcp->rx_queue_depths,
 					 KNAV_DMA_FDQ_PER_CHAN);
 	if (ret < 0) {
 		dev_err(dev, ""missing \""rx-queue-depth\"" parameter\n"");
 		netcp->rx_queue_depths[0] = 128;
 	}
 	ret = of_property_read_u32_array(node_interface, ""rx-pool"", temp, 2);
 	if (ret < 0) {
 		dev_err(dev, ""missing \""rx-pool\"" parameter\n"");
 		ret = -ENODEV;
 		goto quit;
 	}
 	netcp->rx_pool_size = temp[0];
 	netcp->rx_pool_region_id = temp[1];
 	ret = of_property_read_u32_array(node_interface, ""tx-pool"", temp, 2);
 	if (ret < 0) {
 		dev_err(dev, ""missing \""tx-pool\"" parameter\n"");
 		ret = -ENODEV;
 		goto quit;
 	}
 	netcp->tx_pool_size = temp[0];
 	netcp->tx_pool_region_id = temp[1];
 	if (netcp->tx_pool_size < MAX_SKB_FRAGS) {
 		dev_err(dev, ""tx-pool size too small, must be atleast(%ld)\n"",
 			MAX_SKB_FRAGS);
 		ret = -ENODEV;
 		goto quit;
 	}
 	ret = of_property_read_u32(node_interface, ""tx-completion-queue"",
 				   &netcp->tx_compl_qid);
 	if (ret < 0) {
 		dev_warn(dev, ""missing \""tx-completion-queue\"" parameter\n"");
 		netcp->tx_compl_qid = KNAV_QUEUE_QPEND;
 	}
 	
 	netif_napi_add(ndev, &netcp->rx_napi, netcp_rx_poll, NETCP_NAPI_WEIGHT);
 	netif_tx_napi_add(ndev, &netcp->tx_napi, netcp_tx_poll, NETCP_NAPI_WEIGHT);
 	
 	ndev->dev_id		= 0;
 	ndev->watchdog_timeo	= NETCP_TX_TIMEOUT;
 	ndev->netdev_ops	= &netcp_netdev_ops;
 	SET_NETDEV_DEV(ndev, dev);
 	list_add_tail(&netcp->interface_list, &netcp_device->interface_head);
 	return 0;
 quit:
 	free_netdev(ndev);
 	return ret;
 }",142,958
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_lan743x_main_lan743x_mac_init,39,39," static int lan743x_mac_init(struct lan743x_adapter *adapter)
 {
 	bool mac_address_valid = true;
 	struct net_device *netdev;
 	u32 mac_addr_hi = 0;
 	u32 mac_addr_lo = 0;
 	u32 data;
 	int ret;
 	netdev = adapter->netdev;
 	lan743x_csr_write(adapter, MAC_CR, MAC_CR_RST_);
 	ret = lan743x_csr_wait_for_bit(adapter, MAC_CR, MAC_CR_RST_,
 				       0, 1000, 20000, 100);
 	if (ret)
 		return ret;
 	
 	data = lan743x_csr_read(adapter, MAC_CR);
 	data |= MAC_CR_ADD_ | MAC_CR_ASD_;
 	data |= MAC_CR_CNTR_RST_;
 	lan743x_csr_write(adapter, MAC_CR, data);
 	mac_addr_hi = lan743x_csr_read(adapter, MAC_RX_ADDRH);
 	mac_addr_lo = lan743x_csr_read(adapter, MAC_RX_ADDRL);
 	adapter->mac_address[0] = mac_addr_lo & 0xFF;
 	adapter->mac_address[1] = (mac_addr_lo >> 8) & 0xFF;
 	adapter->mac_address[2] = (mac_addr_lo >> 16) & 0xFF;
 	adapter->mac_address[3] = (mac_addr_lo >> 24) & 0xFF;
 	adapter->mac_address[4] = mac_addr_hi & 0xFF;
 	adapter->mac_address[5] = (mac_addr_hi >> 8) & 0xFF;
 	if (((mac_addr_hi & 0x0000FFFF) == 0x0000FFFF) &&
 	    mac_addr_lo == 0xFFFFFFFF) {
 		mac_address_valid = false;
 	} else if (!is_valid_ether_addr(adapter->mac_address)) {
 		mac_address_valid = false;
 	}
 	if (!mac_address_valid)
-		random_ether_addr(adapter->mac_address);
+		eth_random_addr(adapter->mac_address);
 	lan743x_mac_set_address(adapter, adapter->mac_address);
 	ether_addr_copy(netdev->dev_addr, adapter->mac_address);
 	return 0;
 }",40,289
random_ether_addr-84,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/random_ether_addr-84/1529689860_2018-06-22_6c1f0a1ffb7c_hip04_eth_hip04_mac_probe,99,99," static int hip04_mac_probe(struct platform_device *pdev)
 {
 	struct device *d = &pdev->dev;
 	struct device_node *node = d->of_node;
 	struct of_phandle_args arg;
 	struct net_device *ndev;
 	struct hip04_priv *priv;
 	struct resource *res;
 	int irq;
 	int ret;
 	ndev = alloc_etherdev(sizeof(struct hip04_priv));
 	if (!ndev)
 		return -ENOMEM;
 	priv = netdev_priv(ndev);
 	priv->ndev = ndev;
 	platform_set_drvdata(pdev, ndev);
 	SET_NETDEV_DEV(ndev, &pdev->dev);
 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	priv->base = devm_ioremap_resource(d, res);
 	if (IS_ERR(priv->base)) {
 		ret = PTR_ERR(priv->base);
 		goto init_fail;
 	}
 	ret = of_parse_phandle_with_fixed_args(node, ""port-handle"", 2, 0, &arg);
 	if (ret < 0) {
 		dev_warn(d, ""no port-handle\n"");
 		goto init_fail;
 	}
 	priv->port = arg.args[0];
 	priv->chan = arg.args[1] * RX_DESC_NUM;
 	hrtimer_init(&priv->tx_coalesce_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	
 	priv->tx_coalesce_frames = TX_DESC_NUM * 3 / 4;
 	priv->tx_coalesce_usecs = 200;
 	priv->tx_coalesce_timer.function = tx_done;
 	priv->map = syscon_node_to_regmap(arg.np);
 	if (IS_ERR(priv->map)) {
 		dev_warn(d, ""no syscon hisilicon,hip04-ppe\n"");
 		ret = PTR_ERR(priv->map);
 		goto init_fail;
 	}
 	priv->phy_mode = of_get_phy_mode(node);
 	if (priv->phy_mode < 0) {
 		dev_warn(d, ""not find phy-mode\n"");
 		ret = -EINVAL;
 		goto init_fail;
 	}
 	irq = platform_get_irq(pdev, 0);
 	if (irq <= 0) {
 		ret = -EINVAL;
 		goto init_fail;
 	}
 	ret = devm_request_irq(d, irq, hip04_mac_interrupt,
 			       0, pdev->name, ndev);
 	if (ret) {
 		netdev_err(ndev, ""devm_request_irq failed\n"");
 		goto init_fail;
 	}
 	priv->phy_node = of_parse_phandle(node, ""phy-handle"", 0);
 	if (priv->phy_node) {
 		priv->phy = of_phy_connect(ndev, priv->phy_node,
 					   &hip04_adjust_link,
 					   0, priv->phy_mode);
 		if (!priv->phy) {
 			ret = -EPROBE_DEFER;
 			goto init_fail;
 		}
 	}
 	INIT_WORK(&priv->tx_timeout_task, hip04_tx_timeout_task);
 	ndev->netdev_ops = &hip04_netdev_ops;
 	ndev->ethtool_ops = &hip04_ethtool_ops;
 	ndev->watchdog_timeo = TX_TIMEOUT;
 	ndev->priv_flags |= IFF_UNICAST_FLT;
 	ndev->irq = irq;
 	netif_napi_add(ndev, &priv->napi, hip04_rx_poll, NAPI_POLL_WEIGHT);
 	hip04_reset_ppe(priv);
 	if (priv->phy_mode == PHY_INTERFACE_MODE_MII)
 		hip04_config_port(ndev, SPEED_100, DUPLEX_FULL);
 	hip04_config_fifo(priv);
-	random_ether_addr(ndev->dev_addr);
+	eth_random_addr(ndev->dev_addr);
 	hip04_update_mac_address(ndev);
 	ret = hip04_alloc_ring(ndev, d);
 	if (ret) {
 		netdev_err(ndev, ""alloc ring fail\n"");
 		goto alloc_fail;
 	}
 	ret = register_netdev(ndev);
 	if (ret) {
 		free_netdev(ndev);
 		goto alloc_fail;
 	}
 	return 0;
 alloc_fail:
 	hip04_free_ring(ndev, d);
 init_fail:
 	of_node_put(priv->phy_node);
 	free_netdev(ndev);
 	return ret;
 }",100,637
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/snd_soc/1524629994_2018-04-25_e4b31b816c47_mtk-afe-fe-dai_mtk_afe_fe_trigger,55,54," int mtk_afe_fe_trigger(struct snd_pcm_substream *substream, int cmd,
 		       struct snd_soc_dai *dai)
 {
 	struct snd_soc_pcm_runtime *rtd = substream->private_data;
 	struct snd_pcm_runtime * const runtime = substream->runtime;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mtk_base_afe_memif *memif = &afe->memif[rtd->cpu_dai->id];
 	struct mtk_base_afe_irq *irqs = &afe->irqs[memif->irq_usage];
 	const struct mtk_base_irq_data *irq_data = irqs->irq_data;
 	unsigned int counter = runtime->period_size;
 	int fs;
 	dev_dbg(afe->dev, ""%s %s cmd=%d\n"", __func__, memif->data->name, cmd);
 	switch (cmd) {
 	case SNDRV_PCM_TRIGGER_START:
 	case SNDRV_PCM_TRIGGER_RESUME:
 		if (memif->data->enable_shift >= 0)
 			mtk_regmap_update_bits(afe->regmap,
 					       memif->data->enable_reg,
 					       1 << memif->data->enable_shift,
 					       1 << memif->data->enable_shift);
 		
 		mtk_regmap_update_bits(afe->regmap, irq_data->irq_cnt_reg,
 				       irq_data->irq_cnt_maskbit
 				       << irq_data->irq_cnt_shift,
 				       counter << irq_data->irq_cnt_shift);
 		
 		fs = afe->irq_fs(substream, runtime->rate);
 		if (fs < 0)
 			return -EINVAL;
 		mtk_regmap_update_bits(afe->regmap, irq_data->irq_fs_reg,
 				       irq_data->irq_fs_maskbit
 				       << irq_data->irq_fs_shift,
 				       fs << irq_data->irq_fs_shift);
 		
 		mtk_regmap_update_bits(afe->regmap, irq_data->irq_en_reg,
 				       1 << irq_data->irq_en_shift,
 				       1 << irq_data->irq_en_shift);
 		return 0;
 	case SNDRV_PCM_TRIGGER_STOP:
 	case SNDRV_PCM_TRIGGER_SUSPEND:
 		mtk_regmap_update_bits(afe->regmap, memif->data->enable_reg,
 				       1 << memif->data->enable_shift, 0);
 		
 		mtk_regmap_update_bits(afe->regmap, irq_data->irq_en_reg,
 				       1 << irq_data->irq_en_shift,
 				       0 << irq_data->irq_en_shift);
 		
 		mtk_regmap_write(afe->regmap, irq_data->irq_clr_reg,
 				 1 << irq_data->irq_clr_shift);
 		return 0;
 	default:
 		return -EINVAL;
 	}
 }",56,375
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_afe_i2s_startup,11,9," static int mt2701_afe_i2s_startup(struct snd_pcm_substream *substream,
 				  struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	int i2s_num = mt2701_dai_num_to_i2s(afe, dai->id);
 	if (i2s_num < 0)
 		return i2s_num;
 	return mt2701_afe_enable_mclk(afe, i2s_num);
 }",12,90
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/snd_soc/1524629994_2018-04-25_e4b31b816c47_mtk-afe-fe-dai_mtk_afe_fe_startup,48,47," int mtk_afe_fe_startup(struct snd_pcm_substream *substream,
 		       struct snd_soc_dai *dai)
 {
 	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct snd_pcm_runtime *runtime = substream->runtime;
 	int memif_num = rtd->cpu_dai->id;
 	struct mtk_base_afe_memif *memif = &afe->memif[memif_num];
 	const struct snd_pcm_hardware *mtk_afe_hardware = afe->mtk_afe_hardware;
 	int ret;
 	memif->substream = substream;
 	snd_pcm_hw_constraint_step(substream->runtime, 0,
 				   SNDRV_PCM_HW_PARAM_BUFFER_BYTES, 16);
 	
 	mtk_regmap_update_bits(afe->regmap, memif->data->agent_disable_reg,
 			       1 << memif->data->agent_disable_shift,
 			       0 << memif->data->agent_disable_shift);
 	snd_soc_set_runtime_hwparams(substream, mtk_afe_hardware);
 	
 	if (substream->stream == SNDRV_PCM_STREAM_CAPTURE) {
 		int periods_max = mtk_afe_hardware->periods_max;
 		ret = snd_pcm_hw_constraint_minmax(runtime,
 						   SNDRV_PCM_HW_PARAM_PERIODS,
 						   3, periods_max);
 		if (ret < 0) {
 			dev_err(afe->dev, ""hw_constraint_minmax failed\n"");
 			return ret;
 		}
 	}
 	ret = snd_pcm_hw_constraint_integer(runtime,
 					    SNDRV_PCM_HW_PARAM_PERIODS);
 	if (ret < 0)
 		dev_err(afe->dev, ""snd_pcm_hw_constraint_integer failed\n"");
 	
 	if (memif->irq_usage < 0) {
 		int irq_id = mtk_dynamic_irq_acquire(afe);
 		if (irq_id != afe->irqs_size) {
 			
 			memif->irq_usage = irq_id;
 		} else {
 			dev_err(afe->dev, ""%s() error: no more asys irq\n"",
 				__func__);
 			ret = -EBUSY;
 		}
 	}
 	return ret;
 }",49,297
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_btmrg_hw_params,36,34," static int mt2701_btmrg_hw_params(struct snd_pcm_substream *substream,
 				  struct snd_pcm_hw_params *params,
 				  struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	int stream_fs;
 	u32 val, msk;
 	stream_fs = params_rate(params);
 	if ((stream_fs != 8000) && (stream_fs != 16000)) {
 		dev_err(afe->dev, ""%s() btmgr not support this stream_fs %d\n"",
 			__func__, stream_fs);
 		return -EINVAL;
 	}
 	regmap_update_bits(afe->regmap, AFE_MRGIF_CON,
 			   AFE_MRGIF_CON_I2S_MODE_MASK,
 			   AFE_MRGIF_CON_I2S_MODE_32K);
 	val = AFE_DAIBT_CON0_BT_FUNC_EN | AFE_DAIBT_CON0_BT_FUNC_RDY
 	      | AFE_DAIBT_CON0_MRG_USE;
 	msk = val;
 	if (stream_fs == 16000)
 		val |= AFE_DAIBT_CON0_BT_WIDE_MODE_EN;
 	msk |= AFE_DAIBT_CON0_BT_WIDE_MODE_EN;
 	regmap_update_bits(afe->regmap, AFE_DAIBT_CON0, msk, val);
 	regmap_update_bits(afe->regmap, AFE_DAIBT_CON0,
 			   AFE_DAIBT_CON0_DAIBT_EN,
 			   AFE_DAIBT_CON0_DAIBT_EN);
 	regmap_update_bits(afe->regmap, AFE_MRGIF_CON,
 			   AFE_MRGIF_CON_MRG_I2S_EN,
 			   AFE_MRGIF_CON_MRG_I2S_EN);
 	regmap_update_bits(afe->regmap, AFE_MRGIF_CON,
 			   AFE_MRGIF_CON_MRG_EN,
 			   AFE_MRGIF_CON_MRG_EN);
 	return 0;
 }",37,211
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/snd_soc/1524629994_2018-04-25_e4b31b816c47_mtk-afe-fe-dai_mtk_afe_fe_shutdown,18,17," void mtk_afe_fe_shutdown(struct snd_pcm_substream *substream,
 			 struct snd_soc_dai *dai)
 {
 	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mtk_base_afe_memif *memif = &afe->memif[rtd->cpu_dai->id];
 	int irq_id;
 	irq_id = memif->irq_usage;
 	mtk_regmap_update_bits(afe->regmap, memif->data->agent_disable_reg,
 			       1 << memif->data->agent_disable_shift,
 			       1 << memif->data->agent_disable_shift);
 	if (!memif->const_irq) {
 		mtk_dynamic_irq_release(afe, irq_id);
 		memif->irq_usage = -1;
 		memif->substream = NULL;
 	}
 }",19,142
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_dlm_fe_startup,24,22," static int mt2701_dlm_fe_startup(struct snd_pcm_substream *substream,
 				 struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mtk_base_afe_memif *memif_tmp;
 	const struct mtk_base_memif_data *memif_data;
 	int i;
 	for (i = MT2701_MEMIF_DL1; i < MT2701_MEMIF_DL_SINGLE_NUM; ++i) {
 		memif_tmp = &afe->memif[i];
 		if (memif_tmp->substream)
 			return -EBUSY;
 	}
 	
 	for (i = MT2701_MEMIF_DL1; i < MT2701_MEMIF_DL_SINGLE_NUM; ++i) {
 		memif_data = afe->memif[i].data;
 		regmap_update_bits(afe->regmap,
 				   memif_data->agent_disable_reg,
 				   1 << memif_data->agent_disable_shift,
 				   0 << memif_data->agent_disable_shift);
 	}
 	return mtk_afe_fe_startup(substream, dai);
 }",25,167
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_dlm_fe_trigger,25,23," static int mt2701_dlm_fe_trigger(struct snd_pcm_substream *substream,
 				 int cmd, struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mtk_base_afe_memif *memif_tmp = &afe->memif[MT2701_MEMIF_DL1];
 	switch (cmd) {
 	case SNDRV_PCM_TRIGGER_START:
 	case SNDRV_PCM_TRIGGER_RESUME:
 		regmap_update_bits(afe->regmap, memif_tmp->data->enable_reg,
 				   1 << memif_tmp->data->enable_shift,
 				   1 << memif_tmp->data->enable_shift);
 		mtk_afe_fe_trigger(substream, cmd, dai);
 		return 0;
 	case SNDRV_PCM_TRIGGER_STOP:
 	case SNDRV_PCM_TRIGGER_SUSPEND:
 		mtk_afe_fe_trigger(substream, cmd, dai);
 		regmap_update_bits(afe->regmap, memif_tmp->data->enable_reg,
 				   1 << memif_tmp->data->enable_shift, 0);
 		return 0;
 	default:
 		return -EINVAL;
 	}
 }",26,177
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_afe_i2s_shutdown,24,22," static void mt2701_afe_i2s_shutdown(struct snd_pcm_substream *substream,
 				    struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt2701_afe_private *afe_priv = afe->platform_priv;
 	int i2s_num = mt2701_dai_num_to_i2s(afe, dai->id);
 	struct mt2701_i2s_path *i2s_path;
 	if (i2s_num < 0)
 		return;
 	i2s_path = &afe_priv->i2s_path[i2s_num];
 	if (i2s_path->occupied[substream->stream])
 		i2s_path->occupied[substream->stream] = 0;
 	else
 		goto I2S_UNSTART;
 	mt2701_afe_i2s_path_shutdown(substream, dai, i2s_num, 0);
 	
 	if (substream->stream == SNDRV_PCM_STREAM_CAPTURE)
 		mt2701_afe_i2s_path_shutdown(substream, dai, i2s_num, 1);
 I2S_UNSTART:
 	
 	mt2701_afe_disable_mclk(afe, i2s_num);
 }",25,170
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mtk-afe-fe-dai_mtk_afe_fe_prepare,29,28," int mtk_afe_fe_prepare(struct snd_pcm_substream *substream,
 		       struct snd_soc_dai *dai)
 {
 	struct snd_soc_pcm_runtime *rtd  = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mtk_base_afe_memif *memif = &afe->memif[rtd->cpu_dai->id];
 	int hd_audio = 0;
 	
 	switch (substream->runtime->format) {
 	case SNDRV_PCM_FORMAT_S16_LE:
 		hd_audio = 0;
 		break;
 	case SNDRV_PCM_FORMAT_S32_LE:
 		hd_audio = 1;
 		break;
 	case SNDRV_PCM_FORMAT_S24_LE:
 		hd_audio = 1;
 		break;
 	default:
 		dev_err(afe->dev, ""%s() error: unsupported format %d\n"",
 			__func__, substream->runtime->format);
 		break;
 	}
 	mtk_regmap_update_bits(afe->regmap, memif->data->hd_reg,
 			       1 << memif->data->hd_shift,
 			       hd_audio << memif->data->hd_shift);
 	return 0;
 }",30,174
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt8173-afe-pcm_mt8173_afe_i2s_shutdown,13,11," static void mt8173_afe_i2s_shutdown(struct snd_pcm_substream *substream,
 				    struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	if (dai->active)
 		return;
 	mt8173_afe_set_i2s_enable(afe, false);
 	regmap_update_bits(afe->regmap, AUDIO_TOP_CON0,
 			   AUD_TCON0_PDN_22M | AUD_TCON0_PDN_24M,
 			   AUD_TCON0_PDN_22M | AUD_TCON0_PDN_24M);
 }",14,93
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_afe_i2s_path_shutdown,33,31," static int mt2701_afe_i2s_path_shutdown(struct snd_pcm_substream *substream,
 					struct snd_soc_dai *dai,
 					int i2s_num,
 					int dir_invert)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt2701_afe_private *afe_priv = afe->platform_priv;
 	struct mt2701_i2s_path *i2s_path = &afe_priv->i2s_path[i2s_num];
 	const struct mt2701_i2s_data *i2s_data;
 	int stream_dir = substream->stream;
 	if (dir_invert)	{
 		if (stream_dir == SNDRV_PCM_STREAM_PLAYBACK)
 			stream_dir = SNDRV_PCM_STREAM_CAPTURE;
 		else
 			stream_dir = SNDRV_PCM_STREAM_PLAYBACK;
 	}
 	i2s_data = i2s_path->i2s_data[stream_dir];
 	i2s_path->on[stream_dir]--;
 	if (i2s_path->on[stream_dir] < 0) {
 		dev_warn(afe->dev, ""i2s_path->on: %d, dir: %d\n"",
 			 i2s_path->on[stream_dir], stream_dir);
 		i2s_path->on[stream_dir] = 0;
 	}
 	if (i2s_path->on[stream_dir])
 		return 0;
 	
 	regmap_update_bits(afe->regmap, i2s_data->i2s_ctrl_reg,
 			   ASYS_I2S_CON_I2S_EN, 0);
 	mt2701_afe_disable_i2s(afe, i2s_num, stream_dir);
 	return 0;
 }",34,222
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_btmrg_shutdown,19,17," static void mt2701_btmrg_shutdown(struct snd_pcm_substream *substream,
 				  struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt2701_afe_private *afe_priv = afe->platform_priv;
 	
 	if (!afe_priv->mrg_enable[!substream->stream]) {
 		regmap_update_bits(afe->regmap, AFE_DAIBT_CON0,
 				   AFE_DAIBT_CON0_DAIBT_EN, 0);
 		regmap_update_bits(afe->regmap, AFE_MRGIF_CON,
 				   AFE_MRGIF_CON_MRG_EN, 0);
 		regmap_update_bits(afe->regmap, AFE_MRGIF_CON,
 				   AFE_MRGIF_CON_MRG_I2S_EN, 0);
 		mt2701_disable_btmrg_clk(afe);
 	}
 	afe_priv->mrg_enable[substream->stream] = 0;
 }",20,140
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt8173-afe-pcm_mt8173_afe_hdmi_shutdown,12,10," static void mt8173_afe_hdmi_shutdown(struct snd_pcm_substream *substream,
 				     struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt8173_afe_private *afe_priv = afe->platform_priv;
 	if (dai->active)
 		return;
 	mt8173_afe_dais_disable_clks(afe, afe_priv->clocks[MT8173_CLK_I2S3_M],
 				     afe_priv->clocks[MT8173_CLK_I2S3_B]);
 }",13,97
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_btmrg_startup,14,12," static int mt2701_btmrg_startup(struct snd_pcm_substream *substream,
 				struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt2701_afe_private *afe_priv = afe->platform_priv;
 	int ret;
 	ret = mt2701_enable_btmrg_clk(afe);
 	if (ret)
 		return ret;
 	afe_priv->mrg_enable[substream->stream] = 1;
 	return 0;
 }",15,101
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt8173-afe-pcm_mt8173_afe_hdmi_prepare,59,57," static int mt8173_afe_hdmi_prepare(struct snd_pcm_substream *substream,
 				   struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
 	struct snd_pcm_runtime * const runtime = substream->runtime;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt8173_afe_private *afe_priv = afe->platform_priv;
 	unsigned int val;
 	mt8173_afe_dais_set_clks(afe, afe_priv->clocks[MT8173_CLK_I2S3_M],
 				 runtime->rate * 128,
 				 afe_priv->clocks[MT8173_CLK_I2S3_B],
 				 runtime->rate * runtime->channels * 32);
 	val = AFE_TDM_CON1_BCK_INV |
 	      AFE_TDM_CON1_LRCK_INV |
 	      AFE_TDM_CON1_1_BCK_DELAY |
 	      AFE_TDM_CON1_MSB_ALIGNED | 
 	      AFE_TDM_CON1_WLEN_32BIT |
 	      AFE_TDM_CON1_32_BCK_CYCLES |
 	      AFE_TDM_CON1_LRCK_WIDTH(32);
 	regmap_update_bits(afe->regmap, AFE_TDM_CON1, ~AFE_TDM_CON1_EN, val);
 	
 	switch (runtime->channels) {
 	case 1:
 	case 2:
 		val = AFE_TDM_CH_START_O30_O31;
 		val |= (AFE_TDM_CH_ZERO << 4);
 		val |= (AFE_TDM_CH_ZERO << 8);
 		val |= (AFE_TDM_CH_ZERO << 12);
 		break;
 	case 3:
 	case 4:
 		val = AFE_TDM_CH_START_O30_O31;
 		val |= (AFE_TDM_CH_START_O32_O33 << 4);
 		val |= (AFE_TDM_CH_ZERO << 8);
 		val |= (AFE_TDM_CH_ZERO << 12);
 		break;
 	case 5:
 	case 6:
 		val = AFE_TDM_CH_START_O30_O31;
 		val |= (AFE_TDM_CH_START_O32_O33 << 4);
 		val |= (AFE_TDM_CH_START_O34_O35 << 8);
 		val |= (AFE_TDM_CH_ZERO << 12);
 		break;
 	case 7:
 	case 8:
 		val = AFE_TDM_CH_START_O30_O31;
 		val |= (AFE_TDM_CH_START_O32_O33 << 4);
 		val |= (AFE_TDM_CH_START_O34_O35 << 8);
 		val |= (AFE_TDM_CH_START_O36_O37 << 12);
 		break;
 	default:
 		val = 0;
 	}
 	regmap_update_bits(afe->regmap, AFE_TDM_CON2, 0x0000ffff, val);
 	regmap_update_bits(afe->regmap, AFE_HDMI_OUT_CON0,
 			   0x000000f0, runtime->channels << 4);
 	return 0;
 }",60,343
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mtk-afe-fe-dai_mtk_afe_fe_hw_params,45,44," int mtk_afe_fe_hw_params(struct snd_pcm_substream *substream,
 			 struct snd_pcm_hw_params *params,
 			 struct snd_soc_dai *dai)
 {
 	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mtk_base_afe_memif *memif = &afe->memif[rtd->cpu_dai->id];
 	int msb_at_bit33 = 0;
 	int ret, fs = 0;
 	ret = snd_pcm_lib_malloc_pages(substream, params_buffer_bytes(params));
 	if (ret < 0)
 		return ret;
 	msb_at_bit33 = upper_32_bits(substream->runtime->dma_addr) ? 1 : 0;
 	memif->phys_buf_addr = lower_32_bits(substream->runtime->dma_addr);
 	memif->buffer_size = substream->runtime->dma_bytes;
 	
 	mtk_regmap_write(afe->regmap, memif->data->reg_ofs_base,
 			 memif->phys_buf_addr);
 	
 	mtk_regmap_write(afe->regmap,
 			 memif->data->reg_ofs_base + AFE_BASE_END_OFFSET,
 			 memif->phys_buf_addr + memif->buffer_size - 1);
 	
 	mtk_regmap_update_bits(afe->regmap, memif->data->msb_reg,
 			       1 << memif->data->msb_shift,
 			       msb_at_bit33 << memif->data->msb_shift);
 	
 	if (memif->data->mono_shift >= 0) {
 		unsigned int mono = (params_channels(params) == 1) ? 1 : 0;
 		mtk_regmap_update_bits(afe->regmap, memif->data->mono_reg,
 				       1 << memif->data->mono_shift,
 				       mono << memif->data->mono_shift);
 	}
 	
 	if (memif->data->fs_shift < 0)
 		return 0;
 	fs = afe->memif_fs(substream, params_rate(params));
 	if (fs < 0)
 		return -EINVAL;
 	mtk_regmap_update_bits(afe->regmap, memif->data->fs_reg,
 			       memif->data->fs_maskbit << memif->data->fs_shift,
 			       fs << memif->data->fs_shift);
 	return 0;
 }",46,354
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt8173-afe-pcm_mt8173_afe_hdmi_trigger,41,39," static int mt8173_afe_hdmi_trigger(struct snd_pcm_substream *substream, int cmd,
 				   struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	dev_info(afe->dev, ""%s cmd=%d %s\n"", __func__, cmd, dai->name);
 	switch (cmd) {
 	case SNDRV_PCM_TRIGGER_START:
 	case SNDRV_PCM_TRIGGER_RESUME:
 		regmap_update_bits(afe->regmap, AUDIO_TOP_CON0,
 				   AUD_TCON0_PDN_HDMI | AUD_TCON0_PDN_SPDF, 0);
 		
 		regmap_write(afe->regmap, AFE_HDMI_CONN0,
 				 AFE_HDMI_CONN0_O30_I30 |
 				 AFE_HDMI_CONN0_O31_I31 |
 				 AFE_HDMI_CONN0_O32_I34 |
 				 AFE_HDMI_CONN0_O33_I35 |
 				 AFE_HDMI_CONN0_O34_I32 |
 				 AFE_HDMI_CONN0_O35_I33 |
 				 AFE_HDMI_CONN0_O36_I36 |
 				 AFE_HDMI_CONN0_O37_I37);
 		
 		regmap_update_bits(afe->regmap, AFE_HDMI_OUT_CON0, 0x1, 0x1);
 		
 		regmap_update_bits(afe->regmap, AFE_TDM_CON1, 0x1, 0x1);
 		return 0;
 	case SNDRV_PCM_TRIGGER_STOP:
 	case SNDRV_PCM_TRIGGER_SUSPEND:
 		
 		regmap_update_bits(afe->regmap, AFE_TDM_CON1, 0x1, 0);
 		
 		regmap_update_bits(afe->regmap, AFE_HDMI_OUT_CON0, 0x1, 0);
 		regmap_update_bits(afe->regmap, AUDIO_TOP_CON0,
 				   AUD_TCON0_PDN_HDMI | AUD_TCON0_PDN_SPDF,
 				   AUD_TCON0_PDN_HDMI | AUD_TCON0_PDN_SPDF);
 		return 0;
 	default:
 		return -EINVAL;
 	}
 }",42,223
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt8173-afe-pcm_mt8173_afe_i2s_startup,12,10," static int mt8173_afe_i2s_startup(struct snd_pcm_substream *substream,
 				  struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	if (dai->active)
 		return 0;
 	regmap_update_bits(afe->regmap, AUDIO_TOP_CON0,
 			   AUD_TCON0_PDN_22M | AUD_TCON0_PDN_24M, 0);
 	return 0;
 }",13,88
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_simple_fe_startup,20,19," static int mt2701_simple_fe_startup(struct snd_pcm_substream *substream,
 				    struct snd_soc_dai *dai)
 {
 	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	int stream_dir = substream->stream;
 	int memif_num = rtd->cpu_dai->id;
 	struct mtk_base_afe_memif *memif_tmp;
 	
 	if (stream_dir == SNDRV_PCM_STREAM_PLAYBACK) {
 		memif_tmp = &afe->memif[MT2701_MEMIF_DLM];
 		if (memif_tmp->substream) {
 			dev_warn(afe->dev, ""%s memif is not available, stream_dir %d, memif_num %d\n"",
 				 __func__, stream_dir, memif_num);
 			return -EBUSY;
 		}
 	}
 	return mtk_afe_fe_startup(substream, dai);
 }",21,137
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt8173-afe-pcm_mt8173_afe_i2s_prepare,20,18," static int mt8173_afe_i2s_prepare(struct snd_pcm_substream *substream,
 				  struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
 	struct snd_pcm_runtime * const runtime = substream->runtime;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt8173_afe_private *afe_priv = afe->platform_priv;
 	int ret;
 	mt8173_afe_dais_set_clks(afe, afe_priv->clocks[MT8173_CLK_I2S1_M],
 				 runtime->rate * 256, NULL, 0);
 	mt8173_afe_dais_set_clks(afe, afe_priv->clocks[MT8173_CLK_I2S2_M],
 				 runtime->rate * 256, NULL, 0);
 	
 	ret = mt8173_afe_set_i2s(afe, substream->runtime->rate);
 	if (ret)
 		return ret;
 	mt8173_afe_set_i2s_enable(afe, true);
 	return 0;
 }",21,157
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_i2s_path_prepare_enable,60,58," static int mt2701_i2s_path_prepare_enable(struct snd_pcm_substream *substream,
 					  struct snd_soc_dai *dai,
 					  int i2s_num,
 					  int dir_invert)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt2701_afe_private *afe_priv = afe->platform_priv;
 	struct mt2701_i2s_path *i2s_path = &afe_priv->i2s_path[i2s_num];
 	const struct mt2701_i2s_data *i2s_data;
 	struct snd_pcm_runtime * const runtime = substream->runtime;
 	int reg, fs, w_len = 1; 
 	int stream_dir = substream->stream;
 	unsigned int mask = 0, val = 0;
 	if (dir_invert) {
 		if (stream_dir == SNDRV_PCM_STREAM_PLAYBACK)
 			stream_dir = SNDRV_PCM_STREAM_CAPTURE;
 		else
 			stream_dir = SNDRV_PCM_STREAM_PLAYBACK;
 	}
 	i2s_data = i2s_path->i2s_data[stream_dir];
 	
 	i2s_path->on[stream_dir]++;
 	if (i2s_path->on[stream_dir] != 1)
 		return 0;
 	fs = mt2701_afe_i2s_fs(runtime->rate);
 	mask = ASYS_I2S_CON_FS |
 	       ASYS_I2S_CON_I2S_COUPLE_MODE | 
 	       ASYS_I2S_CON_I2S_MODE |
 	       ASYS_I2S_CON_WIDE_MODE;
 	val = ASYS_I2S_CON_FS_SET(fs) |
 	      ASYS_I2S_CON_I2S_MODE |
 	      ASYS_I2S_CON_WIDE_MODE_SET(w_len);
 	if (stream_dir == SNDRV_PCM_STREAM_CAPTURE) {
 		mask |= ASYS_I2S_IN_PHASE_FIX;
 		val |= ASYS_I2S_IN_PHASE_FIX;
 	}
 	regmap_update_bits(afe->regmap, i2s_data->i2s_ctrl_reg, mask, val);
 	if (stream_dir == SNDRV_PCM_STREAM_PLAYBACK)
 		reg = ASMO_TIMING_CON1;
 	else
 		reg = ASMI_TIMING_CON1;
 	regmap_update_bits(afe->regmap, reg,
 			   i2s_data->i2s_asrc_fs_mask
 			   << i2s_data->i2s_asrc_fs_shift,
 			   fs << i2s_data->i2s_asrc_fs_shift);
 	
 	mt2701_afe_enable_i2s(afe, i2s_num, stream_dir);
 	
 	regmap_update_bits(afe->regmap, i2s_data->i2s_ctrl_reg,
 			   ASYS_I2S_CON_RESET, ASYS_I2S_CON_RESET);
 	udelay(1);
 	regmap_update_bits(afe->regmap, i2s_data->i2s_ctrl_reg,
 			   ASYS_I2S_CON_RESET, 0);
 	udelay(1);
 	regmap_update_bits(afe->regmap, i2s_data->i2s_ctrl_reg,
 			   ASYS_I2S_CON_I2S_EN, ASYS_I2S_CON_I2S_EN);
 	return 0;
 }",61,352
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_dlm_fe_hw_params,22,20," static int mt2701_dlm_fe_hw_params(struct snd_pcm_substream *substream,
 				   struct snd_pcm_hw_params *params,
 				   struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	int channels = params_channels(params);
 	regmap_update_bits(afe->regmap,
 			   AFE_MEMIF_PBUF_SIZE,
 			   AFE_MEMIF_PBUF_SIZE_DLM_MASK,
 			   AFE_MEMIF_PBUF_SIZE_FULL_INTERLEAVE);
 	regmap_update_bits(afe->regmap,
 			   AFE_MEMIF_PBUF_SIZE,
 			   AFE_MEMIF_PBUF_SIZE_DLM_BYTE_MASK,
 			   AFE_MEMIF_PBUF_SIZE_DLM_32BYTES);
 	regmap_update_bits(afe->regmap,
 			   AFE_MEMIF_PBUF_SIZE,
 			   AFE_MEMIF_PBUF_SIZE_DLM_CH_MASK,
 			   AFE_MEMIF_PBUF_SIZE_DLM_CH(channels));
 	return mtk_afe_fe_hw_params(substream, params, dai);
 }",23,126
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_afe_i2s_prepare,39,37," static int mt2701_afe_i2s_prepare(struct snd_pcm_substream *substream,
 				  struct snd_soc_dai *dai)
 {
 	int clk_domain;
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt2701_afe_private *afe_priv = afe->platform_priv;
 	int i2s_num = mt2701_dai_num_to_i2s(afe, dai->id);
 	struct mt2701_i2s_path *i2s_path;
 	int mclk_rate;
 	if (i2s_num < 0)
 		return i2s_num;
 	i2s_path = &afe_priv->i2s_path[i2s_num];
 	mclk_rate = i2s_path->mclk_rate;
 	if (i2s_path->occupied[substream->stream])
 		return -EBUSY;
 	i2s_path->occupied[substream->stream] = 1;
 	if (MT2701_PLL_DOMAIN_0_RATE % mclk_rate == 0) {
 		clk_domain = 0;
 	} else if (MT2701_PLL_DOMAIN_1_RATE % mclk_rate == 0) {
 		clk_domain = 1;
 	} else {
 		dev_err(dai->dev, ""%s() bad mclk rate %d\n"",
 			__func__, mclk_rate);
 		return -EINVAL;
 	}
 	mt2701_mclk_configuration(afe, i2s_num, clk_domain, mclk_rate);
 	if (substream->stream == SNDRV_PCM_STREAM_PLAYBACK) {
 		mt2701_i2s_path_prepare_enable(substream, dai, i2s_num, 0);
 	} else {
 		
 		
 		mt2701_i2s_path_prepare_enable(substream, dai, i2s_num, 1);
 		
 		mt2701_i2s_path_prepare_enable(substream, dai, i2s_num, 0);
 	}
 	return 0;
 }",40,256
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_simple_fe_hw_params,17,15," static int mt2701_simple_fe_hw_params(struct snd_pcm_substream *substream,
 				      struct snd_pcm_hw_params *params,
 				      struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	int stream_dir = substream->stream;
 	
 	if (stream_dir == SNDRV_PCM_STREAM_PLAYBACK) {
 		regmap_update_bits(afe->regmap,
 				   AFE_MEMIF_PBUF_SIZE,
 				   AFE_MEMIF_PBUF_SIZE_DLM_MASK,
 				   AFE_MEMIF_PBUF_SIZE_PAIR_INTERLEAVE);
 	}
 	return mtk_afe_fe_hw_params(substream, params, dai);
 }",18,104
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt8173-afe-pcm_mt8173_afe_hdmi_startup,13,11," static int mt8173_afe_hdmi_startup(struct snd_pcm_substream *substream,
 				   struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	struct mt8173_afe_private *afe_priv = afe->platform_priv;
 	if (dai->active)
 		return 0;
 	mt8173_afe_dais_enable_clks(afe, afe_priv->clocks[MT8173_CLK_I2S3_M],
 				    afe_priv->clocks[MT8173_CLK_I2S3_B]);
 	return 0;
 }",14,101
snd_soc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/snd_soc/1524629994_2018-04-25_e4b31b816c47_mt2701-afe-pcm_mt2701_dlm_fe_shutdown,17,15," static void mt2701_dlm_fe_shutdown(struct snd_pcm_substream *substream,
 				   struct snd_soc_dai *dai)
 {
-	struct snd_soc_pcm_runtime *rtd = substream->private_data;
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
-	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(component);
+	struct mtk_base_afe *afe = snd_soc_dai_get_drvdata(dai);
 	const struct mtk_base_memif_data *memif_data;
 	int i;
 	for (i = MT2701_MEMIF_DL1; i < MT2701_MEMIF_DL_SINGLE_NUM; ++i) {
 		memif_data = afe->memif[i].data;
 		regmap_update_bits(afe->regmap,
 				   memif_data->agent_disable_reg,
 				   1 << memif_data->agent_disable_shift,
 				   1 << memif_data->agent_disable_shift);
 	}
 	return mtk_afe_fe_shutdown(substream, dai);
 }",18,127
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_qxl_object_qxl_bo_create,42,42," int qxl_bo_create(struct qxl_device *qdev,
 		  unsigned long size, bool kernel, bool pinned, u32 domain,
 		  struct qxl_surface *surf,
 		  struct qxl_bo **bo_ptr)
 {
 	struct qxl_bo *bo;
 	enum ttm_bo_type type;
 	int r;
 	if (kernel)
 		type = ttm_bo_type_kernel;
 	else
 		type = ttm_bo_type_device;
 	*bo_ptr = NULL;
 	bo = kzalloc(sizeof(struct qxl_bo), GFP_KERNEL);
 	if (bo == NULL)
 		return -ENOMEM;
 	size = roundup(size, PAGE_SIZE);
 	r = drm_gem_object_init(&qdev->ddev, &bo->gem_base, size);
 	if (unlikely(r)) {
 		kfree(bo);
 		return r;
 	}
 	bo->type = domain;
 	bo->pin_count = pinned ? 1 : 0;
 	bo->surface_id = 0;
 	INIT_LIST_HEAD(&bo->list);
 	if (surf)
 		bo->surf = *surf;
 	qxl_ttm_placement_from_domain(bo, domain, pinned);
 	r = ttm_bo_init(&qdev->mman.bdev, &bo->tbo, size, type,
-			&bo->placement, 0, !kernel, NULL, size,
+			&bo->placement, 0, !kernel, size,
 			NULL, NULL, &qxl_ttm_bo_destroy);
 	if (unlikely(r != 0)) {
 		if (r != -ERESTARTSYS)
 			dev_err(qdev->ddev.dev,
 				""object_init failed for (%lu, 0x%08X)\n"",
 				size, domain);
 		return r;
 	}
 	*bo_ptr = bo;
 	return 0;
 }",43,283
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_nouveau_bo_nouveau_bo_new,73,73," int nouveau_bo_new(struct nouveau_cli *cli, u64 size, int align, uint32_t flags, uint32_t tile_mode, uint32_t tile_flags, struct sg_table *sg, struct reservation_object *robj, struct nouveau_bo **pnvbo) {
   struct nouveau_drm *drm = cli->drm;
   struct nouveau_bo *nvbo;
   struct nvif_mmu *mmu = &cli->mmu;
   struct nvif_vmm *vmm = &cli->vmm.vmm;
   size_t acc_size;
   int type = ttm_bo_type_device;
   int ret, i, pi = -1;
   if (!size) {
     NV_WARN(drm, ""skipped size %016llx\n"", size);
     return -EINVAL;
   }
   if (sg) type = ttm_bo_type_sg;
   nvbo = kzalloc(sizeof(struct nouveau_bo), GFP_KERNEL);
   if (!nvbo) return -ENOMEM;
   INIT_LIST_HEAD(&nvbo->head);
   INIT_LIST_HEAD(&nvbo->entry);
   INIT_LIST_HEAD(&nvbo->vma_list);
   nvbo->bo.bdev = &drm->ttm.bdev;
   nvbo->cli = cli;
   
   if (flags & TTM_PL_FLAG_UNCACHED) {
     
     if (!nouveau_drm_use_coherent_gpu_mapping(drm)) nvbo->force_coherent = true;
   }
   if (cli->device.info.family >= NV_DEVICE_INFO_V0_FERMI) {
     nvbo->kind = (tile_flags & 0x0000ff00) >> 8;
     if (!nvif_mmu_kind_valid(mmu, nvbo->kind)) {
       kfree(nvbo);
       return -EINVAL;
     }
     nvbo->comp = mmu->kind[nvbo->kind] != nvbo->kind;
   } else if (cli->device.info.family >= NV_DEVICE_INFO_V0_TESLA) {
     nvbo->kind = (tile_flags & 0x00007f00) >> 8;
     nvbo->comp = (tile_flags & 0x00030000) >> 16;
     if (!nvif_mmu_kind_valid(mmu, nvbo->kind)) {
       kfree(nvbo);
       return -EINVAL;
     }
   } else {
     nvbo->zeta = (tile_flags & 0x00000007);
   }
   nvbo->mode = tile_mode;
   nvbo->contig = !(tile_flags & NOUVEAU_GEM_TILE_NONCONTIG);
   
   for (i = 0; i < vmm->page_nr; i++) {
     
     if (cli->device.info.family > NV_DEVICE_INFO_V0_CURIE && (flags & TTM_PL_FLAG_VRAM) && !vmm->page[i].vram) continue;
     if ((flags & TTM_PL_FLAG_TT) && (!vmm->page[i].host || vmm->page[i].shift > PAGE_SHIFT)) continue;
     
     if (pi < 0 || !nvbo->comp || vmm->page[i].comp) pi = i;
     
     if (size >= 1ULL << vmm->page[i].shift) break;
   }
   if (WARN_ON(pi < 0)) return -EINVAL;
   
   if (nvbo->comp && !vmm->page[pi].comp) {
     if (mmu->object.oclass >= NVIF_CLASS_MMU_GF100) nvbo->kind = mmu->kind[nvbo->kind];
     nvbo->comp = 0;
   }
   nvbo->page = vmm->page[pi].shift;
   nouveau_bo_fixup_align(nvbo, flags, &align, &size);
   nvbo->bo.mem.num_pages = size >> PAGE_SHIFT;
   nouveau_bo_placement_set(nvbo, flags, 0);
   acc_size = ttm_bo_dma_acc_size(&drm->ttm.bdev, size, sizeof(struct nouveau_bo));
-  ret = ttm_bo_init(&drm->ttm.bdev, &nvbo->bo, size, type, &nvbo->placement, align >> PAGE_SHIFT, false, NULL, acc_size, sg, robj, nouveau_bo_del_ttm);
+  ret = ttm_bo_init(&drm->ttm.bdev, &nvbo->bo, size, type, &nvbo->placement, align >> PAGE_SHIFT, false, acc_size, sg, robj, nouveau_bo_del_ttm);
   if (ret) {
     
     return ret;
   }
   *pnvbo = nvbo;
   return 0;
 }",74,719
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_ast_ttm_ast_bo_create,29,29," int ast_bo_create(struct drm_device *dev, int size, int align,
 		  uint32_t flags, struct ast_bo **pastbo)
 {
 	struct ast_private *ast = dev->dev_private;
 	struct ast_bo *astbo;
 	size_t acc_size;
 	int ret;
 	astbo = kzalloc(sizeof(struct ast_bo), GFP_KERNEL);
 	if (!astbo)
 		return -ENOMEM;
 	ret = drm_gem_object_init(dev, &astbo->gem, size);
 	if (ret)
 		goto error;
 	astbo->bo.bdev = &ast->ttm.bdev;
 	ast_ttm_placement(astbo, TTM_PL_FLAG_VRAM | TTM_PL_FLAG_SYSTEM);
 	acc_size = ttm_bo_dma_acc_size(&ast->ttm.bdev, size,
 				       sizeof(struct ast_bo));
 	ret = ttm_bo_init(&ast->ttm.bdev, &astbo->bo, size,
 			  ttm_bo_type_device, &astbo->placement,
-			  align >> PAGE_SHIFT, false, NULL, acc_size,
+			  align >> PAGE_SHIFT, false, acc_size,
 			  NULL, NULL, ast_bo_ttm_destroy);
 	if (ret)
 		goto error;
 	*pastbo = astbo;
 	return 0;
 error:
 	kfree(astbo);
 	return ret;
 }",30,207
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_hibmc_ttm_hibmc_bo_create,28,28," int hibmc_bo_create(struct drm_device *dev, int size, int align, u32 flags, struct hibmc_bo **phibmcbo) {
   struct hibmc_drm_private *hibmc = dev->dev_private;
   struct hibmc_bo *hibmcbo;
   size_t acc_size;
   int ret;
   hibmcbo = kzalloc(sizeof(*hibmcbo), GFP_KERNEL);
   if (!hibmcbo) {
     DRM_ERROR(""failed to allocate hibmcbo\n"");
     return -ENOMEM;
   }
   ret = drm_gem_object_init(dev, &hibmcbo->gem, size);
   if (ret) {
     DRM_ERROR(""failed to initialize drm gem object: %d\n"", ret);
     kfree(hibmcbo);
     return ret;
   }
   hibmcbo->bo.bdev = &hibmc->bdev;
   hibmc_ttm_placement(hibmcbo, TTM_PL_FLAG_VRAM | TTM_PL_FLAG_SYSTEM);
   acc_size = ttm_bo_dma_acc_size(&hibmc->bdev, size, sizeof(struct hibmc_bo));
-  ret = ttm_bo_init(&hibmc->bdev, &hibmcbo->bo, size, ttm_bo_type_device, &hibmcbo->placement, align >> PAGE_SHIFT, false, NULL, acc_size, NULL, NULL, hibmc_bo_ttm_destroy);
+  ret = ttm_bo_init(&hibmc->bdev, &hibmcbo->bo, size, ttm_bo_type_device, &hibmcbo->placement, align >> PAGE_SHIFT, false, acc_size, NULL, NULL, hibmc_bo_ttm_destroy);
   if (ret) {
     hibmc_bo_unref(&hibmcbo);
     DRM_ERROR(""failed to initialize ttm_bo: %d\n"", ret);
     return ret;
   }
   *phibmcbo = hibmcbo;
   return 0;
 }",29,266
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_mgag200_ttm_mgag200_bo_create,20,20," int mgag200_bo_create(struct drm_device *dev, int size, int align, uint32_t flags, struct mgag200_bo **pmgabo) {
   struct mga_device *mdev = dev->dev_private;
   struct mgag200_bo *mgabo;
   size_t acc_size;
   int ret;
   mgabo = kzalloc(sizeof(struct mgag200_bo), GFP_KERNEL);
   if (!mgabo) return -ENOMEM;
   ret = drm_gem_object_init(dev, &mgabo->gem, size);
   if (ret) {
     kfree(mgabo);
     return ret;
   }
   mgabo->bo.bdev = &mdev->ttm.bdev;
   mgag200_ttm_placement(mgabo, TTM_PL_FLAG_VRAM | TTM_PL_FLAG_SYSTEM);
   acc_size = ttm_bo_dma_acc_size(&mdev->ttm.bdev, size, sizeof(struct mgag200_bo));
-  ret = ttm_bo_init(&mdev->ttm.bdev, &mgabo->bo, size, ttm_bo_type_device, &mgabo->placement, align >> PAGE_SHIFT, false, NULL, acc_size, NULL, NULL, mgag200_bo_ttm_destroy);
+  ret = ttm_bo_init(&mdev->ttm.bdev, &mgabo->bo, size, ttm_bo_type_device, &mgabo->placement, align >> PAGE_SHIFT, false, acc_size, NULL, NULL, mgag200_bo_ttm_destroy);
   if (ret) return ret;
   *pmgabo = mgabo;
   return 0;
 }",21,236
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_virtgpu_object_virtio_gpu_object_create,27,27," int virtio_gpu_object_create(struct virtio_gpu_device *vgdev, unsigned long size, bool kernel, bool pinned, struct virtio_gpu_object **bo_ptr) {
   struct virtio_gpu_object *bo;
   enum ttm_bo_type type;
   size_t acc_size;
   int ret;
   if (kernel)
     type = ttm_bo_type_kernel;
   else
     type = ttm_bo_type_device;
   *bo_ptr = NULL;
   acc_size = ttm_bo_dma_acc_size(&vgdev->mman.bdev, size, sizeof(struct virtio_gpu_object));
   bo = kzalloc(sizeof(struct virtio_gpu_object), GFP_KERNEL);
   if (bo == NULL) return -ENOMEM;
   size = roundup(size, PAGE_SIZE);
   ret = drm_gem_object_init(vgdev->ddev, &bo->gem_base, size);
   if (ret != 0) {
     kfree(bo);
     return ret;
   }
   bo->dumb = false;
   virtio_gpu_init_ttm_placement(bo, pinned);
-  ret = ttm_bo_init(&vgdev->mman.bdev, &bo->tbo, size, type, &bo->placement, 0, !kernel, NULL, acc_size, NULL, NULL, &virtio_gpu_ttm_bo_destroy);
+  ret = ttm_bo_init(&vgdev->mman.bdev, &bo->tbo, size, type, &bo->placement, 0, !kernel, acc_size, NULL, NULL, &virtio_gpu_ttm_bo_destroy);
   
   if (ret != 0) return ret;
   *bo_ptr = bo;
   return 0;
 }",28,257
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_cirrus_ttm_cirrus_bo_create,20,20," int cirrus_bo_create(struct drm_device *dev, int size, int align, uint32_t flags, struct cirrus_bo **pcirrusbo) {
   struct cirrus_device *cirrus = dev->dev_private;
   struct cirrus_bo *cirrusbo;
   size_t acc_size;
   int ret;
   cirrusbo = kzalloc(sizeof(struct cirrus_bo), GFP_KERNEL);
   if (!cirrusbo) return -ENOMEM;
   ret = drm_gem_object_init(dev, &cirrusbo->gem, size);
   if (ret) {
     kfree(cirrusbo);
     return ret;
   }
   cirrusbo->bo.bdev = &cirrus->ttm.bdev;
   cirrus_ttm_placement(cirrusbo, TTM_PL_FLAG_VRAM | TTM_PL_FLAG_SYSTEM);
   acc_size = ttm_bo_dma_acc_size(&cirrus->ttm.bdev, size, sizeof(struct cirrus_bo));
-  ret = ttm_bo_init(&cirrus->ttm.bdev, &cirrusbo->bo, size, ttm_bo_type_device, &cirrusbo->placement, align >> PAGE_SHIFT, false, NULL, acc_size, NULL, NULL, cirrus_bo_ttm_destroy);
+  ret = ttm_bo_init(&cirrus->ttm.bdev, &cirrusbo->bo, size, ttm_bo_type_device, &cirrusbo->placement, align >> PAGE_SHIFT, false, acc_size, NULL, NULL, cirrus_bo_ttm_destroy);
   if (ret) return ret;
   *pcirrusbo = cirrusbo;
   return 0;
 }",21,236
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_bochs_mm_bochs_bo_create,21,21," static int bochs_bo_create(struct drm_device *dev, int size, int align, uint32_t flags, struct bochs_bo **pbochsbo) {
   struct bochs_device *bochs = dev->dev_private;
   struct bochs_bo *bochsbo;
   size_t acc_size;
   int ret;
   bochsbo = kzalloc(sizeof(struct bochs_bo), GFP_KERNEL);
   if (!bochsbo) return -ENOMEM;
   ret = drm_gem_object_init(dev, &bochsbo->gem, size);
   if (ret) {
     kfree(bochsbo);
     return ret;
   }
   bochsbo->bo.bdev = &bochs->ttm.bdev;
   bochsbo->bo.bdev->dev_mapping = dev->anon_inode->i_mapping;
   bochs_ttm_placement(bochsbo, TTM_PL_FLAG_VRAM | TTM_PL_FLAG_SYSTEM);
   acc_size = ttm_bo_dma_acc_size(&bochs->ttm.bdev, size, sizeof(struct bochs_bo));
-  ret = ttm_bo_init(&bochs->ttm.bdev, &bochsbo->bo, size, ttm_bo_type_device, &bochsbo->placement, align >> PAGE_SHIFT, false, NULL, acc_size, NULL, NULL, bochs_bo_ttm_destroy);
+  ret = ttm_bo_init(&bochs->ttm.bdev, &bochsbo->bo, size, ttm_bo_type_device, &bochsbo->placement, align >> PAGE_SHIFT, false, acc_size, NULL, NULL, bochs_bo_ttm_destroy);
   if (ret) return ret;
   *pbochsbo = bochsbo;
   return 0;
 }",22,251
ttm_bo_init-60,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/ttm_bo_init-60/1519311151_2018-02-22_724daa4fd65d_vbox_ttm_vbox_bo_create,20,20," int vbox_bo_create(struct drm_device *dev, int size, int align, u32 flags, struct vbox_bo **pvboxbo) {
   struct vbox_private *vbox = dev->dev_private;
   struct vbox_bo *vboxbo;
   size_t acc_size;
   int ret;
   vboxbo = kzalloc(sizeof(*vboxbo), GFP_KERNEL);
   if (!vboxbo) return -ENOMEM;
   ret = drm_gem_object_init(dev, &vboxbo->gem, size);
   if (ret) goto err_free_vboxbo;
   vboxbo->bo.bdev = &vbox->ttm.bdev;
   vbox_ttm_placement(vboxbo, TTM_PL_FLAG_VRAM | TTM_PL_FLAG_SYSTEM);
   acc_size = ttm_bo_dma_acc_size(&vbox->ttm.bdev, size, sizeof(struct vbox_bo));
-  ret = ttm_bo_init(&vbox->ttm.bdev, &vboxbo->bo, size, ttm_bo_type_device, &vboxbo->placement, align >> PAGE_SHIFT, false, NULL, acc_size, NULL, NULL, vbox_bo_ttm_destroy);
+  ret = ttm_bo_init(&vbox->ttm.bdev, &vboxbo->bo, size, ttm_bo_type_device, &vboxbo->placement, align >> PAGE_SHIFT, false, acc_size, NULL, NULL, vbox_bo_ttm_destroy);
   if (ret) goto err_free_vboxbo;
   *pvboxbo = vboxbo;
   return 0;
 err_free_vboxbo:
   kfree(vboxbo);
   return ret;
 }",21,239
uartlite,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/uartlite/1360605874_2013-02-11_6d53c3b71d32_uartlite_ulite_get_poll_char,7,7," static int ulite_get_poll_char(struct uart_port *port)
 {
-	if (!(ioread32be(port->membase + ULITE_STATUS)
+	if (!(uart_in32(ULITE_STATUS, port)
 						& ULITE_STATUS_RXVALID))
 		return NO_POLL_CHAR;
-	return ioread32be(port->membase + ULITE_RX);
+	return uart_in32(ULITE_RX, port);
 }",9,62
uartlite,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/uartlite/1360605874_2013-02-11_6d53c3b71d32_uartlite_ulite_receive,34,34," static int ulite_receive(struct uart_port *port, int stat)
 {
 	struct tty_port *tport = &port->state->port;
 	unsigned char ch = 0;
 	char flag = TTY_NORMAL;
 	if ((stat & (ULITE_STATUS_RXVALID | ULITE_STATUS_OVERRUN
 		     | ULITE_STATUS_FRAME)) == 0)
 		return 0;
 	
 	if (stat & ULITE_STATUS_RXVALID) {
 		port->icount.rx++;
-		ch = ioread32be(port->membase + ULITE_RX);
+		ch = uart_in32(ULITE_RX, port);
 		if (stat & ULITE_STATUS_PARITY)
 			port->icount.parity++;
 	}
 	if (stat & ULITE_STATUS_OVERRUN)
 		port->icount.overrun++;
 	if (stat & ULITE_STATUS_FRAME)
 		port->icount.frame++;
 	
 	if (stat & port->ignore_status_mask & ULITE_STATUS_PARITY)
 		stat &= ~ULITE_STATUS_RXVALID;
 	stat &= port->read_status_mask;
 	if (stat & ULITE_STATUS_PARITY)
 		flag = TTY_PARITY;
 	stat &= ~port->ignore_status_mask;
 	if (stat & ULITE_STATUS_RXVALID)
 		tty_insert_flip_char(tport, ch, flag);
 	if (stat & ULITE_STATUS_FRAME)
 		tty_insert_flip_char(tport, 0, TTY_FRAME);
 	if (stat & ULITE_STATUS_OVERRUN)
 		tty_insert_flip_char(tport, 0, TTY_OVERRUN);
 	return 1;
 }",35,218
uartlite,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/uartlite/1360605874_2013-02-11_6d53c3b71d32_uartlite_ulite_isr,18,18," static irqreturn_t ulite_isr(int irq, void *dev_id)
 {
 	struct uart_port *port = dev_id;
 	int busy, n = 0;
 	do {
-		int stat = ioread32be(port->membase + ULITE_STATUS);
+		int stat = uart_in32(ULITE_STATUS, port);
 		busy  = ulite_receive(port, stat);
 		busy |= ulite_transmit(port, stat);
 		n++;
 	} while (busy);
 	
 	if (n > 1) {
 		tty_flip_buffer_push(&port->state->port);
 		return IRQ_HANDLED;
 	} else {
 		return IRQ_NONE;
 	}
 }",19,107
uartlite,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/uartlite/1360605874_2013-02-11_6d53c3b71d32_uartlite_ulite_start_tx,4,4," static void ulite_start_tx(struct uart_port *port)
 {
-	ulite_transmit(port, ioread32be(port->membase + ULITE_STATUS));
+	ulite_transmit(port, uart_in32(ULITE_STATUS, port));
 }",5,39
uartlite,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/uartlite/1360605874_2013-02-11_6d53c3b71d32_uartlite_ulite_tx_empty,9,9," static unsigned int ulite_tx_empty(struct uart_port *port)
 {
 	unsigned long flags;
 	unsigned int ret;
 	spin_lock_irqsave(&port->lock, flags);
-	ret = ioread32be(port->membase + ULITE_STATUS);
+	ret = uart_in32(ULITE_STATUS, port);
 	spin_unlock_irqrestore(&port->lock, flags);
 	return ret & ULITE_STATUS_TXEMPTY ? TIOCSER_TEMT : 0;
 }",10,71
uartlite,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/uartlite/1360605874_2013-02-11_6d53c3b71d32_uartlite_ulite_console_wait_tx,12,12," static void ulite_console_wait_tx(struct uart_port *port)
 {
 	int i;
 	u8 val;
 	
 	for (i = 0; i < 100000; i++) {
-		val = ioread32be(port->membase + ULITE_STATUS);
+		val = uart_in32(ULITE_STATUS, port);
 		if ((val & ULITE_STATUS_TXFULL) == 0)
 			break;
 		cpu_relax();
 	}
 }",13,70
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_code-reading_do_test_code_reading,146,146," static int do_test_code_reading(bool try_kcore)
 {
 	struct machine *machine;
 	struct thread *thread;
 	struct record_opts opts = {
 		.mmap_pages	     = UINT_MAX,
 		.user_freq	     = UINT_MAX,
 		.user_interval	     = ULLONG_MAX,
 		.freq		     = 500,
 		.target		     = {
 			.uses_mmap   = true,
 		},
 	};
 	struct state state = {
 		.done_cnt = 0,
 	};
 	struct thread_map *threads = NULL;
 	struct cpu_map *cpus = NULL;
 	struct perf_evlist *evlist = NULL;
 	struct perf_evsel *evsel = NULL;
 	int err = -1, ret;
 	pid_t pid;
 	struct map *map;
 	bool have_vmlinux, have_kcore, excl_kernel = false;
 	pid = getpid();
 	machine = machine__new_host();
 	ret = machine__create_kernel_maps(machine);
 	if (ret < 0) {
 		pr_debug(""machine__create_kernel_maps failed\n"");
 		goto out_err;
 	}
 	
 	if (try_kcore)
 		symbol_conf.kallsyms_name = ""/proc/kallsyms"";
 	
 	map = machine__kernel_map(machine);
 	ret = map__load(map);
 	if (ret < 0) {
 		pr_debug(""map__load failed\n"");
 		goto out_err;
 	}
 	have_vmlinux = dso__is_vmlinux(map->dso);
 	have_kcore = dso__is_kcore(map->dso);
 	
 	if (try_kcore && !have_kcore)
 		return TEST_CODE_READING_NO_KCORE;
 	
 	if (!have_vmlinux && !have_kcore)
 		excl_kernel = true;
 	threads = thread_map__new_by_tid(pid);
 	if (!threads) {
 		pr_debug(""thread_map__new_by_tid failed\n"");
 		goto out_err;
 	}
 	ret = perf_event__synthesize_thread_map(NULL, threads,
 						perf_event__process, machine, false, 500);
 	if (ret < 0) {
 		pr_debug(""perf_event__synthesize_thread_map failed\n"");
 		goto out_err;
 	}
 	thread = machine__findnew_thread(machine, pid, pid);
 	if (!thread) {
 		pr_debug(""machine__findnew_thread failed\n"");
 		goto out_put;
 	}
 	cpus = cpu_map__new(NULL);
 	if (!cpus) {
 		pr_debug(""cpu_map__new failed\n"");
 		goto out_put;
 	}
 	while (1) {
 		const char *str;
 		evlist = perf_evlist__new();
 		if (!evlist) {
 			pr_debug(""perf_evlist__new failed\n"");
 			goto out_put;
 		}
 		perf_evlist__set_maps(evlist, cpus, threads);
 		if (excl_kernel)
 			str = ""cycles:u"";
 		else
 			str = ""cycles"";
 		pr_debug(""Parsing event '%s'\n"", str);
 		ret = parse_events(evlist, str, NULL);
 		if (ret < 0) {
 			pr_debug(""parse_events failed\n"");
 			goto out_put;
 		}
 		perf_evlist__config(evlist, &opts, NULL);
 		evsel = perf_evlist__first(evlist);
 		evsel->attr.comm = 1;
 		evsel->attr.disabled = 1;
 		evsel->attr.enable_on_exec = 0;
 		ret = perf_evlist__open(evlist);
 		if (ret < 0) {
 			if (!excl_kernel) {
 				excl_kernel = true;
 				
 				cpu_map__get(cpus);
 				thread_map__get(threads);
 				perf_evlist__set_maps(evlist, NULL, NULL);
 				perf_evlist__delete(evlist);
 				evlist = NULL;
 				continue;
 			}
 			if (verbose > 0) {
 				char errbuf[512];
 				perf_evlist__strerror_open(evlist, errno, errbuf, sizeof(errbuf));
 				pr_debug(""perf_evlist__open() failed!\n%s\n"", errbuf);
 			}
 			goto out_put;
 		}
 		break;
 	}
-	ret = perf_evlist__mmap(evlist, UINT_MAX, false);
+	ret = perf_evlist__mmap(evlist, UINT_MAX);
 	if (ret < 0) {
 		pr_debug(""perf_evlist__mmap failed\n"");
 		goto out_put;
 	}
 	perf_evlist__enable(evlist);
 	do_something();
 	perf_evlist__disable(evlist);
 	ret = process_events(machine, evlist, &state);
 	if (ret < 0)
 		goto out_put;
 	if (!have_vmlinux && !have_kcore && !try_kcore)
 		err = TEST_CODE_READING_NO_KERNEL_OBJ;
 	else if (!have_vmlinux && !try_kcore)
 		err = TEST_CODE_READING_NO_VMLINUX;
 	else if (excl_kernel)
 		err = TEST_CODE_READING_NO_ACCESS;
 	else
 		err = TEST_CODE_READING_OK;
 out_put:
 	thread__put(thread);
 out_err:
 	if (evlist) {
 		perf_evlist__delete(evlist);
 	} else {
 		cpu_map__put(cpus);
 		thread_map__put(threads);
 	}
 	machine__delete_threads(machine);
 	machine__delete(machine);
 	return err;
 }",147,766
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_sw-clock___test__sw_clock_freq,87,87," static int __test__sw_clock_freq(enum perf_sw_ids clock_id)
 {
 	int i, err = -1;
 	volatile int tmp = 0;
 	u64 total_periods = 0;
 	int nr_samples = 0;
 	char sbuf[STRERR_BUFSIZE];
 	union perf_event *event;
 	struct perf_evsel *evsel;
 	struct perf_evlist *evlist;
 	struct perf_event_attr attr = {
 		.type = PERF_TYPE_SOFTWARE,
 		.config = clock_id,
 		.sample_type = PERF_SAMPLE_PERIOD,
 		.exclude_kernel = 1,
 		.disabled = 1,
 		.freq = 1,
 	};
 	struct cpu_map *cpus;
 	struct thread_map *threads;
 	attr.sample_freq = 500;
 	evlist = perf_evlist__new();
 	if (evlist == NULL) {
 		pr_debug(""perf_evlist__new\n"");
 		return -1;
 	}
 	evsel = perf_evsel__new(&attr);
 	if (evsel == NULL) {
 		pr_debug(""perf_evsel__new\n"");
 		goto out_delete_evlist;
 	}
 	perf_evlist__add(evlist, evsel);
 	cpus = cpu_map__dummy_new();
 	threads = thread_map__new_by_tid(getpid());
 	if (!cpus || !threads) {
 		err = -ENOMEM;
 		pr_debug(""Not enough memory to create thread/cpu maps\n"");
 		goto out_free_maps;
 	}
 	perf_evlist__set_maps(evlist, cpus, threads);
 	cpus	= NULL;
 	threads = NULL;
 	if (perf_evlist__open(evlist)) {
 		const char *knob = ""/proc/sys/kernel/perf_event_max_sample_rate"";
 		err = -errno;
 		pr_debug(""Couldn't open evlist: %s\nHint: check %s, using %"" PRIu64 "" in this test.\n"",
 			 str_error_r(errno, sbuf, sizeof(sbuf)),
 			 knob, (u64)attr.sample_freq);
 		goto out_delete_evlist;
 	}
-	err = perf_evlist__mmap(evlist, 128, false);
+	err = perf_evlist__mmap(evlist, 128);
 	if (err < 0) {
 		pr_debug(""failed to mmap event: %d (%s)\n"", errno,
 			 str_error_r(errno, sbuf, sizeof(sbuf)));
 		goto out_delete_evlist;
 	}
 	perf_evlist__enable(evlist);
 	
 	for (i = 0; i < NR_LOOPS; i++)
 		tmp++;
 	perf_evlist__disable(evlist);
 	while ((event = perf_evlist__mmap_read(evlist, 0)) != NULL) {
 		struct perf_sample sample;
 		if (event->header.type != PERF_RECORD_SAMPLE)
 			goto next_event;
 		err = perf_evlist__parse_sample(evlist, event, &sample);
 		if (err < 0) {
 			pr_debug(""Error during parse sample\n"");
 			goto out_delete_evlist;
 		}
 		total_periods += sample.period;
 		nr_samples++;
 next_event:
 		perf_evlist__mmap_consume(evlist, 0);
 	}
 	if ((u64) nr_samples == total_periods) {
 		pr_debug(""All (%d) samples have period value of 1!\n"",
 			 nr_samples);
 		err = -1;
 	}
 out_free_maps:
 	cpu_map__put(cpus);
 	thread_map__put(threads);
 out_delete_evlist:
 	perf_evlist__delete(evlist);
 	return err;
 }",88,496
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_bpf_do_test,72,72," static int do_test(struct bpf_object *obj, int (*func)(void), int expect) {
   struct record_opts opts = {
       .target =
           {
               .uid = UINT_MAX,
               .uses_mmap = true,
           },
       .freq = 0,
       .mmap_pages = 256,
       .default_interval = 1,
   };
   char pid[16];
   char sbuf[STRERR_BUFSIZE];
   struct perf_evlist *evlist;
   int i, ret = TEST_FAIL, err = 0, count = 0;
   struct parse_events_state parse_state;
   struct parse_events_error parse_error;
   bzero(&parse_error, sizeof(parse_error));
   bzero(&parse_state, sizeof(parse_state));
   parse_state.error = &parse_error;
   INIT_LIST_HEAD(&parse_state.list);
   err = parse_events_load_bpf_obj(&parse_state, &parse_state.list, obj, NULL);
   if (err || list_empty(&parse_state.list)) {
     pr_debug(""Failed to add events selected by BPF\n"");
     return TEST_FAIL;
   }
   snprintf(pid, sizeof(pid), ""%d"", getpid());
   pid[sizeof(pid) - 1] = '\0';
   opts.target.tid = opts.target.pid = pid;
   
   evlist = perf_evlist__new();
   if (!evlist) {
     pr_debug(""Not enough memory to create evlist\n"");
     return TEST_FAIL;
   }
   err = perf_evlist__create_maps(evlist, &opts.target);
   if (err < 0) {
     pr_debug(""Not enough memory to create thread/cpu maps\n"");
     goto out_delete_evlist;
   }
   perf_evlist__splice_list_tail(evlist, &parse_state.list);
   evlist->nr_groups = parse_state.nr_groups;
   perf_evlist__config(evlist, &opts, NULL);
   err = perf_evlist__open(evlist);
   if (err < 0) {
     pr_debug(""perf_evlist__open: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
-  err = perf_evlist__mmap(evlist, opts.mmap_pages, false);
+  err = perf_evlist__mmap(evlist, opts.mmap_pages);
   if (err < 0) {
     pr_debug(""perf_evlist__mmap: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
   perf_evlist__enable(evlist);
   (*func)();
   perf_evlist__disable(evlist);
   for (i = 0; i < evlist->nr_mmaps; i++) {
     union perf_event *event;
     while ((event = perf_evlist__mmap_read(evlist, i)) != NULL) {
       const u32 type = event->header.type;
       if (type == PERF_RECORD_SAMPLE) count++;
     }
   }
   if (count != expect) {
     pr_debug(""BPF filter result incorrect\n"");
     goto out_delete_evlist;
   }
   ret = TEST_OK;
 out_delete_evlist:
   perf_evlist__delete(evlist);
   return ret;
 }",73,512
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_perf-time-to-tsc_test__perf_time_to_tsc,83,83," int test__perf_time_to_tsc(struct test *test __maybe_unused, int subtest __maybe_unused) {
   struct record_opts opts = {
       .mmap_pages = UINT_MAX,
       .user_freq = UINT_MAX,
       .user_interval = ULLONG_MAX,
       .target =
           {
               .uses_mmap = true,
           },
       .sample_time = true,
   };
   struct thread_map *threads = NULL;
   struct cpu_map *cpus = NULL;
   struct perf_evlist *evlist = NULL;
   struct perf_evsel *evsel = NULL;
   int err = -1, ret, i;
   const char *comm1, *comm2;
   struct perf_tsc_conversion tc;
   struct perf_event_mmap_page *pc;
   union perf_event *event;
   u64 test_tsc, comm1_tsc, comm2_tsc;
   u64 test_time, comm1_time = 0, comm2_time = 0;
   threads = thread_map__new(-1, getpid(), UINT_MAX);
   CHECK_NOT_NULL__(threads);
   cpus = cpu_map__new(NULL);
   CHECK_NOT_NULL__(cpus);
   evlist = perf_evlist__new();
   CHECK_NOT_NULL__(evlist);
   perf_evlist__set_maps(evlist, cpus, threads);
   CHECK__(parse_events(evlist, ""cycles:u"", NULL));
   perf_evlist__config(evlist, &opts, NULL);
   evsel = perf_evlist__first(evlist);
   evsel->attr.comm = 1;
   evsel->attr.disabled = 1;
   evsel->attr.enable_on_exec = 0;
   CHECK__(perf_evlist__open(evlist));
-  CHECK__(perf_evlist__mmap(evlist, UINT_MAX, false));
+  CHECK__(perf_evlist__mmap(evlist, UINT_MAX));
   pc = evlist->mmap[0].base;
   ret = perf_read_tsc_conversion(pc, &tc);
   if (ret) {
     if (ret == -EOPNOTSUPP) {
       fprintf(stderr, "" (not supported)"");
       return 0;
     }
     goto out_err;
   }
   perf_evlist__enable(evlist);
   comm1 = ""Test COMM 1"";
   CHECK__(prctl(PR_SET_NAME, (unsigned long)comm1, 0, 0, 0));
   test_tsc = rdtsc();
   comm2 = ""Test COMM 2"";
   CHECK__(prctl(PR_SET_NAME, (unsigned long)comm2, 0, 0, 0));
   perf_evlist__disable(evlist);
   for (i = 0; i < evlist->nr_mmaps; i++) {
     while ((event = perf_evlist__mmap_read(evlist, i)) != NULL) {
       struct perf_sample sample;
       if (event->header.type != PERF_RECORD_COMM || (pid_t)event->comm.pid != getpid() || (pid_t)event->comm.tid != getpid()) goto next_event;
       if (strcmp(event->comm.comm, comm1) == 0) {
         CHECK__(perf_evsel__parse_sample(evsel, event, &sample));
         comm1_time = sample.time;
       }
       if (strcmp(event->comm.comm, comm2) == 0) {
         CHECK__(perf_evsel__parse_sample(evsel, event, &sample));
         comm2_time = sample.time;
       }
     next_event:
       perf_evlist__mmap_consume(evlist, i);
     }
   }
   if (!comm1_time || !comm2_time) goto out_err;
   test_time = tsc_to_perf_time(test_tsc, &tc);
   comm1_tsc = perf_time_to_tsc(comm1_time, &tc);
   comm2_tsc = perf_time_to_tsc(comm2_time, &tc);
   pr_debug(""1st event perf time %"" PRIu64 "" tsc %"" PRIu64 ""\n"", comm1_time, comm1_tsc);
   pr_debug(""rdtsc          time %"" PRIu64 "" tsc %"" PRIu64 ""\n"", test_time, test_tsc);
   pr_debug(""2nd event perf time %"" PRIu64 "" tsc %"" PRIu64 ""\n"", comm2_time, comm2_tsc);
   if (test_time <= comm1_time || test_time >= comm2_time) goto out_err;
   if (test_tsc <= comm1_tsc || test_tsc >= comm2_tsc) goto out_err;
   err = 0;
 out_err:
   perf_evlist__delete(evlist);
   return err;
 }",84,682
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_builtin-kvm_kvm_live_open_events,43,43," static int kvm_live_open_events(struct perf_kvm_stat *kvm) {
   int err, rc = -1;
   struct perf_evsel *pos;
   struct perf_evlist *evlist = kvm->evlist;
   char sbuf[STRERR_BUFSIZE];
   perf_evlist__config(evlist, &kvm->opts, NULL);
   
   evlist__for_each_entry(evlist, pos) {
     struct perf_event_attr *attr = &pos->attr;
     
     perf_evsel__set_sample_bit(pos, TID);
     perf_evsel__set_sample_bit(pos, TIME);
     perf_evsel__set_sample_bit(pos, CPU);
     perf_evsel__set_sample_bit(pos, RAW);
     
     perf_evsel__reset_sample_bit(pos, PERIOD);
     perf_evsel__reset_sample_bit(pos, IP);
     perf_evsel__reset_sample_bit(pos, CALLCHAIN);
     perf_evsel__reset_sample_bit(pos, ADDR);
     perf_evsel__reset_sample_bit(pos, READ);
     attr->mmap = 0;
     attr->comm = 0;
     attr->task = 0;
     attr->sample_period = 1;
     attr->watermark = 0;
     attr->wakeup_events = 1000;
     
     attr->disabled = 1;
   }
   err = perf_evlist__open(evlist);
   if (err < 0) {
     printf(""Couldn't create the events: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out;
   }
-  if (perf_evlist__mmap(evlist, kvm->opts.mmap_pages, false) < 0) {
+  if (perf_evlist__mmap(evlist, kvm->opts.mmap_pages) < 0) {
     ui__error(""Failed to mmap the events: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     perf_evlist__close(evlist);
     goto out;
   }
   rc = 0;
 out:
   return rc;
 }",44,287
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_keep-tracking_test__keep_tracking,69,69," int test__keep_tracking(struct test *test __maybe_unused, int subtest __maybe_unused) {
   struct record_opts opts = {
       .mmap_pages = UINT_MAX,
       .user_freq = UINT_MAX,
       .user_interval = ULLONG_MAX,
       .target =
           {
               .uses_mmap = true,
           },
   };
   struct thread_map *threads = NULL;
   struct cpu_map *cpus = NULL;
   struct perf_evlist *evlist = NULL;
   struct perf_evsel *evsel = NULL;
   int found, err = -1;
   const char *comm;
   threads = thread_map__new(-1, getpid(), UINT_MAX);
   CHECK_NOT_NULL__(threads);
   cpus = cpu_map__new(NULL);
   CHECK_NOT_NULL__(cpus);
   evlist = perf_evlist__new();
   CHECK_NOT_NULL__(evlist);
   perf_evlist__set_maps(evlist, cpus, threads);
   CHECK__(parse_events(evlist, ""dummy:u"", NULL));
   CHECK__(parse_events(evlist, ""cycles:u"", NULL));
   perf_evlist__config(evlist, &opts, NULL);
   evsel = perf_evlist__first(evlist);
   evsel->attr.comm = 1;
   evsel->attr.disabled = 1;
   evsel->attr.enable_on_exec = 0;
   if (perf_evlist__open(evlist) < 0) {
     pr_debug(""Unable to open dummy and cycles event\n"");
     err = TEST_SKIP;
     goto out_err;
   }
-  CHECK__(perf_evlist__mmap(evlist, UINT_MAX, false));
+  CHECK__(perf_evlist__mmap(evlist, UINT_MAX));
   
   perf_evlist__enable(evlist);
   comm = ""Test COMM 1"";
   CHECK__(prctl(PR_SET_NAME, (unsigned long)comm, 0, 0, 0));
   perf_evlist__disable(evlist);
   found = find_comm(evlist, comm);
   if (found != 1) {
     pr_debug(""First time, failed to find tracking event.\n"");
     goto out_err;
   }
   
   perf_evlist__enable(evlist);
   evsel = perf_evlist__last(evlist);
   CHECK__(perf_evsel__disable(evsel));
   comm = ""Test COMM 2"";
   CHECK__(prctl(PR_SET_NAME, (unsigned long)comm, 0, 0, 0));
   perf_evlist__disable(evlist);
   found = find_comm(evlist, comm);
   if (found != 1) {
     pr_debug(""Seconf time, failed to find tracking event.\n"");
     goto out_err;
   }
   err = 0;
 out_err:
   if (evlist) {
     perf_evlist__disable(evlist);
     perf_evlist__delete(evlist);
   } else {
     cpu_map__put(cpus);
     thread_map__put(threads);
   }
   return err;
 }",70,438
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_backward-ring-buffer_do_test,15,15," static int do_test(struct perf_evlist *evlist, int mmap_pages, int *sample_count, int *comm_count) {
   int err;
   char sbuf[STRERR_BUFSIZE];
-  err = perf_evlist__mmap(evlist, mmap_pages, false);
+  err = perf_evlist__mmap(evlist, mmap_pages);
   if (err < 0) {
     pr_debug(""perf_evlist__mmap: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     return TEST_FAIL;
   }
   perf_evlist__enable(evlist);
   testcase();
   perf_evlist__disable(evlist);
   err = count_samples(evlist, sample_count, comm_count);
   perf_evlist__munmap(evlist);
   return err;
 }",16,117
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_perf-record_test__PERF_RECORD,191,191," int test__PERF_RECORD(struct test *test __maybe_unused, int subtest __maybe_unused) {
   struct record_opts opts = {
       .target =
           {
               .uid = UINT_MAX,
               .uses_mmap = true,
           },
       .no_buffering = true,
       .mmap_pages = 256,
   };
   cpu_set_t cpu_mask;
   size_t cpu_mask_size = sizeof(cpu_mask);
   struct perf_evlist *evlist = perf_evlist__new_dummy();
   struct perf_evsel *evsel;
   struct perf_sample sample;
   const char *cmd = ""sleep"";
   const char *argv[] = {
       cmd,
       ""1"",
       NULL,
   };
   char *bname, *mmap_filename;
   u64 prev_time = 0;
   bool found_cmd_mmap = false, found_libc_mmap = false, found_vdso_mmap = false, found_ld_mmap = false;
   int err = -1, errs = 0, i, wakeups = 0;
   u32 cpu;
   int total_events = 0, nr_events[PERF_RECORD_MAX] = {
                             0,
                         };
   char sbuf[STRERR_BUFSIZE];
   if (evlist == NULL) 
     evlist = perf_evlist__new_default();
   if (evlist == NULL) {
     pr_debug(""Not enough memory to create evlist\n"");
     goto out;
   }
   
   err = perf_evlist__create_maps(evlist, &opts.target);
   if (err < 0) {
     pr_debug(""Not enough memory to create thread/cpu maps\n"");
     goto out_delete_evlist;
   }
   
   err = perf_evlist__prepare_workload(evlist, &opts.target, argv, false, NULL);
   if (err < 0) {
     pr_debug(""Couldn't run the workload!\n"");
     goto out_delete_evlist;
   }
   
   evsel = perf_evlist__first(evlist);
   perf_evsel__set_sample_bit(evsel, CPU);
   perf_evsel__set_sample_bit(evsel, TID);
   perf_evsel__set_sample_bit(evsel, TIME);
   perf_evlist__config(evlist, &opts, NULL);
   err = sched__get_first_possible_cpu(evlist->workload.pid, &cpu_mask);
   if (err < 0) {
     pr_debug(""sched__get_first_possible_cpu: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
   cpu = err;
   
   if (sched_setaffinity(evlist->workload.pid, cpu_mask_size, &cpu_mask) < 0) {
     pr_debug(""sched_setaffinity: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
   
   err = perf_evlist__open(evlist);
   if (err < 0) {
     pr_debug(""perf_evlist__open: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
   
-  err = perf_evlist__mmap(evlist, opts.mmap_pages, false);
+  err = perf_evlist__mmap(evlist, opts.mmap_pages);
   if (err < 0) {
     pr_debug(""perf_evlist__mmap: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
   
   perf_evlist__enable(evlist);
   
   perf_evlist__start_workload(evlist);
   while (1) {
     int before = total_events;
     for (i = 0; i < evlist->nr_mmaps; i++) {
       union perf_event *event;
       while ((event = perf_evlist__mmap_read(evlist, i)) != NULL) {
         const u32 type = event->header.type;
         const char *name = perf_event__name(type);
         ++total_events;
         if (type < PERF_RECORD_MAX) nr_events[type]++;
         err = perf_evlist__parse_sample(evlist, event, &sample);
         if (err < 0) {
           if (verbose > 0) perf_event__fprintf(event, stderr);
           pr_debug(""Couldn't parse sample\n"");
           goto out_delete_evlist;
         }
         if (verbose > 0) {
           pr_info(""%"" PRIu64 "" %d "", sample.time, sample.cpu);
           perf_event__fprintf(event, stderr);
         }
         if (prev_time > sample.time) {
           pr_debug(""%s going backwards in time, prev=%"" PRIu64 "", curr=%"" PRIu64 ""\n"", name, prev_time, sample.time);
           ++errs;
         }
         prev_time = sample.time;
         if (sample.cpu != cpu) {
           pr_debug(""%s with unexpected cpu, expected %d, got %d\n"", name, cpu, sample.cpu);
           ++errs;
         }
         if ((pid_t)sample.pid != evlist->workload.pid) {
           pr_debug(""%s with unexpected pid, expected %d, got %d\n"", name, evlist->workload.pid, sample.pid);
           ++errs;
         }
         if ((pid_t)sample.tid != evlist->workload.pid) {
           pr_debug(""%s with unexpected tid, expected %d, got %d\n"", name, evlist->workload.pid, sample.tid);
           ++errs;
         }
         if ((type == PERF_RECORD_COMM || type == PERF_RECORD_MMAP || type == PERF_RECORD_MMAP2 || type == PERF_RECORD_FORK || type == PERF_RECORD_EXIT) && (pid_t)event->comm.pid != evlist->workload.pid) {
           pr_debug(""%s with unexpected pid/tid\n"", name);
           ++errs;
         }
         if ((type == PERF_RECORD_COMM || type == PERF_RECORD_MMAP || type == PERF_RECORD_MMAP2) && event->comm.pid != event->comm.tid) {
           pr_debug(""%s with different pid/tid!\n"", name);
           ++errs;
         }
         switch (type) {
         case PERF_RECORD_COMM:
           if (strcmp(event->comm.comm, cmd)) {
             pr_debug(""%s with unexpected comm!\n"", name);
             ++errs;
           }
           break;
         case PERF_RECORD_EXIT: goto found_exit;
         case PERF_RECORD_MMAP: mmap_filename = event->mmap.filename; goto check_bname;
         case PERF_RECORD_MMAP2:
           mmap_filename = event->mmap2.filename;
         check_bname:
           bname = strrchr(mmap_filename, '/');
           if (bname != NULL) {
             if (!found_cmd_mmap) found_cmd_mmap = !strcmp(bname + 1, cmd);
             if (!found_libc_mmap) found_libc_mmap = !strncmp(bname + 1, ""libc"", 4);
             if (!found_ld_mmap) found_ld_mmap = !strncmp(bname + 1, ""ld"", 2);
           } else if (!found_vdso_mmap)
             found_vdso_mmap = !strcmp(mmap_filename, ""[vdso]"");
           break;
         case PERF_RECORD_SAMPLE:
           
           break;
         default: pr_debug(""Unexpected perf_event->header.type %d!\n"", type); ++errs;
         }
         perf_evlist__mmap_consume(evlist, i);
       }
     }
     
     if (total_events == before && false) perf_evlist__poll(evlist, -1);
     sleep(1);
     if (++wakeups > 5) {
       pr_debug(""No PERF_RECORD_EXIT event!\n"");
       break;
     }
   }
 found_exit:
   if (nr_events[PERF_RECORD_COMM] > 1) {
     pr_debug(""Excessive number of PERF_RECORD_COMM events!\n"");
     ++errs;
   }
   if (nr_events[PERF_RECORD_COMM] == 0) {
     pr_debug(""Missing PERF_RECORD_COMM for %s!\n"", cmd);
     ++errs;
   }
   if (!found_cmd_mmap) {
     pr_debug(""PERF_RECORD_MMAP for %s missing!\n"", cmd);
     ++errs;
   }
   if (!found_libc_mmap) {
     pr_debug(""PERF_RECORD_MMAP for %s missing!\n"", ""libc"");
     ++errs;
   }
   if (!found_ld_mmap) {
     pr_debug(""PERF_RECORD_MMAP for %s missing!\n"", ""ld"");
     ++errs;
   }
   if (!found_vdso_mmap) {
     pr_debug(""PERF_RECORD_MMAP for %s missing!\n"", ""[vdso]"");
     ++errs;
   }
 out_delete_evlist:
   perf_evlist__delete(evlist);
 out:
   return (err < 0 || errs > 0) ? -1 : 0;
 }",192,1311
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_task-exit_test__task_exit,76,76," int test__task_exit(struct test *test __maybe_unused, int subtest __maybe_unused) {
   int err = -1;
   union perf_event *event;
   struct perf_evsel *evsel;
   struct perf_evlist *evlist;
   struct target target = {
       .uid = UINT_MAX,
       .uses_mmap = true,
   };
   const char *argv[] = {""true"", NULL};
   char sbuf[STRERR_BUFSIZE];
   struct cpu_map *cpus;
   struct thread_map *threads;
   signal(SIGCHLD, sig_handler);
   evlist = perf_evlist__new_default();
   if (evlist == NULL) {
     pr_debug(""perf_evlist__new_default\n"");
     return -1;
   }
   
   cpus = cpu_map__dummy_new();
   threads = thread_map__new_by_tid(-1);
   if (!cpus || !threads) {
     err = -ENOMEM;
     pr_debug(""Not enough memory to create thread/cpu maps\n"");
     goto out_free_maps;
   }
   perf_evlist__set_maps(evlist, cpus, threads);
   cpus = NULL;
   threads = NULL;
   err = perf_evlist__prepare_workload(evlist, &target, argv, false, workload_exec_failed_signal);
   if (err < 0) {
     pr_debug(""Couldn't run the workload!\n"");
     goto out_delete_evlist;
   }
   evsel = perf_evlist__first(evlist);
   evsel->attr.task = 1;
 #ifdef __s390x__
   evsel->attr.sample_freq = 1000000;
 #else
   evsel->attr.sample_freq = 1;
 #endif
   evsel->attr.inherit = 0;
   evsel->attr.watermark = 0;
   evsel->attr.wakeup_events = 1;
   evsel->attr.exclude_kernel = 1;
   err = perf_evlist__open(evlist);
   if (err < 0) {
     pr_debug(""Couldn't open the evlist: %s\n"", str_error_r(-err, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
-  if (perf_evlist__mmap(evlist, 128, false) < 0) {
+  if (perf_evlist__mmap(evlist, 128) < 0) {
     pr_debug(""failed to mmap events: %d (%s)\n"", errno, str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
   perf_evlist__start_workload(evlist);
 retry:
   while ((event = perf_evlist__mmap_read(evlist, 0)) != NULL) {
     if (event->header.type == PERF_RECORD_EXIT) nr_exit++;
     perf_evlist__mmap_consume(evlist, 0);
   }
   if (!exited || !nr_exit) {
     perf_evlist__poll(evlist, -1);
     goto retry;
   }
   if (nr_exit != 1) {
     pr_debug(""received %d EXIT records\n"", nr_exit);
     err = -1;
   }
 out_free_maps:
   cpu_map__put(cpus);
   thread_map__put(threads);
 out_delete_evlist:
   perf_evlist__delete(evlist);
   return err;
 }",77,475
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_builtin-top_perf_top__start_counters,26,26," static int perf_top__start_counters(struct perf_top *top) {
   char msg[BUFSIZ];
   struct perf_evsel *counter;
   struct perf_evlist *evlist = top->evlist;
   struct record_opts *opts = &top->record_opts;
   perf_evlist__config(evlist, opts, &callchain_param);
   evlist__for_each_entry(evlist, counter) {
   try_again:
     if (perf_evsel__open(counter, top->evlist->cpus, top->evlist->threads) < 0) {
       if (perf_evsel__fallback(counter, errno, msg, sizeof(msg))) {
         if (verbose > 0) ui__warning(""%s\n"", msg);
         goto try_again;
       }
       perf_evsel__open_strerror(counter, &opts->target, errno, msg, sizeof(msg));
       ui__error(""%s\n"", msg);
       goto out_err;
     }
   }
-  if (perf_evlist__mmap(evlist, opts->mmap_pages, false) < 0) {
+  if (perf_evlist__mmap(evlist, opts->mmap_pages) < 0) {
     ui__error(""Failed to mmap with %d (%s)\n"", errno, str_error_r(errno, msg, sizeof(msg)));
     goto out_err;
   }
   return 0;
 out_err:
   return -1;
 }",27,221
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_builtin-trace_trace__run,169,169," static int trace__run(struct trace *trace, int argc, const char **argv) {
   struct perf_evlist *evlist = trace->evlist;
   struct perf_evsel *evsel, *pgfault_maj = NULL, *pgfault_min = NULL;
   int err = -1, i;
   unsigned long before;
   const bool forks = argc > 0;
   bool draining = false;
   trace->live = true;
   if (trace->trace_syscalls && trace__add_syscall_newtp(trace)) goto out_error_raw_syscalls;
   if (trace->trace_syscalls) trace->vfs_getname = perf_evlist__add_vfs_getname(evlist);
   if ((trace->trace_pgfaults & TRACE_PFMAJ)) {
     pgfault_maj = perf_evsel__new_pgfault(PERF_COUNT_SW_PAGE_FAULTS_MAJ);
     if (pgfault_maj == NULL) goto out_error_mem;
     perf_evlist__add(evlist, pgfault_maj);
   }
   if ((trace->trace_pgfaults & TRACE_PFMIN)) {
     pgfault_min = perf_evsel__new_pgfault(PERF_COUNT_SW_PAGE_FAULTS_MIN);
     if (pgfault_min == NULL) goto out_error_mem;
     perf_evlist__add(evlist, pgfault_min);
   }
   if (trace->sched && perf_evlist__add_newtp(evlist, ""sched"", ""sched_stat_runtime"", trace__sched_stat_runtime)) goto out_error_sched_stat_runtime;
   err = perf_evlist__create_maps(evlist, &trace->opts.target);
   if (err < 0) {
     fprintf(trace->output, ""Problems parsing the target to trace, check your options!\n"");
     goto out_delete_evlist;
   }
   err = trace__symbols_init(trace, evlist);
   if (err < 0) {
     fprintf(trace->output, ""Problems initializing symbol libraries!\n"");
     goto out_delete_evlist;
   }
   perf_evlist__config(evlist, &trace->opts, NULL);
   if (callchain_param.enabled) {
     bool use_identifier = false;
     if (trace->syscalls.events.sys_exit) {
       perf_evsel__config_callchain(trace->syscalls.events.sys_exit, &trace->opts, &callchain_param);
       use_identifier = true;
     }
     if (pgfault_maj) {
       perf_evsel__config_callchain(pgfault_maj, &trace->opts, &callchain_param);
       use_identifier = true;
     }
     if (pgfault_min) {
       perf_evsel__config_callchain(pgfault_min, &trace->opts, &callchain_param);
       use_identifier = true;
     }
     if (use_identifier) {
       
       perf_evlist__set_sample_bit(evlist, IDENTIFIER);
       perf_evlist__reset_sample_bit(evlist, ID);
     }
   }
   signal(SIGCHLD, sig_handler);
   signal(SIGINT, sig_handler);
   if (forks) {
     err = perf_evlist__prepare_workload(evlist, &trace->opts.target, argv, false, NULL);
     if (err < 0) {
       fprintf(trace->output, ""Couldn't run the workload!\n"");
       goto out_delete_evlist;
     }
   }
   err = perf_evlist__open(evlist);
   if (err < 0) goto out_error_open;
   err = bpf__apply_obj_config();
   if (err) {
     char errbuf[BUFSIZ];
     bpf__strerror_apply_obj_config(err, errbuf, sizeof(errbuf));
     pr_err(""ERROR: Apply config to BPF failed: %s\n"", errbuf);
     goto out_error_open;
   }
   
   if (trace->filter_pids.nr > 0)
     err = perf_evlist__set_filter_pids(evlist, trace->filter_pids.nr, trace->filter_pids.entries);
   else if (thread_map__pid(evlist->threads, 0) == -1)
     err = trace__set_filter_loop_pids(trace);
   if (err < 0) goto out_error_mem;
   if (trace->ev_qualifier_ids.nr > 0) {
     err = trace__set_ev_qualifier_filter(trace);
     if (err < 0) goto out_errno;
     pr_debug(""event qualifier tracepoint filter: %s\n"", trace->syscalls.events.sys_exit->filter);
   }
   err = perf_evlist__apply_filters(evlist, &evsel);
   if (err < 0) goto out_error_apply_filters;
-  err = perf_evlist__mmap(evlist, trace->opts.mmap_pages, false);
+  err = perf_evlist__mmap(evlist, trace->opts.mmap_pages);
   if (err < 0) goto out_error_mmap;
   if (!target__none(&trace->opts.target) && !trace->opts.initial_delay) perf_evlist__enable(evlist);
   if (forks) perf_evlist__start_workload(evlist);
   if (trace->opts.initial_delay) {
     usleep(trace->opts.initial_delay * 1000);
     perf_evlist__enable(evlist);
   }
   trace->multiple_threads = thread_map__pid(evlist->threads, 0) == -1 || evlist->threads->nr > 1 || perf_evlist__first(evlist)->attr.inherit;
 again:
   before = trace->nr_events;
   for (i = 0; i < evlist->nr_mmaps; i++) {
     union perf_event *event;
     while ((event = perf_evlist__mmap_read(evlist, i)) != NULL) {
       struct perf_sample sample;
       ++trace->nr_events;
       err = perf_evlist__parse_sample(evlist, event, &sample);
       if (err) {
         fprintf(trace->output, ""Can't parse sample, err = %d, skipping...\n"", err);
         goto next_event;
       }
       trace__handle_event(trace, event, &sample);
     next_event:
       perf_evlist__mmap_consume(evlist, i);
       if (interrupted) goto out_disable;
       if (done && !draining) {
         perf_evlist__disable(evlist);
         draining = true;
       }
     }
   }
   if (trace->nr_events == before) {
     int timeout = done ? 100 : -1;
     if (!draining && perf_evlist__poll(evlist, timeout) > 0) {
       if (perf_evlist__filter_pollfd(evlist, POLLERR | POLLHUP) == 0) draining = true;
       goto again;
     }
   } else {
     goto again;
   }
 out_disable:
   thread__zput(trace->current);
   perf_evlist__disable(evlist);
   if (!err) {
     if (trace->summary) trace__fprintf_thread_summary(trace, trace->output);
     if (trace->show_tool_stats) {
       fprintf(trace->output,
               ""Stats:\n ""
               "" vfs_getname : %"" PRIu64 ""\n""
               "" proc_getname: %"" PRIu64 ""\n"",
               trace->stats.vfs_getname, trace->stats.proc_getname);
     }
   }
 out_delete_evlist:
   trace__symbols__exit(trace);
   perf_evlist__delete(evlist);
   trace->evlist = NULL;
   trace->live = false;
   return err;
   {
     char errbuf[BUFSIZ];
   out_error_sched_stat_runtime:
     tracing_path__strerror_open_tp(errno, errbuf, sizeof(errbuf), ""sched"", ""sched_stat_runtime"");
     goto out_error;
   out_error_raw_syscalls:
     tracing_path__strerror_open_tp(errno, errbuf, sizeof(errbuf), ""raw_syscalls"", ""sys_(enter|exit)"");
     goto out_error;
   out_error_mmap:
     perf_evlist__strerror_mmap(evlist, errno, errbuf, sizeof(errbuf));
     goto out_error;
   out_error_open:
     perf_evlist__strerror_open(evlist, errno, errbuf, sizeof(errbuf));
   out_error:
     fprintf(trace->output, ""%s\n"", errbuf);
     goto out_delete_evlist;
   out_error_apply_filters:
     fprintf(trace->output, ""Failed to set filter \""%s\"" on event %s with %d (%s)\n"", evsel->filter, perf_evsel__name(evsel), errno, str_error_r(errno, errbuf, sizeof(errbuf)));
     goto out_delete_evlist;
   }
 out_error_mem:
   fprintf(trace->output, ""Not enough memory to run!\n"");
   goto out_delete_evlist;
 out_errno:
   fprintf(trace->output, ""errno=%d,%s\n"", errno, strerror(errno));
   goto out_delete_evlist;
 }",170,1272
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_openat-syscall-tp-fields_test__syscall_openat_tp_fields,87,87," int test__syscall_openat_tp_fields(struct test *test __maybe_unused, int subtest __maybe_unused) {
   struct record_opts opts = {
       .target =
           {
               .uid = UINT_MAX,
               .uses_mmap = true,
           },
       .no_buffering = true,
       .freq = 1,
       .mmap_pages = 256,
       .raw_samples = true,
   };
   const char *filename = ""/etc/passwd"";
   int flags = O_RDONLY | O_DIRECTORY;
   struct perf_evlist *evlist = perf_evlist__new();
   struct perf_evsel *evsel;
   int err = -1, i, nr_events = 0, nr_polls = 0;
   char sbuf[STRERR_BUFSIZE];
   if (evlist == NULL) {
     pr_debug(""%s: perf_evlist__new\n"", __func__);
     goto out;
   }
   evsel = perf_evsel__newtp(""syscalls"", ""sys_enter_openat"");
   if (IS_ERR(evsel)) {
     pr_debug(""%s: perf_evsel__newtp\n"", __func__);
     goto out_delete_evlist;
   }
   perf_evlist__add(evlist, evsel);
   err = perf_evlist__create_maps(evlist, &opts.target);
   if (err < 0) {
     pr_debug(""%s: perf_evlist__create_maps\n"", __func__);
     goto out_delete_evlist;
   }
   perf_evsel__config(evsel, &opts, NULL);
   thread_map__set_pid(evlist->threads, 0, getpid());
   err = perf_evlist__open(evlist);
   if (err < 0) {
     pr_debug(""perf_evlist__open: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
-  err = perf_evlist__mmap(evlist, UINT_MAX, false);
+  err = perf_evlist__mmap(evlist, UINT_MAX);
   if (err < 0) {
     pr_debug(""perf_evlist__mmap: %s\n"", str_error_r(errno, sbuf, sizeof(sbuf)));
     goto out_delete_evlist;
   }
   perf_evlist__enable(evlist);
   
   openat(AT_FDCWD, filename, flags);
   while (1) {
     int before = nr_events;
     for (i = 0; i < evlist->nr_mmaps; i++) {
       union perf_event *event;
       while ((event = perf_evlist__mmap_read(evlist, i)) != NULL) {
         const u32 type = event->header.type;
         int tp_flags;
         struct perf_sample sample;
         ++nr_events;
         if (type != PERF_RECORD_SAMPLE) {
           perf_evlist__mmap_consume(evlist, i);
           continue;
         }
         err = perf_evsel__parse_sample(evsel, event, &sample);
         if (err) {
           pr_debug(""Can't parse sample, err = %d\n"", err);
           goto out_delete_evlist;
         }
         tp_flags = perf_evsel__intval(evsel, &sample, ""flags"");
         if (flags != tp_flags) {
           pr_debug(""%s: Expected flags=%#x, got %#x\n"", __func__, flags, tp_flags);
           goto out_delete_evlist;
         }
         goto out_ok;
       }
     }
     if (nr_events == before) perf_evlist__poll(evlist, 10);
     if (++nr_polls > 5) {
       pr_debug(""%s: no events!\n"", __func__);
       goto out_delete_evlist;
     }
   }
 out_ok:
   err = 0;
 out_delete_evlist:
   perf_evlist__delete(evlist);
 out:
   return err;
 }",88,546
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_python_pyrf_evlist__mmap,12,12," static PyObject *pyrf_evlist__mmap(struct pyrf_evlist *pevlist, PyObject *args, PyObject *kwargs) {
   struct perf_evlist *evlist = &pevlist->evlist;
   static char *kwlist[] = {""pages"", ""overwrite"", NULL};
   int pages = 128, overwrite = false;
   if (!PyArg_ParseTupleAndKeywords(args, kwargs, ""|ii"", kwlist, &pages, &overwrite)) return NULL;
-  if (perf_evlist__mmap(evlist, pages, overwrite) < 0) {
+  if (perf_evlist__mmap(evlist, pages) < 0) {
     PyErr_SetFromErrno(PyExc_OSError);
     return NULL;
   }
   Py_INCREF(Py_None);
   return Py_None;
 }",13,129
perf_evlist__mmap-69,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/perf_evlist__mmap-69/1512266437_2017-12-03_f74b9d3a1ac2_switch-tracking_test__switch_tracking,217,217," int test__switch_tracking(struct test *test __maybe_unused, int subtest __maybe_unused) {
   const char *sched_switch = ""sched:sched_switch"";
   struct switch_tracking switch_tracking = {
       .tids = NULL,
   };
   struct record_opts opts = {
       .mmap_pages = UINT_MAX,
       .user_freq = UINT_MAX,
       .user_interval = ULLONG_MAX,
       .freq = 4000,
       .target =
           {
               .uses_mmap = true,
           },
   };
   struct thread_map *threads = NULL;
   struct cpu_map *cpus = NULL;
   struct perf_evlist *evlist = NULL;
   struct perf_evsel *evsel, *cpu_clocks_evsel, *cycles_evsel;
   struct perf_evsel *switch_evsel, *tracking_evsel;
   const char *comm;
   int err = -1;
   threads = thread_map__new(-1, getpid(), UINT_MAX);
   if (!threads) {
     pr_debug(""thread_map__new failed!\n"");
     goto out_err;
   }
   cpus = cpu_map__new(NULL);
   if (!cpus) {
     pr_debug(""cpu_map__new failed!\n"");
     goto out_err;
   }
   evlist = perf_evlist__new();
   if (!evlist) {
     pr_debug(""perf_evlist__new failed!\n"");
     goto out_err;
   }
   perf_evlist__set_maps(evlist, cpus, threads);
   
   err = parse_events(evlist, ""cpu-clock:u"", NULL);
   if (err) {
     pr_debug(""Failed to parse event dummy:u\n"");
     goto out_err;
   }
   cpu_clocks_evsel = perf_evlist__last(evlist);
   
   err = parse_events(evlist, ""cycles:u"", NULL);
   if (err) {
     pr_debug(""Failed to parse event cycles:u\n"");
     goto out_err;
   }
   cycles_evsel = perf_evlist__last(evlist);
   
   if (!perf_evlist__can_select_event(evlist, sched_switch)) {
     pr_debug(""No sched_switch\n"");
     err = 0;
     goto out;
   }
   err = parse_events(evlist, sched_switch, NULL);
   if (err) {
     pr_debug(""Failed to parse event %s\n"", sched_switch);
     goto out_err;
   }
   switch_evsel = perf_evlist__last(evlist);
   perf_evsel__set_sample_bit(switch_evsel, CPU);
   perf_evsel__set_sample_bit(switch_evsel, TIME);
   switch_evsel->system_wide = true;
   switch_evsel->no_aux_samples = true;
   switch_evsel->immediate = true;
   
   if (cycles_evsel == perf_evlist__first(evlist)) {
     pr_debug(""cycles event already at front"");
     goto out_err;
   }
   perf_evlist__to_front(evlist, cycles_evsel);
   if (cycles_evsel != perf_evlist__first(evlist)) {
     pr_debug(""Failed to move cycles event to front"");
     goto out_err;
   }
   perf_evsel__set_sample_bit(cycles_evsel, CPU);
   perf_evsel__set_sample_bit(cycles_evsel, TIME);
   
   err = parse_events(evlist, ""dummy:u"", NULL);
   if (err) {
     pr_debug(""Failed to parse event dummy:u\n"");
     goto out_err;
   }
   tracking_evsel = perf_evlist__last(evlist);
   perf_evlist__set_tracking_event(evlist, tracking_evsel);
   tracking_evsel->attr.freq = 0;
   tracking_evsel->attr.sample_period = 1;
   perf_evsel__set_sample_bit(tracking_evsel, TIME);
   
   perf_evlist__config(evlist, &opts, NULL);
   
   if (cycles_evsel != perf_evlist__first(evlist)) {
     pr_debug(""Front event no longer at front"");
     goto out_err;
   }
   
   if (!tracking_evsel->attr.mmap || !tracking_evsel->attr.comm) {
     pr_debug(""Tracking event not tracking\n"");
     goto out_err;
   }
   
   evlist__for_each_entry(evlist, evsel) {
     if (evsel != tracking_evsel) {
       if (evsel->attr.mmap || evsel->attr.comm) {
         pr_debug(""Non-tracking event is tracking\n"");
         goto out_err;
       }
     }
   }
   if (perf_evlist__open(evlist) < 0) {
     pr_debug(""Not supported\n"");
     err = 0;
     goto out;
   }
-  err = perf_evlist__mmap(evlist, UINT_MAX, false);
+  err = perf_evlist__mmap(evlist, UINT_MAX);
   if (err) {
     pr_debug(""perf_evlist__mmap failed!\n"");
     goto out_err;
   }
   perf_evlist__enable(evlist);
   err = perf_evsel__disable(cpu_clocks_evsel);
   if (err) {
     pr_debug(""perf_evlist__disable_event failed!\n"");
     goto out_err;
   }
   err = spin_sleep();
   if (err) {
     pr_debug(""spin_sleep failed!\n"");
     goto out_err;
   }
   comm = ""Test COMM 1"";
   err = prctl(PR_SET_NAME, (unsigned long)comm, 0, 0, 0);
   if (err) {
     pr_debug(""PR_SET_NAME failed!\n"");
     goto out_err;
   }
   err = perf_evsel__disable(cycles_evsel);
   if (err) {
     pr_debug(""perf_evlist__disable_event failed!\n"");
     goto out_err;
   }
   comm = ""Test COMM 2"";
   err = prctl(PR_SET_NAME, (unsigned long)comm, 0, 0, 0);
   if (err) {
     pr_debug(""PR_SET_NAME failed!\n"");
     goto out_err;
   }
   err = spin_sleep();
   if (err) {
     pr_debug(""spin_sleep failed!\n"");
     goto out_err;
   }
   comm = ""Test COMM 3"";
   err = prctl(PR_SET_NAME, (unsigned long)comm, 0, 0, 0);
   if (err) {
     pr_debug(""PR_SET_NAME failed!\n"");
     goto out_err;
   }
   err = perf_evsel__enable(cycles_evsel);
   if (err) {
     pr_debug(""perf_evlist__disable_event failed!\n"");
     goto out_err;
   }
   comm = ""Test COMM 4"";
   err = prctl(PR_SET_NAME, (unsigned long)comm, 0, 0, 0);
   if (err) {
     pr_debug(""PR_SET_NAME failed!\n"");
     goto out_err;
   }
   err = spin_sleep();
   if (err) {
     pr_debug(""spin_sleep failed!\n"");
     goto out_err;
   }
   perf_evlist__disable(evlist);
   switch_tracking.switch_evsel = switch_evsel;
   switch_tracking.cycles_evsel = cycles_evsel;
   err = process_events(evlist, &switch_tracking);
   zfree(&switch_tracking.tids);
   if (err) goto out_err;
   
   if (!switch_tracking.comm_seen[0] || !switch_tracking.comm_seen[1] || !switch_tracking.comm_seen[2] || !switch_tracking.comm_seen[3]) {
     pr_debug(""Missing comm events\n"");
     goto out_err;
   }
   
   if (!switch_tracking.cycles_before_comm_1) {
     pr_debug(""Missing cycles events\n"");
     goto out_err;
   }
   
   if (switch_tracking.cycles_between_comm_2_and_comm_3) {
     pr_debug(""cycles events even though event was disabled\n"");
     goto out_err;
   }
   
   if (!switch_tracking.cycles_after_comm_4) {
     pr_debug(""Missing cycles events\n"");
     goto out_err;
   }
 out:
   if (evlist) {
     perf_evlist__disable(evlist);
     perf_evlist__delete(evlist);
   } else {
     cpu_map__put(cpus);
     thread_map__put(threads);
   }
   return err;
 out_err:
   err = -1;
   goto out;
 }",218,1187
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/early_memunmap/1424769208_2015-02-24_8d4a40bc0651_setup_relocate_initrd,35,35," static void __init relocate_initrd(void) {
   
   u64 ramdisk_image = get_ramdisk_image();
   u64 ramdisk_size = get_ramdisk_size();
   u64 area_size = PAGE_ALIGN(ramdisk_size);
   unsigned long slop, clen, mapaddr;
   char *p, *q;
   
   relocated_ramdisk = memblock_find_in_range(0, PFN_PHYS(max_pfn_mapped), area_size, PAGE_SIZE);
   if (!relocated_ramdisk) panic(""Cannot find place for new RAMDISK of size %lld\n"", ramdisk_size);
   
   memblock_reserve(relocated_ramdisk, area_size);
   initrd_start = relocated_ramdisk + PAGE_OFFSET;
   initrd_end = initrd_start + ramdisk_size;
   printk(KERN_INFO ""Allocated new RAMDISK: [mem %#010llx-%#010llx]\n"", relocated_ramdisk, relocated_ramdisk + ramdisk_size - 1);
   q = (char *)initrd_start;
   
   while (ramdisk_size) {
     slop = ramdisk_image & ~PAGE_MASK;
     clen = ramdisk_size;
     if (clen > MAX_MAP_CHUNK - slop) clen = MAX_MAP_CHUNK - slop;
     mapaddr = ramdisk_image & PAGE_MASK;
     p = early_memremap(mapaddr, clen + slop);
     memcpy(q, p + slop, clen);
-    early_iounmap(p, clen + slop);
+    early_memunmap(p, clen + slop);
     q += clen;
     ramdisk_image += clen;
     ramdisk_size -= clen;
   }
   ramdisk_image = get_ramdisk_image();
   ramdisk_size = get_ramdisk_size();
   printk(KERN_INFO ""Move RAMDISK from [mem %#010llx-%#010llx] to""
                    "" [mem %#010llx-%#010llx]\n"",
          ramdisk_image, ramdisk_image + ramdisk_size - 1, relocated_ramdisk, relocated_ramdisk + ramdisk_size - 1);
 }",36,252
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/early_memunmap/1424769208_2015-02-24_8d4a40bc0651_e820_parse_e820_ext,14,14," void __init parse_e820_ext(u64 phys_addr, u32 data_len)
 {
 	int entries;
 	struct e820entry *extmap;
 	struct setup_data *sdata;
 	sdata = early_memremap(phys_addr, data_len);
 	entries = sdata->len / sizeof(struct e820entry);
 	extmap = (struct e820entry *)(sdata->data);
 	__append_e820_map(extmap, entries);
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
-	early_iounmap(sdata, data_len);
+	early_memunmap(sdata, data_len);
 	printk(KERN_INFO ""e820: extended physical RAM map:\n"");
 	e820_print_map(""extended"");
 }",15,117
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/early_memunmap/1424769208_2015-02-24_8d4a40b_setup_relocate_initrd,42,42," static void __init relocate_initrd(void)
 {
 	
 	u64 ramdisk_image = get_ramdisk_image();
 	u64 ramdisk_size  = get_ramdisk_size();
 	u64 area_size     = PAGE_ALIGN(ramdisk_size);
 	unsigned long slop, clen, mapaddr;
 	char *p, *q;
 	
 	relocated_ramdisk = memblock_find_in_range(0, PFN_PHYS(max_pfn_mapped),
 						   area_size, PAGE_SIZE);
 	if (!relocated_ramdisk)
 		panic(""Cannot find place for new RAMDISK of size %lld\n"",
 		      ramdisk_size);
 	
 	memblock_reserve(relocated_ramdisk, area_size);
 	initrd_start = relocated_ramdisk + PAGE_OFFSET;
 	initrd_end   = initrd_start + ramdisk_size;
 	printk(KERN_INFO ""Allocated new RAMDISK: [mem %#010llx-%#010llx]\n"",
 	       relocated_ramdisk, relocated_ramdisk + ramdisk_size - 1);
 	q = (char *)initrd_start;
 	
 	while (ramdisk_size) {
 		slop = ramdisk_image & ~PAGE_MASK;
 		clen = ramdisk_size;
 		if (clen > MAX_MAP_CHUNK-slop)
 			clen = MAX_MAP_CHUNK-slop;
 		mapaddr = ramdisk_image & PAGE_MASK;
 		p = early_memremap(mapaddr, clen+slop);
 		memcpy(q, p+slop, clen);
-		early_iounmap(p, clen+slop);
+		early_memunmap(p, clen + slop);
 		q += clen;
 		ramdisk_image += clen;
 		ramdisk_size  -= clen;
 	}
 	ramdisk_image = get_ramdisk_image();
 	ramdisk_size  = get_ramdisk_size();
 	printk(KERN_INFO ""Move RAMDISK from [mem %#010llx-%#010llx] to""
 		"" [mem %#010llx-%#010llx]\n"",
 		ramdisk_image, ramdisk_image + ramdisk_size - 1,
 		relocated_ramdisk, relocated_ramdisk + ramdisk_size - 1);
 }",43,252
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/early_memunmap/1424769208_2015-02-24_8d4a40b_setup_memblock_x86_reserve_range_setup_data,11,11," static void __init memblock_x86_reserve_range_setup_data(void) {
   struct setup_data *data;
   u64 pa_data;
   pa_data = boot_params.hdr.setup_data;
   while (pa_data) {
     data = early_memremap(pa_data, sizeof(*data));
     memblock_reserve(pa_data, sizeof(*data) + data->len);
     pa_data = data->next;
-    early_iounmap(data, sizeof(*data));
+    early_memunmap(data, sizeof(*data));
   }
 }",12,89
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/early_memunmap/1424769208_2015-02-24_8d4a40bc0651_setup_memblock_x86_reserve_range_setup_data,11,11," static void __init memblock_x86_reserve_range_setup_data(void) {
   struct setup_data *data;
   u64 pa_data;
   pa_data = boot_params.hdr.setup_data;
   while (pa_data) {
     data = early_memremap(pa_data, sizeof(*data));
     memblock_reserve(pa_data, sizeof(*data) + data->len);
     pa_data = data->next;
-    early_iounmap(data, sizeof(*data));
+    early_memunmap(data, sizeof(*data));
   }
 }",12,89
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/early_memunmap/1424769208_2015-02-24_8d4a40bc0651_setup_e820_reserve_setup_data,18,18," static void __init e820_reserve_setup_data(void) {
   struct setup_data *data;
   u64 pa_data;
   int found = 0;
   pa_data = boot_params.hdr.setup_data;
   while (pa_data) {
     data = early_memremap(pa_data, sizeof(*data));
     e820_update_range(pa_data, sizeof(*data) + data->len, E820_RAM, E820_RESERVED_KERN);
     found = 1;
     pa_data = data->next;
-    early_iounmap(data, sizeof(*data));
+    early_memunmap(data, sizeof(*data));
   }
   if (!found) return;
   sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
   memcpy(&e820_saved, &e820, sizeof(struct e820map));
   printk(KERN_INFO ""extended physical RAM map:\n"");
   e820_print_map(""reserve setup_data"");
 }",19,159
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/early_memunmap/1424769208_2015-02-24_8d4a40b_e820_parse_e820_ext,13,13," void __init parse_e820_ext(u64 phys_addr, u32 data_len) {
   int entries;
   struct e820entry *extmap;
   struct setup_data *sdata;
   sdata = early_memremap(phys_addr, data_len);
   entries = sdata->len / sizeof(struct e820entry);
   extmap = (struct e820entry *)(sdata->data);
   __append_e820_map(extmap, entries);
   sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
-  early_iounmap(sdata, data_len);
+  early_memunmap(sdata, data_len);
   printk(KERN_INFO ""e820: extended physical RAM map:\n"");
   e820_print_map(""extended"");
 }",14,117
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/early_memunmap/1424769208_2015-02-24_8d4a40b_setup_e820_reserve_setup_data,18,18," static void __init e820_reserve_setup_data(void) {
   struct setup_data *data;
   u64 pa_data;
   int found = 0;
   pa_data = boot_params.hdr.setup_data;
   while (pa_data) {
     data = early_memremap(pa_data, sizeof(*data));
     e820_update_range(pa_data, sizeof(*data) + data->len, E820_RAM, E820_RESERVED_KERN);
     found = 1;
     pa_data = data->next;
-    early_iounmap(data, sizeof(*data));
+    early_memunmap(data, sizeof(*data));
   }
   if (!found) return;
   sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
   memcpy(&e820_saved, &e820, sizeof(struct e820map));
   printk(KERN_INFO ""extended physical RAM map:\n"");
   e820_print_map(""reserve setup_data"");
 }",19,159
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/early_memunmap/1402317686_2014-06-09_98a716b66cab_quirks_efi_reuse_config,31,31," int __init efi_reuse_config(u64 tables, int nr_tables) {
   int i, sz, ret = 0;
   void *p, *tablep;
   struct efi_setup_data *data;
   if (!efi_setup) return 0;
   if (!efi_enabled(EFI_64BIT)) return 0;
   data = early_memremap(efi_setup, sizeof(*data));
   if (!data) {
     ret = -ENOMEM;
     goto out;
   }
   if (!data->smbios) goto out_memremap;
   sz = sizeof(efi_config_table_64_t);
   p = tablep = early_memremap(tables, nr_tables * sz);
   if (!p) {
     pr_err(""Could not map Configuration table!\n"");
     ret = -ENOMEM;
     goto out_memremap;
   }
   for (i = 0; i < efi.systab->nr_tables; i++) {
     efi_guid_t guid;
     guid = ((efi_config_table_64_t *)p)->guid;
     if (!efi_guidcmp(guid, SMBIOS_TABLE_GUID)) ((efi_config_table_64_t *)p)->table = data->smbios;
     p += sz;
   }
-  early_iounmap(tablep, nr_tables * sz);
+  early_memunmap(tablep, nr_tables * sz);
 out_memremap:
-  early_iounmap(data, sizeof(*data));
+  early_memunmap(data, sizeof(*data));
 out:
   return ret;
 }",33,246
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/early_memunmap/1424769208_2015-02-24_8d4a40b_setup_parse_setup_data,21,21," static void __init parse_setup_data(void) {
   struct setup_data *data;
   u64 pa_data, pa_next;
   pa_data = boot_params.hdr.setup_data;
   while (pa_data) {
     u32 data_len, data_type;
     data = early_memremap(pa_data, sizeof(*data));
     data_len = data->len + sizeof(struct setup_data);
     data_type = data->type;
     pa_next = data->next;
-    early_iounmap(data, sizeof(*data));
+    early_memunmap(data, sizeof(*data));
     switch (data_type) {
     case SETUP_E820_EXT: parse_e820_ext(pa_data, data_len); break;
     case SETUP_DTB: add_dtb(pa_data); break;
     case SETUP_EFI: parse_efi_setup(pa_data, data_len); break;
     case SETUP_KASLR: parse_kaslr_setup(pa_data, data_len); break;
     default: break;
     }
     pa_data = pa_next;
   }
 }",22,159
early_memunmap,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/early_memunmap/1402317686_2014-06-09_98a716b_quirks_efi_reuse_config,31,31," int __init efi_reuse_config(u64 tables, int nr_tables) {
   int i, sz, ret = 0;
   void *p, *tablep;
   struct efi_setup_data *data;
   if (!efi_setup) return 0;
   if (!efi_enabled(EFI_64BIT)) return 0;
   data = early_memremap(efi_setup, sizeof(*data));
   if (!data) {
     ret = -ENOMEM;
     goto out;
   }
   if (!data->smbios) goto out_memremap;
   sz = sizeof(efi_config_table_64_t);
   p = tablep = early_memremap(tables, nr_tables * sz);
   if (!p) {
     pr_err(""Could not map Configuration table!\n"");
     ret = -ENOMEM;
     goto out_memremap;
   }
   for (i = 0; i < efi.systab->nr_tables; i++) {
     efi_guid_t guid;
     guid = ((efi_config_table_64_t *)p)->guid;
     if (!efi_guidcmp(guid, SMBIOS_TABLE_GUID)) ((efi_config_table_64_t *)p)->table = data->smbios;
     p += sz;
   }
-  early_iounmap(tablep, nr_tables * sz);
+  early_memunmap(tablep, nr_tables * sz);
 out_memremap:
-  early_iounmap(data, sizeof(*data));
+  early_memunmap(data, sizeof(*data));
 out:
   return ret;
 }",33,246
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_isdn_common_isdn_init,62,61," static int __init isdn_init(void)
 {
 	int i;
 	char tmprev[50];
 	dev = vzalloc(sizeof(isdn_dev));
 	if (!dev) {
 		printk(KERN_WARNING ""isdn: Could not allocate device-struct.\n"");
 		return -EIO;
 	}
-	init_timer(&dev->timer);
-	dev->timer.function = isdn_timer_funct;
+	setup_timer(&dev->timer, isdn_timer_funct, 0UL);
 	spin_lock_init(&dev->lock);
 	spin_lock_init(&dev->timerlock);
 #ifdef MODULE
 	dev->owner = THIS_MODULE;
 #endif
 	mutex_init(&dev->mtx);
 	init_waitqueue_head(&dev->info_waitq);
 	for (i = 0; i < ISDN_MAX_CHANNELS; i++) {
 		dev->drvmap[i] = -1;
 		dev->chanmap[i] = -1;
 		dev->m_idx[i] = -1;
 		strcpy(dev->num[i], ""???"");
 	}
 	if (register_chrdev(ISDN_MAJOR, ""isdn"", &isdn_fops)) {
 		printk(KERN_WARNING ""isdn: Could not register control devices\n"");
 		vfree(dev);
 		return -EIO;
 	}
 	if ((isdn_tty_modem_init()) < 0) {
 		printk(KERN_WARNING ""isdn: Could not register tty devices\n"");
 		vfree(dev);
 		unregister_chrdev(ISDN_MAJOR, ""isdn"");
 		return -EIO;
 	}
 #ifdef CONFIG_ISDN_PPP
 	if (isdn_ppp_init() < 0) {
 		printk(KERN_WARNING ""isdn: Could not create PPP-device-structs\n"");
 		isdn_tty_exit();
 		unregister_chrdev(ISDN_MAJOR, ""isdn"");
 		vfree(dev);
 		return -EIO;
 	}
 #endif                          
 	strcpy(tmprev, isdn_revision);
 	printk(KERN_NOTICE ""ISDN subsystem Rev: %s/"", isdn_getrev(tmprev));
 	strcpy(tmprev, isdn_net_revision);
 	printk(""%s/"", isdn_getrev(tmprev));
 	strcpy(tmprev, isdn_ppp_revision);
 	printk(""%s/"", isdn_getrev(tmprev));
 	strcpy(tmprev, isdn_audio_revision);
 	printk(""%s/"", isdn_getrev(tmprev));
 	strcpy(tmprev, isdn_v110_revision);
 	printk(""%s"", isdn_getrev(tmprev));
 #ifdef MODULE
 	printk("" loaded\n"");
 #else
 	printk(""\n"");
 #endif
 	isdn_info_update();
 	return 0;
 }",63,415
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_arcmsr_hba_arcmsr_probe,113,112," static int arcmsr_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 {
 	struct Scsi_Host *host;
 	struct AdapterControlBlock *acb;
 	uint8_t bus,dev_fun;
 	int error;
 	error = pci_enable_device(pdev);
 	if(error){
 		return -ENODEV;
 	}
 	host = scsi_host_alloc(&arcmsr_scsi_host_template, sizeof(struct AdapterControlBlock));
 	if(!host){
     		goto pci_disable_dev;
 	}
 	error = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));
 	if(error){
 		error = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));
 		if(error){
 			printk(KERN_WARNING
 			       ""scsi%d: No suitable DMA mask available\n"",
 			       host->host_no);
 			goto scsi_host_release;
 		}
 	}
 	init_waitqueue_head(&wait_q);
 	bus = pdev->bus->number;
 	dev_fun = pdev->devfn;
 	acb = (struct AdapterControlBlock *) host->hostdata;
 	memset(acb,0,sizeof(struct AdapterControlBlock));
 	acb->pdev = pdev;
 	acb->host = host;
 	host->max_lun = ARCMSR_MAX_TARGETLUN;
 	host->max_id = ARCMSR_MAX_TARGETID;		
 	host->max_cmd_len = 16;	 			
 	host->can_queue = ARCMSR_MAX_OUTSTANDING_CMD;
 	host->cmd_per_lun = ARCMSR_MAX_CMD_PERLUN;	    
 	host->this_id = ARCMSR_SCSI_INITIATOR_ID;
 	host->unique_id = (bus << 8) | dev_fun;
 	pci_set_drvdata(pdev, host);
 	pci_set_master(pdev);
 	error = pci_request_regions(pdev, ""arcmsr"");
 	if(error){
 		goto scsi_host_release;
 	}
 	spin_lock_init(&acb->eh_lock);
 	spin_lock_init(&acb->ccblist_lock);
 	spin_lock_init(&acb->postq_lock);
 	spin_lock_init(&acb->doneq_lock);
 	spin_lock_init(&acb->rqbuffer_lock);
 	spin_lock_init(&acb->wqbuffer_lock);
 	acb->acb_flags |= (ACB_F_MESSAGE_WQBUFFER_CLEARED |
 			ACB_F_MESSAGE_RQBUFFER_CLEARED |
 			ACB_F_MESSAGE_WQBUFFER_READED);
 	acb->acb_flags &= ~ACB_F_SCSISTOPADAPTER;
 	INIT_LIST_HEAD(&acb->ccb_free_list);
 	acb->adapter_type = id->driver_data;
 	error = arcmsr_remap_pciregion(acb);
 	if(!error){
 		goto pci_release_regs;
 	}
 	error = arcmsr_alloc_io_queue(acb);
 	if (!error)
 		goto unmap_pci_region;
 	error = arcmsr_get_firmware_spec(acb);
 	if(!error){
 		goto free_hbb_mu;
 	}
 	error = arcmsr_alloc_ccb_pool(acb);
 	if(error){
 		goto free_hbb_mu;
 	}
 	error = scsi_add_host(host, &pdev->dev);
 	if(error){
 		goto free_ccb_pool;
 	}
 	if (arcmsr_request_irq(pdev, acb) == FAILED)
 		goto scsi_host_remove;
 	arcmsr_iop_init(acb);
 	INIT_WORK(&acb->arcmsr_do_message_isr_bh, arcmsr_message_isr_bh_fn);
 	atomic_set(&acb->rq_map_token, 16);
 	atomic_set(&acb->ante_token_value, 16);
 	acb->fw_flag = FW_NORMAL;
-	init_timer(&acb->eternal_timer);
+	setup_timer(&acb->eternal_timer, &arcmsr_request_device_map,
+		    (unsigned long)acb);
 	acb->eternal_timer.expires = jiffies + msecs_to_jiffies(6 * HZ);
-	acb->eternal_timer.data = (unsigned long) acb;
-	acb->eternal_timer.function = &arcmsr_request_device_map;
 	add_timer(&acb->eternal_timer);
 	if(arcmsr_alloc_sysfs_attr(acb))
 		goto out_free_sysfs;
 	scsi_scan_host(host);
 	return 0;
 out_free_sysfs:
 	del_timer_sync(&acb->eternal_timer);
 	flush_work(&acb->arcmsr_do_message_isr_bh);
 	arcmsr_stop_adapter_bgrb(acb);
 	arcmsr_flush_adapter_cache(acb);
 	arcmsr_free_irq(pdev, acb);
 scsi_host_remove:
 	scsi_remove_host(host);
 free_ccb_pool:
 	arcmsr_free_ccb_pool(acb);
 free_hbb_mu:
 	arcmsr_free_mu(acb);
 unmap_pci_region:
 	arcmsr_unmap_pciregion(acb);
 pci_release_regs:
 	pci_release_regions(pdev);
 scsi_host_release:
 	scsi_host_put(host);
 pci_disable_dev:
 	pci_disable_device(pdev);
 	return -ENODEV;
 }",115,668
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_nmi_init_nmi_wdt,12,11," static int __init init_nmi_wdt(void)
 {
 	nmi_wdt_set_timeout(timeout);
 	nmi_wdt_start();
 	nmi_active = true;
-	init_timer(&ntimer);
-	ntimer.function = nmi_wdt_timer;
+	setup_timer(&ntimer, nmi_wdt_timer, 0UL);
 	ntimer.expires = jiffies + NMI_CHECK_TIMEOUT;
 	add_timer(&ntimer);
 	pr_info(""nmi_wdt: initialized: timeout=%d sec\n"", timeout);
 	return 0;
 }",13,74
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_isdn_net_isdn_net_ciscohdlck_connected,18,18," static void
 isdn_net_ciscohdlck_connected(isdn_net_local *lp)
 {
 	lp->cisco_myseq = 0;
 	lp->cisco_mineseen = 0;
 	lp->cisco_yourseq = 0;
 	lp->cisco_keepalive_period = ISDN_TIMER_KEEPINT;
 	lp->cisco_last_slarp_in = 0;
 	lp->cisco_line_state = 0;
 	lp->cisco_debserint = 0;
 	
 	isdn_net_ciscohdlck_slarp_send_request(lp);
-	init_timer(&lp->cisco_timer);
-	lp->cisco_timer.data = (unsigned long) lp;
-	lp->cisco_timer.function = isdn_net_ciscohdlck_slarp_send_keepalive;
+	setup_timer(&lp->cisco_timer,
+		    isdn_net_ciscohdlck_slarp_send_keepalive,
+		    (unsigned long)lp);
 	lp->cisco_timer.expires = jiffies + lp->cisco_keepalive_period * HZ;
 	add_timer(&lp->cisco_timer);
 }",21,129
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_dasd_dasd_alloc_device,40,39," struct dasd_device *dasd_alloc_device(void)
 {
 	struct dasd_device *device;
 	device = kzalloc(sizeof(struct dasd_device), GFP_ATOMIC);
 	if (!device)
 		return ERR_PTR(-ENOMEM);
 	
 	device->ccw_mem = (void *) __get_free_pages(GFP_ATOMIC | GFP_DMA, 1);
 	if (!device->ccw_mem) {
 		kfree(device);
 		return ERR_PTR(-ENOMEM);
 	}
 	
 	device->erp_mem = (void *) get_zeroed_page(GFP_ATOMIC | GFP_DMA);
 	if (!device->erp_mem) {
 		free_pages((unsigned long) device->ccw_mem, 1);
 		kfree(device);
 		return ERR_PTR(-ENOMEM);
 	}
 	dasd_init_chunklist(&device->ccw_chunks, device->ccw_mem, PAGE_SIZE*2);
 	dasd_init_chunklist(&device->erp_chunks, device->erp_mem, PAGE_SIZE);
 	spin_lock_init(&device->mem_lock);
 	atomic_set(&device->tasklet_scheduled, 0);
 	tasklet_init(&device->tasklet,
 		     (void (*)(unsigned long)) dasd_device_tasklet,
 		     (unsigned long) device);
 	INIT_LIST_HEAD(&device->ccw_queue);
-	init_timer(&device->timer);
-	device->timer.function = dasd_device_timeout;
-	device->timer.data = (unsigned long) device;
+	setup_timer(&device->timer, dasd_device_timeout,
+		    (unsigned long)device);
 	INIT_WORK(&device->kick_work, do_kick_device);
 	INIT_WORK(&device->restore_device, do_restore_device);
 	INIT_WORK(&device->reload_device, do_reload_device);
 	INIT_WORK(&device->requeue_requests, do_requeue_requests);
 	device->state = DASD_STATE_NEW;
 	device->target = DASD_STATE_NEW;
 	mutex_init(&device->state_mutex);
 	spin_lock_init(&device->profile.lock);
 	return device;
 }",42,330
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_push-switch_switch_drv_probe,42,40," static int switch_drv_probe(struct platform_device *pdev)
 {
 	struct push_switch_platform_info *psw_info;
 	struct push_switch *psw;
 	int ret, irq;
 	psw = kzalloc(sizeof(struct push_switch), GFP_KERNEL);
 	if (unlikely(!psw))
 		return -ENOMEM;
 	irq = platform_get_irq(pdev, 0);
 	if (unlikely(irq < 0)) {
 		ret = -ENODEV;
 		goto err;
 	}
 	psw_info = pdev->dev.platform_data;
 	BUG_ON(!psw_info);
 	ret = request_irq(irq, psw_info->irq_handler,
 			  psw_info->irq_flags,
 			  psw_info->name ? psw_info->name : DRV_NAME, pdev);
 	if (unlikely(ret < 0))
 		goto err;
 	if (psw_info->name) {
 		ret = device_create_file(&pdev->dev, &dev_attr_switch);
 		if (unlikely(ret)) {
 			dev_err(&pdev->dev, ""Failed creating device attrs\n"");
 			ret = -EINVAL;
 			goto err_irq;
 		}
 	}
 	INIT_WORK(&psw->work, switch_work_handler);
-	init_timer(&psw->debounce);
-	psw->debounce.function = switch_timer;
-	psw->debounce.data = (unsigned long)psw;
+	setup_timer(&psw->debounce, switch_timer, (unsigned long)psw);
 	
 	psw->pdev = pdev;
 	platform_set_drvdata(pdev, psw);
 	return 0;
 err_irq:
 	free_irq(irq, pdev);
 err:
 	kfree(psw);
 	return ret;
 }",43,276
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_n2100_n2100_request_gpios,23,22," static int __init n2100_request_gpios(void)
 {
 	int ret;
 	if (!machine_is_n2100())
 		return 0;
 	ret = gpio_request(N2100_HARDWARE_RESET, ""reset"");
 	if (ret)
 		pr_err(""could not request reset GPIO\n"");
 	ret = gpio_request(N2100_POWER_BUTTON, ""power"");
 	if (ret)
 		pr_err(""could not request power GPIO\n"");
 	else {
 		ret = gpio_direction_input(N2100_POWER_BUTTON);
 		if (ret)
 			pr_err(""could not set power GPIO as input\n"");
 	}
 	
-	init_timer(&power_button_poll_timer);
-	power_button_poll_timer.function = power_button_poll;
+	setup_timer(&power_button_poll_timer, power_button_poll, 0UL);
 	power_button_poll_timer.expires = jiffies + (HZ / 10);
 	add_timer(&power_button_poll_timer);
 	return 0;
 }",24,136
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_fas216_fas216_init,29,27," int fas216_init(struct Scsi_Host *host)
 {
 	FAS216_Info *info = (FAS216_Info *)host->hostdata;
 	info->magic_start    = MAGIC;
 	info->magic_end      = MAGIC;
 	info->host           = host;
 	info->scsi.cfg[0]    = host->this_id | CNTL1_PERE;
 	info->scsi.cfg[1]    = CNTL2_ENF | CNTL2_S2FE;
 	info->scsi.cfg[2]    = info->ifcfg.cntl3 |
 			       CNTL3_ADIDCHK | CNTL3_QTAG | CNTL3_G2CB | CNTL3_LBTM;
 	info->scsi.async_stp = fas216_syncperiod(info, info->ifcfg.asyncperiod);
 	info->rst_dev_status = -1;
 	info->rst_bus_status = -1;
 	init_waitqueue_head(&info->eh_wait);
-	init_timer(&info->eh_timer);
-	info->eh_timer.data  = (unsigned long)info;
-	info->eh_timer.function = fas216_eh_timer;
+	setup_timer(&info->eh_timer, fas216_eh_timer, (unsigned long)info);
 	
 	spin_lock_init(&info->host_lock);
 	memset(&info->stats, 0, sizeof(info->stats));
 	msgqueue_initialise(&info->scsi.msgs);
 	if (!queue_initialise(&info->queues.issue))
 		return -ENOMEM;
 	if (!queue_initialise(&info->queues.disconnected)) {
 		queue_free(&info->queues.issue);
 		return -ENOMEM;
 	}
 	return 0;
 }",30,260
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/kees_timer1/1508184939_2017-10-16_b9eaf1872222_bfad_bfad_init_timer,9,7," void
 bfad_init_timer(struct bfad_s *bfad)
 {
-	init_timer(&bfad->hal_tmo);
-	bfad->hal_tmo.function = bfad_bfa_tmo;
-	bfad->hal_tmo.data = (unsigned long)bfad;
+	setup_timer(&bfad->hal_tmo, bfad_bfa_tmo, (unsigned long)bfad);
 	mod_timer(&bfad->hal_tmo,
 		  jiffies + msecs_to_jiffies(BFA_TIMER_FREQ));
 }",10,73
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_dsi_dsi_bind,131,129," static int dsi_bind(struct device *dev, struct device *master, void *data)
 {
 	struct platform_device *dsidev = to_platform_device(dev);
 	const struct soc_device_attribute *soc;
 	const struct dsi_module_id_data *d;
 	u32 rev;
 	int r, i;
 	struct dsi_data *dsi;
 	struct resource *dsi_mem;
 	struct resource *res;
 	dsi = devm_kzalloc(&dsidev->dev, sizeof(*dsi), GFP_KERNEL);
 	if (!dsi)
 		return -ENOMEM;
 	dsi->pdev = dsidev;
 	dev_set_drvdata(&dsidev->dev, dsi);
 	spin_lock_init(&dsi->irq_lock);
 	spin_lock_init(&dsi->errors_lock);
 	dsi->errors = 0;
 #ifdef CONFIG_OMAP2_DSS_COLLECT_IRQ_STATS
 	spin_lock_init(&dsi->irq_stats_lock);
 	dsi->irq_stats.last_reset = jiffies;
 #endif
 	mutex_init(&dsi->lock);
 	sema_init(&dsi->bus_lock, 1);
 	INIT_DEFERRABLE_WORK(&dsi->framedone_timeout_work,
 			     dsi_framedone_timeout_work_callback);
 #ifdef DSI_CATCH_MISSING_TE
-	init_timer(&dsi->te_timer);
-	dsi->te_timer.function = dsi_te_timeout;
-	dsi->te_timer.data = 0;
+	setup_timer(&dsi->te_timer, dsi_te_timeout, 0);
 #endif
 	dsi_mem = platform_get_resource_byname(dsidev, IORESOURCE_MEM, ""proto"");
 	dsi->proto_base = devm_ioremap_resource(&dsidev->dev, dsi_mem);
 	if (IS_ERR(dsi->proto_base))
 		return PTR_ERR(dsi->proto_base);
 	res = platform_get_resource_byname(dsidev, IORESOURCE_MEM, ""phy"");
 	dsi->phy_base = devm_ioremap_resource(&dsidev->dev, res);
 	if (IS_ERR(dsi->phy_base))
 		return PTR_ERR(dsi->phy_base);
 	res = platform_get_resource_byname(dsidev, IORESOURCE_MEM, ""pll"");
 	dsi->pll_base = devm_ioremap_resource(&dsidev->dev, res);
 	if (IS_ERR(dsi->pll_base))
 		return PTR_ERR(dsi->pll_base);
 	dsi->irq = platform_get_irq(dsi->pdev, 0);
 	if (dsi->irq < 0) {
 		DSSERR(""platform_get_irq failed\n"");
 		return -ENODEV;
 	}
 	r = devm_request_irq(&dsidev->dev, dsi->irq, omap_dsi_irq_handler,
 			     IRQF_SHARED, dev_name(&dsidev->dev), dsi->pdev);
 	if (r < 0) {
 		DSSERR(""request_irq failed\n"");
 		return r;
 	}
 	soc = soc_device_match(dsi_soc_devices);
 	if (soc)
 		dsi->data = soc->data;
 	else
 		dsi->data = of_match_node(dsi_of_match, dev->of_node)->data;
 	d = dsi->data->modules;
 	while (d->address != 0 && d->address != dsi_mem->start)
 		d++;
 	if (d->address == 0) {
 		DSSERR(""unsupported DSI module\n"");
 		return -ENODEV;
 	}
 	dsi->module_id = d->id;
 	if (dsi->data->model == DSI_MODEL_OMAP4 ||
 	    dsi->data->model == DSI_MODEL_OMAP5) {
 		struct device_node *np;
 		
 		np = of_find_node_by_name(NULL,
 			dsi->data->model == DSI_MODEL_OMAP4 ?
 			""omap4_padconf_global"" : ""omap5_padconf_global"");
 		if (!np)
 			return -ENODEV;
 		dsi->syscon = syscon_node_to_regmap(np);
 		of_node_put(np);
 	}
 	
 	for (i = 0; i < ARRAY_SIZE(dsi->vc); i++) {
 		dsi->vc[i].source = DSI_VC_SOURCE_L4;
 		dsi->vc[i].dssdev = NULL;
 		dsi->vc[i].vc_id = 0;
 	}
 	r = dsi_get_clocks(dsidev);
 	if (r)
 		return r;
 	dsi_init_pll_data(dsidev);
 	pm_runtime_enable(&dsidev->dev);
 	r = dsi_runtime_get(dsidev);
 	if (r)
 		goto err_runtime_get;
 	rev = dsi_read_reg(dsidev, DSI_REVISION);
 	dev_dbg(&dsidev->dev, ""OMAP DSI rev %d.%d\n"",
 	       FLD_GET(rev, 7, 4), FLD_GET(rev, 3, 0));
 	
 	if (dsi->data->quirks & DSI_QUIRK_GNQ)
 		
 		dsi->num_lanes_supported = 1 + REG_GET(dsidev, DSI_GNQ, 11, 9);
 	else
 		dsi->num_lanes_supported = 3;
 	dsi->line_buffer_size = dsi_get_line_buf_size(dsidev);
 	dsi_init_output(dsidev);
 	r = dsi_probe_of(dsidev);
 	if (r) {
 		DSSERR(""Invalid DSI DT data\n"");
 		goto err_probe_of;
 	}
 	r = of_platform_populate(dsidev->dev.of_node, NULL, NULL, &dsidev->dev);
 	if (r)
 		DSSERR(""Failed to populate DSI child devices: %d\n"", r);
 	dsi_runtime_put(dsidev);
 	if (dsi->module_id == 0)
 		dss_debugfs_create_file(""dsi1_regs"", dsi1_dump_regs);
 	else if (dsi->module_id == 1)
 		dss_debugfs_create_file(""dsi2_regs"", dsi2_dump_regs);
 #ifdef CONFIG_OMAP2_DSS_COLLECT_IRQ_STATS
 	if (dsi->module_id == 0)
 		dss_debugfs_create_file(""dsi1_irqs"", dsi1_dump_irqs);
 	else if (dsi->module_id == 1)
 		dss_debugfs_create_file(""dsi2_irqs"", dsi2_dump_irqs);
 #endif
 	return 0;
 err_probe_of:
 	dsi_uninit_output(dsidev);
 	dsi_runtime_put(dsidev);
 err_runtime_get:
 	pm_runtime_disable(&dsidev->dev);
 	return r;
 }",132,945
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_esas2r_main_esas2r_kickoff_timer,9,7," void esas2r_kickoff_timer(struct esas2r_adapter *a)
 {
-	init_timer(&a->timer);
-	a->timer.function = esas2r_timer_callback;
-	a->timer.data = (unsigned long)a;
+	setup_timer(&a->timer, esas2r_timer_callback, (unsigned long)a);
 	a->timer.expires = jiffies +
 			   msecs_to_jiffies(100);
 	add_timer(&a->timer);
 }",10,79
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_st-nci_se_st_nci_se_init,20,17," int st_nci_se_init(struct nci_dev *ndev, struct st_nci_se_status *se_status)
 {
 	struct st_nci_info *info = nci_get_drvdata(ndev);
 	init_completion(&info->se_info.req_completion);
 	
-	init_timer(&info->se_info.bwi_timer);
-	info->se_info.bwi_timer.data = (unsigned long)info;
-	info->se_info.bwi_timer.function = st_nci_se_wt_timeout;
+	setup_timer(&info->se_info.bwi_timer, st_nci_se_wt_timeout,
+		    (unsigned long)info);
 	info->se_info.bwi_active = false;
-	init_timer(&info->se_info.se_active_timer);
-	info->se_info.se_active_timer.data = (unsigned long)info;
-	info->se_info.se_active_timer.function =
-			st_nci_se_activation_timeout;
+	setup_timer(&info->se_info.se_active_timer,
+		    st_nci_se_activation_timeout, (unsigned long)info);
 	info->se_info.se_active = false;
 	info->se_info.xch_error = false;
 	info->se_info.wt_timeout =
 		ST_NCI_BWI_TO_TIMEOUT(ST_NCI_ATR_DEFAULT_BWI);
 	info->se_info.se_status = se_status;
 	return 0;
 }",24,196
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_umem_init_battery_timer,7,6," static void init_battery_timer(void)
 {
-	init_timer(&battery_timer);
-	battery_timer.function = check_all_batteries;
+	setup_timer(&battery_timer, check_all_batteries, 0UL);
 	battery_timer.expires = jiffies + (HZ * 60);
 	add_timer(&battery_timer);
 }",8,51
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_se_st21nfca_se_init,19,17," void st21nfca_se_init(struct nfc_hci_dev *hdev)
 {
 	struct st21nfca_hci_info *info = nfc_hci_get_clientdata(hdev);
 	init_completion(&info->se_info.req_completion);
 	
-	init_timer(&info->se_info.bwi_timer);
-	info->se_info.bwi_timer.data = (unsigned long)info;
-	info->se_info.bwi_timer.function = st21nfca_se_wt_timeout;
+	setup_timer(&info->se_info.bwi_timer, st21nfca_se_wt_timeout,
+		    (unsigned long)info);
 	info->se_info.bwi_active = false;
-	init_timer(&info->se_info.se_active_timer);
-	info->se_info.se_active_timer.data = (unsigned long)info;
-	info->se_info.se_active_timer.function = st21nfca_se_activation_timeout;
+	setup_timer(&info->se_info.se_active_timer,
+		    st21nfca_se_activation_timeout, (unsigned long)info);
 	info->se_info.se_active = false;
 	info->se_info.count_pipes = 0;
 	info->se_info.expected_pipes = 0;
 	info->se_info.xch_error = false;
 	info->se_info.wt_timeout =
 			ST21NFCA_BWI_TO_TIMEOUT(ST21NFCA_ATR_DEFAULT_BWI);
 }",23,195
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_ncr53c8xx_ncr_attach,175,173," struct Scsi_Host * __init ncr_attach(struct scsi_host_template *tpnt,
 					int unit, struct ncr_device *device)
 {
 	struct host_data *host_data;
 	struct ncb *np = NULL;
 	struct Scsi_Host *instance = NULL;
 	u_long flags = 0;
 	int i;
 	if (!tpnt->name)
 		tpnt->name	= SCSI_NCR_DRIVER_NAME;
 	if (!tpnt->shost_attrs)
 		tpnt->shost_attrs = ncr53c8xx_host_attrs;
 	tpnt->queuecommand	= ncr53c8xx_queue_command;
 	tpnt->slave_configure	= ncr53c8xx_slave_configure;
 	tpnt->slave_alloc	= ncr53c8xx_slave_alloc;
 	tpnt->eh_bus_reset_handler = ncr53c8xx_bus_reset;
 	tpnt->can_queue		= SCSI_NCR_CAN_QUEUE;
 	tpnt->this_id		= 7;
 	tpnt->sg_tablesize	= SCSI_NCR_SG_TABLESIZE;
 	tpnt->cmd_per_lun	= SCSI_NCR_CMD_PER_LUN;
 	tpnt->use_clustering	= ENABLE_CLUSTERING;
 	if (device->differential)
 		driver_setup.diff_support = device->differential;
 	printk(KERN_INFO ""ncr53c720-%d: rev 0x%x irq %d\n"",
 		unit, device->chip.revision_id, device->slot.irq);
 	instance = scsi_host_alloc(tpnt, sizeof(*host_data));
 	if (!instance)
 	        goto attach_error;
 	host_data = (struct host_data *) instance->hostdata;
 	np = __m_calloc_dma(device->dev, sizeof(struct ncb), ""NCB"");
 	if (!np)
 		goto attach_error;
 	spin_lock_init(&np->smp_lock);
 	np->dev = device->dev;
 	np->p_ncb = vtobus(np);
 	host_data->ncb = np;
 	np->ccb = m_calloc_dma(sizeof(struct ccb), ""CCB"");
 	if (!np->ccb)
 		goto attach_error;
 	
 	np->unit	= unit;
 	np->verbose	= driver_setup.verbose;
 	sprintf(np->inst_name, ""ncr53c720-%d"", np->unit);
 	np->revision_id	= device->chip.revision_id;
 	np->features	= device->chip.features;
 	np->clock_divn	= device->chip.nr_divisor;
 	np->maxoffs	= device->chip.offset_max;
 	np->maxburst	= device->chip.burst_max;
 	np->myaddr	= device->host_id;
 	
 	np->script0 = m_calloc_dma(sizeof(struct script), ""SCRIPT"");
 	if (!np->script0)
 		goto attach_error;
 	np->scripth0 = m_calloc_dma(sizeof(struct scripth), ""SCRIPTH"");
 	if (!np->scripth0)
 		goto attach_error;
-	init_timer(&np->timer);
-	np->timer.data     = (unsigned long) np;
-	np->timer.function = ncr53c8xx_timeout;
+	setup_timer(&np->timer, ncr53c8xx_timeout, (unsigned long)np);
 	
 	np->paddr	= device->slot.base;
 	np->paddr2	= (np->features & FE_RAM) ? device->slot.base_2 : 0;
 	if (device->slot.base_v)
 		np->vaddr = device->slot.base_v;
 	else
 		np->vaddr = ioremap(device->slot.base_c, 128);
 	if (!np->vaddr) {
 		printk(KERN_ERR
 			""%s: can't map memory mapped IO region\n"",ncr_name(np));
 		goto attach_error;
 	} else {
 		if (bootverbose > 1)
 			printk(KERN_INFO
 				""%s: using memory mapped IO at virtual address 0x%lx\n"", ncr_name(np), (u_long) np->vaddr);
 	}
 	
 	np->reg = (struct ncr_reg __iomem *)np->vaddr;
 	
 	ncr_prepare_setting(np);
 	if (np->paddr2 && sizeof(struct script) > 4096) {
 		np->paddr2 = 0;
 		printk(KERN_WARNING ""%s: script too large, NOT using on chip RAM.\n"",
 			ncr_name(np));
 	}
 	instance->max_channel	= 0;
 	instance->this_id       = np->myaddr;
 	instance->max_id	= np->maxwide ? 16 : 8;
 	instance->max_lun	= SCSI_NCR_MAX_LUN;
 	instance->base		= (unsigned long) np->reg;
 	instance->irq		= device->slot.irq;
 	instance->unique_id	= device->slot.base;
 	instance->dma_channel	= 0;
 	instance->cmd_per_lun	= MAX_TAGS;
 	instance->can_queue	= (MAX_START-4);
 	
 	BUG_ON(!ncr53c8xx_transport_template);
 	instance->transportt	= ncr53c8xx_transport_template;
 	
 	ncr_script_fill(&script0, &scripth0);
 	np->scripth	= np->scripth0;
 	np->p_scripth	= vtobus(np->scripth);
 	np->p_script	= (np->paddr2) ?  np->paddr2 : vtobus(np->script0);
 	ncr_script_copy_and_bind(np, (ncrcmd *) &script0,
 			(ncrcmd *) np->script0, sizeof(struct script));
 	ncr_script_copy_and_bind(np, (ncrcmd *) &scripth0,
 			(ncrcmd *) np->scripth0, sizeof(struct scripth));
 	np->ccb->p_ccb	= vtobus (np->ccb);
 	
 	if (np->features & FE_LED0) {
 		np->script0->idle[0]  =
 				cpu_to_scr(SCR_REG_REG(gpreg, SCR_OR,  0x01));
 		np->script0->reselected[0] =
 				cpu_to_scr(SCR_REG_REG(gpreg, SCR_AND, 0xfe));
 		np->script0->start[0] =
 				cpu_to_scr(SCR_REG_REG(gpreg, SCR_AND, 0xfe));
 	}
 	
 	for (i = 0 ; i < 4 ; i++) {
 		np->jump_tcb[i].l_cmd   =
 				cpu_to_scr((SCR_JUMP ^ IFTRUE (MASK (i, 3))));
 		np->jump_tcb[i].l_paddr =
 				cpu_to_scr(NCB_SCRIPTH_PHYS (np, bad_target));
 	}
 	ncr_chip_reset(np, 100);
 	
 	if (ncr_snooptest(np)) {
 		printk(KERN_ERR ""CACHE INCORRECTLY CONFIGURED.\n"");
 		goto attach_error;
 	}
 	
 	np->irq = device->slot.irq;
 	
 	ncr_init_ccb(np, np->ccb);
 	
 	spin_lock_irqsave(&np->smp_lock, flags);
 	if (ncr_reset_scsi_bus(np, 0, driver_setup.settle_delay) != 0) {
 		printk(KERN_ERR ""%s: FATAL ERROR: CHECK SCSI BUS - CABLES, TERMINATION, DEVICE POWER etc.!\n"", ncr_name(np));
 		spin_unlock_irqrestore(&np->smp_lock, flags);
 		goto attach_error;
 	}
 	ncr_exception(np);
 	np->disc = 1;
 	
 	if (driver_setup.settle_delay > 2) {
 		printk(KERN_INFO ""%s: waiting %d seconds for scsi devices to settle...\n"",
 			ncr_name(np), driver_setup.settle_delay);
 		mdelay(1000 * driver_setup.settle_delay);
 	}
 	
 	np->lasttime=0;
 	ncr_timeout (np);
 	
 #ifdef SCSI_NCR_ALWAYS_SIMPLE_TAG
 	np->order = SIMPLE_QUEUE_TAG;
 #endif
 	spin_unlock_irqrestore(&np->smp_lock, flags);
 	return instance;
  attach_error:
 	if (!instance)
 		return NULL;
 	printk(KERN_INFO ""%s: detaching...\n"", ncr_name(np));
 	if (!np)
 		goto unregister;
 	if (np->scripth0)
 		m_free_dma(np->scripth0, sizeof(struct scripth), ""SCRIPTH"");
 	if (np->script0)
 		m_free_dma(np->script0, sizeof(struct script), ""SCRIPT"");
 	if (np->ccb)
 		m_free_dma(np->ccb, sizeof(struct ccb), ""CCB"");
 	m_free_dma(np, sizeof(struct ncb), ""NCB"");
 	host_data->ncb = NULL;
  unregister:
 	scsi_host_put(instance);
 	return NULL;
 }",176,1305
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_pn533_pn533_register_device,64,63," struct pn533 *pn533_register_device(u32 device_type,
 				u32 protocols,
 				enum pn533_protocol_type protocol_type,
 				void *phy,
 				struct pn533_phy_ops *phy_ops,
 				struct pn533_frame_ops *fops,
 				struct device *dev,
 				struct device *parent)
 {
 	struct pn533 *priv;
 	int rc = -ENOMEM;
 	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
 	if (!priv)
 		return ERR_PTR(-ENOMEM);
 	priv->phy = phy;
 	priv->phy_ops = phy_ops;
 	priv->dev = dev;
 	if (fops != NULL)
 		priv->ops = fops;
 	else
 		priv->ops = &pn533_std_frame_ops;
 	priv->protocol_type = protocol_type;
 	priv->device_type = device_type;
 	mutex_init(&priv->cmd_lock);
 	INIT_WORK(&priv->cmd_work, pn533_wq_cmd);
 	INIT_WORK(&priv->cmd_complete_work, pn533_wq_cmd_complete);
 	INIT_WORK(&priv->mi_rx_work, pn533_wq_mi_recv);
 	INIT_WORK(&priv->mi_tx_work, pn533_wq_mi_send);
 	INIT_WORK(&priv->tg_work, pn533_wq_tg_get_data);
 	INIT_WORK(&priv->mi_tm_rx_work, pn533_wq_tm_mi_recv);
 	INIT_WORK(&priv->mi_tm_tx_work, pn533_wq_tm_mi_send);
 	INIT_DELAYED_WORK(&priv->poll_work, pn533_wq_poll);
 	INIT_WORK(&priv->rf_work, pn533_wq_rf);
 	priv->wq = alloc_ordered_workqueue(""pn533"", 0);
 	if (priv->wq == NULL)
 		goto error;
-	init_timer(&priv->listen_timer);
-	priv->listen_timer.data = (unsigned long) priv;
-	priv->listen_timer.function = pn533_listen_mode_timer;
+	setup_timer(&priv->listen_timer, pn533_listen_mode_timer,
+		    (unsigned long)priv);
 	skb_queue_head_init(&priv->resp_q);
 	skb_queue_head_init(&priv->fragment_skb);
 	INIT_LIST_HEAD(&priv->cmd_queue);
 	priv->nfc_dev = nfc_allocate_device(&pn533_nfc_ops, protocols,
 					   priv->ops->tx_header_len +
 					   PN533_CMD_DATAEXCH_HEAD_LEN,
 					   priv->ops->tx_tail_len);
 	if (!priv->nfc_dev) {
 		rc = -ENOMEM;
 		goto destroy_wq;
 	}
 	nfc_set_parent_dev(priv->nfc_dev, parent);
 	nfc_set_drvdata(priv->nfc_dev, priv);
 	rc = nfc_register_device(priv->nfc_dev);
 	if (rc)
 		goto free_nfc_dev;
 	return priv;
 free_nfc_dev:
 	nfc_free_device(priv->nfc_dev);
 destroy_wq:
 	destroy_workqueue(priv->wq);
 error:
 	kfree(priv);
 	return ERR_PTR(rc);
 }",66,433
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_firestream_fs_init,193,191," static int fs_init(struct fs_dev *dev)
 {
 	struct pci_dev  *pci_dev;
 	int isr, to;
 	int i;
 	func_enter ();
 	pci_dev = dev->pci_dev;
 	printk (KERN_INFO ""found a FireStream %d card, base %16llx, irq%d.\n"",
 		IS_FS50(dev)?50:155,
 		(unsigned long long)pci_resource_start(pci_dev, 0),
 		dev->pci_dev->irq);
 	if (fs_debug & FS_DEBUG_INIT)
 		my_hd ((unsigned char *) dev, sizeof (*dev));
 	undocumented_pci_fix (pci_dev);
 	dev->hw_base = pci_resource_start(pci_dev, 0);
 	dev->base = ioremap(dev->hw_base, 0x1000);
 	reset_chip (dev);
   
 	write_fs (dev, SARMODE0, 0 
 		  | (0 * SARMODE0_SHADEN) 
 		  | (1 * SARMODE0_INTMODE_READCLEAR)
 		  | (1 * SARMODE0_CWRE)
 		  | (IS_FS50(dev) ? SARMODE0_PRPWT_FS50_5:
 			  SARMODE0_PRPWT_FS155_3)
 		  | (1 * SARMODE0_CALSUP_1)
 		  | (IS_FS50(dev) ? (0
 				   | SARMODE0_RXVCS_32
 				   | SARMODE0_ABRVCS_32 
 				   | SARMODE0_TXVCS_32):
 		                  (0
 				   | SARMODE0_RXVCS_1k
 				   | SARMODE0_ABRVCS_1k 
 				   | SARMODE0_TXVCS_1k)));
 	
 	to = 100;
 	while (--to) {
 		isr = read_fs (dev, ISR);
 		
 		if (isr & ISR_INIT_ERR) {
 			printk (KERN_ERR ""Error initializing the FS... \n"");
 			goto unmap;
 		}
 		if (isr & ISR_INIT) {
 			fs_dprintk (FS_DEBUG_INIT, ""Ha! Initialized OK!\n"");
 			break;
 		}
 		
 		msleep(10);
 	}
 	if (!to) {
 		printk (KERN_ERR ""timeout initializing the FS... \n"");
 		goto unmap;
 	}
 	
 	dev->channel_mask = 0x1f; 
 	dev->channo = 0;
 	
 	write_fs (dev, SARMODE1, 0 
 		  | (fs_keystream * SARMODE1_DEFHEC) 
 		  | ((loopback == 1) * SARMODE1_TSTLP) 
 		  | (1 * SARMODE1_DCRM)
 		  | (1 * SARMODE1_DCOAM)
 		  | (0 * SARMODE1_OAMCRC)
 		  | (0 * SARMODE1_DUMPE)
 		  | (0 * SARMODE1_GPLEN) 
 		  | (0 * SARMODE1_GNAM)
 		  | (0 * SARMODE1_GVAS)
 		  | (0 * SARMODE1_GPAS)
 		  | (1 * SARMODE1_GPRI)
 		  | (0 * SARMODE1_PMS)
 		  | (0 * SARMODE1_GFCR)
 		  | (1 * SARMODE1_HECM2)
 		  | (1 * SARMODE1_HECM1)
 		  | (1 * SARMODE1_HECM0)
 		  | (1 << 12) 
 		  | (0 * 0xff) );
 	
 	
 	write_fs (dev, TMCONF, 0x0000000f);
 	write_fs (dev, CALPRESCALE, 0x01010101 * num);
 	write_fs (dev, 0x80, 0x000F00E4);
 	
 	write_fs (dev, CELLOSCONF, 0
 		  | (   0 * CELLOSCONF_CEN)
 		  | (       CELLOSCONF_SC1)
 		  | (0x80 * CELLOSCONF_COBS)
 		  | (num  * CELLOSCONF_COPK)  
 		  | (num  * CELLOSCONF_COST));
 	
 	write_fs (dev, CELLOSCONF_COST, 0x0B809191);
 	if (IS_FS50 (dev)) {
 		write_fs (dev, RAS0, RAS0_DCD_XHLT);
 		dev->atm_dev->ci_range.vpi_bits = 12;
 		dev->atm_dev->ci_range.vci_bits = 16;
 		dev->nchannels = FS50_NR_CHANNELS;
 	} else {
 		write_fs (dev, RAS0, RAS0_DCD_XHLT 
 			  | (((1 << FS155_VPI_BITS) - 1) * RAS0_VPSEL)
 			  | (((1 << FS155_VCI_BITS) - 1) * RAS0_VCSEL));
 		
 		dev->atm_dev->ci_range.vpi_bits = FS155_VPI_BITS;
 		dev->atm_dev->ci_range.vci_bits = FS155_VCI_BITS;
     
 		
 		write_fs (dev, RAC, 0);
 		
 		write_fs (dev, RAM, (1 << (28 - FS155_VPI_BITS - FS155_VCI_BITS)) - 1);
 		dev->nchannels = FS155_NR_CHANNELS;
 	}
 	dev->atm_vccs = kcalloc (dev->nchannels, sizeof (struct atm_vcc *),
 				 GFP_KERNEL);
 	fs_dprintk (FS_DEBUG_ALLOC, ""Alloc atmvccs: %p(%zd)\n"",
 		    dev->atm_vccs, dev->nchannels * sizeof (struct atm_vcc *));
 	if (!dev->atm_vccs) {
 		printk (KERN_WARNING ""Couldn't allocate memory for VCC buffers. Woops!\n"");
 		
 		goto unmap;
 	}
 	dev->tx_inuse = kzalloc (dev->nchannels / 8  , GFP_KERNEL);
 	fs_dprintk (FS_DEBUG_ALLOC, ""Alloc tx_inuse: %p(%d)\n"", 
 		    dev->atm_vccs, dev->nchannels / 8);
 	if (!dev->tx_inuse) {
 		printk (KERN_WARNING ""Couldn't allocate memory for tx_inuse bits!\n"");
 		
 		goto unmap;
 	}
 	
 	
 	
 	write_fs (dev, DMAMR, DMAMR_TX_MODE_FULL);
 	init_q (dev, &dev->hp_txq, TX_PQ(TXQ_HP), TXQ_NENTRIES, 0);
 	init_q (dev, &dev->lp_txq, TX_PQ(TXQ_LP), TXQ_NENTRIES, 0);
 	init_q (dev, &dev->tx_relq, TXB_RQ, TXQ_NENTRIES, 1);
 	init_q (dev, &dev->st_q, ST_Q, TXQ_NENTRIES, 1);
 	for (i=0;i < FS_NR_FREE_POOLS;i++) {
 		init_fp (dev, &dev->rx_fp[i], RXB_FP(i), 
 			 rx_buf_sizes[i], rx_pool_sizes[i]);
 		top_off_fp (dev, &dev->rx_fp[i], GFP_KERNEL);
 	}
 	for (i=0;i < FS_NR_RX_QUEUES;i++)
 		init_q (dev, &dev->rx_rq[i], RXB_RQ(i), RXRQ_NENTRIES, 1);
 	dev->irq = pci_dev->irq;
 	if (request_irq (dev->irq, fs_irq, IRQF_SHARED, ""firestream"", dev)) {
 		printk (KERN_WARNING ""couldn't get irq %d for firestream.\n"", pci_dev->irq);
 		
 		goto unmap;
 	}
 	fs_dprintk (FS_DEBUG_INIT, ""Grabbed irq %d for dev at %p.\n"", dev->irq, dev);
   
 	
 	write_fs (dev, IMR, 0
 		  | ISR_RBRQ0_W 
 		  | ISR_RBRQ1_W 
 		  | ISR_RBRQ2_W 
 		  | ISR_RBRQ3_W 
 		  | ISR_TBRQ_W
 		  | ISR_CSQ_W);
 	write_fs (dev, SARMODE0, 0 
 		  | (0 * SARMODE0_SHADEN) 
 		  | (1 * SARMODE0_GINT)
 		  | (1 * SARMODE0_INTMODE_READCLEAR)
 		  | (0 * SARMODE0_CWRE)
 		  | (IS_FS50(dev)?SARMODE0_PRPWT_FS50_5: 
 		                  SARMODE0_PRPWT_FS155_3)
 		  | (1 * SARMODE0_CALSUP_1)
 		  | (IS_FS50 (dev)?(0
 				    | SARMODE0_RXVCS_32
 				    | SARMODE0_ABRVCS_32 
 				    | SARMODE0_TXVCS_32):
 		                   (0
 				    | SARMODE0_RXVCS_1k
 				    | SARMODE0_ABRVCS_1k 
 				    | SARMODE0_TXVCS_1k))
 		  | (1 * SARMODE0_RUN));
 	init_phy (dev, PHY_NTC_INIT);
 	if (loopback == 2) {
 		write_phy (dev, 0x39, 0x000e);
 	}
 #ifdef FS_POLL_FREQ
-	init_timer (&dev->timer);
-	dev->timer.data = (unsigned long) dev;
-	dev->timer.function = fs_poll;
+	setup_timer(&dev->timer, fs_poll, (unsigned long)dev);
 	dev->timer.expires = jiffies + FS_POLL_FREQ;
 	add_timer (&dev->timer);
 #endif
 	dev->atm_dev->dev_data = dev;
   
 	func_exit ();
 	return 0;
 unmap:
 	iounmap(dev->base);
 	return 1;
 }",194,1208
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_ndlc_ndlc_probe,26,22," int ndlc_probe(void *phy_id, struct nfc_phy_ops *phy_ops, struct device *dev,
 	       int phy_headroom, int phy_tailroom, struct llt_ndlc **ndlc_id,
 	       struct st_nci_se_status *se_status)
 {
 	struct llt_ndlc *ndlc;
 	ndlc = devm_kzalloc(dev, sizeof(struct llt_ndlc), GFP_KERNEL);
 	if (!ndlc)
 		return -ENOMEM;
 	ndlc->ops = phy_ops;
 	ndlc->phy_id = phy_id;
 	ndlc->dev = dev;
 	ndlc->powered = 0;
 	*ndlc_id = ndlc;
 	
-	init_timer(&ndlc->t1_timer);
-	ndlc->t1_timer.data = (unsigned long)ndlc;
-	ndlc->t1_timer.function = ndlc_t1_timeout;
-	init_timer(&ndlc->t2_timer);
-	ndlc->t2_timer.data = (unsigned long)ndlc;
-	ndlc->t2_timer.function = ndlc_t2_timeout;
+	setup_timer(&ndlc->t1_timer, ndlc_t1_timeout, (unsigned long)ndlc);
+	setup_timer(&ndlc->t2_timer, ndlc_t2_timeout, (unsigned long)ndlc);
 	skb_queue_head_init(&ndlc->rcv_q);
 	skb_queue_head_init(&ndlc->send_q);
 	skb_queue_head_init(&ndlc->ack_pending_q);
 	INIT_WORK(&ndlc->sm_work, llt_ndlc_sm_work);
 	return st_nci_probe(ndlc, phy_headroom, phy_tailroom, se_status);
 }",28,236
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_sym_glue_sym_attach,106,104," static struct Scsi_Host *sym_attach(struct scsi_host_template *tpnt, int unit,
 				    struct sym_device *dev)
 {
 	struct sym_data *sym_data;
 	struct sym_hcb *np = NULL;
 	struct Scsi_Host *shost = NULL;
 	struct pci_dev *pdev = dev->pdev;
 	unsigned long flags;
 	struct sym_fw *fw;
 	int do_free_irq = 0;
 	printk(KERN_INFO ""sym%d: <%s> rev 0x%x at pci %s irq %u\n"",
 		unit, dev->chip.name, pdev->revision, pci_name(pdev),
 		pdev->irq);
 	
 	fw = sym_find_firmware(&dev->chip);
 	if (!fw)
 		goto attach_failed;
 	shost = scsi_host_alloc(tpnt, sizeof(*sym_data));
 	if (!shost)
 		goto attach_failed;
 	sym_data = shost_priv(shost);
 	
 	np = __sym_calloc_dma(&pdev->dev, sizeof(*np), ""HCB"");
 	if (!np)
 		goto attach_failed;
 	np->bus_dmat = &pdev->dev; 
 	sym_data->ncb = np;
 	sym_data->pdev = pdev;
 	np->s.host = shost;
 	pci_set_drvdata(pdev, shost);
 	
 	np->hcb_ba	= vtobus(np);
 	np->verbose	= sym_driver_setup.verbose;
 	np->s.unit	= unit;
 	np->features	= dev->chip.features;
 	np->clock_divn	= dev->chip.nr_divisor;
 	np->maxoffs	= dev->chip.offset_max;
 	np->maxburst	= dev->chip.burst_max;
 	np->myaddr	= dev->host_id;
 	np->mmio_ba	= (u32)dev->mmio_base;
 	np->ram_ba	= (u32)dev->ram_base;
 	np->s.ioaddr	= dev->s.ioaddr;
 	np->s.ramaddr	= dev->s.ramaddr;
 	
 	strlcpy(np->s.chip_name, dev->chip.name, sizeof(np->s.chip_name));
 	sprintf(np->s.inst_name, ""sym%d"", np->s.unit);
 	if ((SYM_CONF_DMA_ADDRESSING_MODE > 0) && (np->features & FE_DAC) &&
 			!pci_set_dma_mask(pdev, DMA_DAC_MASK)) {
 		set_dac(np);
 	} else if (pci_set_dma_mask(pdev, DMA_BIT_MASK(32))) {
 		printf_warning(""%s: No suitable DMA available\n"", sym_name(np));
 		goto attach_failed;
 	}
 	if (sym_hcb_attach(shost, fw, dev->nvram))
 		goto attach_failed;
 	
 	if (request_irq(pdev->irq, sym53c8xx_intr, IRQF_SHARED, NAME53C8XX,
 			shost)) {
 		printf_err(""%s: request irq %u failure\n"",
 			sym_name(np), pdev->irq);
 		goto attach_failed;
 	}
 	do_free_irq = 1;
 	
 	spin_lock_irqsave(shost->host_lock, flags);
 	if (sym_reset_scsi_bus(np, 0))
 		goto reset_failed;
 	
 	sym_start_up(shost, 1);
 	
-	init_timer(&np->s.timer);
-	np->s.timer.data     = (unsigned long) np;
-	np->s.timer.function = sym53c8xx_timer;
+	setup_timer(&np->s.timer, sym53c8xx_timer, (unsigned long)np);
 	np->s.lasttime=0;
 	sym_timer (np);
 	
 	shost->max_channel	= 0;
 	shost->this_id		= np->myaddr;
 	shost->max_id		= np->maxwide ? 16 : 8;
 	shost->max_lun		= SYM_CONF_MAX_LUN;
 	shost->unique_id	= pci_resource_start(pdev, 0);
 	shost->cmd_per_lun	= SYM_CONF_MAX_TAG;
 	shost->can_queue	= (SYM_CONF_MAX_START-2);
 	shost->sg_tablesize	= SYM_CONF_MAX_SG;
 	shost->max_cmd_len	= 16;
 	BUG_ON(sym2_transport_template == NULL);
 	shost->transportt	= sym2_transport_template;
 	
 	if (pdev->device == PCI_DEVICE_ID_NCR_53C896 && pdev->revision < 2)
 		shost->dma_boundary = 0xFFFFFF;
 	spin_unlock_irqrestore(shost->host_lock, flags);
 	return shost;
  reset_failed:
 	printf_err(""%s: FATAL ERROR: CHECK SCSI BUS - CABLES, ""
 		   ""TERMINATION, DEVICE POWER etc.!\n"", sym_name(np));
 	spin_unlock_irqrestore(shost->host_lock, flags);
  attach_failed:
 	printf_info(""sym%d: giving up ...\n"", unit);
 	if (np)
 		sym_free_resources(np, pdev, do_free_irq);
 	else
 		sym_iounmap_device(dev);
 	if (shost)
 		scsi_host_put(shost);
 	return NULL;
  }",107,769
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_hostap_hw_prism2_init_local_data,145,143," static struct net_device *
 prism2_init_local_data(struct prism2_helper_functions *funcs, int card_idx,
 		       struct device *sdev)
 {
 	struct net_device *dev;
 	struct hostap_interface *iface;
 	struct local_info *local;
 	int len, i, ret;
 	if (funcs == NULL)
 		return NULL;
 	len = strlen(dev_template);
 	if (len >= IFNAMSIZ || strstr(dev_template, ""%d"") == NULL) {
 		printk(KERN_WARNING ""hostap: Invalid dev_template='%s'\n"",
 		       dev_template);
 		return NULL;
 	}
 	len = sizeof(struct hostap_interface) +
 		3 + sizeof(struct local_info) +
 		3 + sizeof(struct ap_data);
 	dev = alloc_etherdev(len);
 	if (dev == NULL)
 		return NULL;
 	iface = netdev_priv(dev);
 	local = (struct local_info *) ((((long) (iface + 1)) + 3) & ~3);
 	local->ap = (struct ap_data *) ((((long) (local + 1)) + 3) & ~3);
 	local->dev = iface->dev = dev;
 	iface->local = local;
 	iface->type = HOSTAP_INTERFACE_MASTER;
 	INIT_LIST_HEAD(&local->hostap_interfaces);
 	local->hw_module = THIS_MODULE;
 #ifdef PRISM2_IO_DEBUG
 	local->io_debug_enabled = 1;
 #endif 
 	local->func = funcs;
 	local->func->cmd = hfa384x_cmd;
 	local->func->read_regs = hfa384x_read_regs;
 	local->func->get_rid = hfa384x_get_rid;
 	local->func->set_rid = hfa384x_set_rid;
 	local->func->hw_enable = prism2_hw_enable;
 	local->func->hw_config = prism2_hw_config;
 	local->func->hw_reset = prism2_hw_reset;
 	local->func->hw_shutdown = prism2_hw_shutdown;
 	local->func->reset_port = prism2_reset_port;
 	local->func->schedule_reset = prism2_schedule_reset;
 #ifdef PRISM2_DOWNLOAD_SUPPORT
 	local->func->read_aux_fops = &prism2_download_aux_dump_proc_fops;
 	local->func->download = prism2_download;
 #endif 
 	local->func->tx = prism2_tx_80211;
 	local->func->set_tim = prism2_set_tim;
 	local->func->need_tx_headroom = 0; 
 	local->mtu = mtu;
 	rwlock_init(&local->iface_lock);
 	spin_lock_init(&local->txfidlock);
 	spin_lock_init(&local->cmdlock);
 	spin_lock_init(&local->baplock);
 	spin_lock_init(&local->lock);
 	spin_lock_init(&local->irq_init_lock);
 	mutex_init(&local->rid_bap_mtx);
 	if (card_idx < 0 || card_idx >= MAX_PARM_DEVICES)
 		card_idx = 0;
 	local->card_idx = card_idx;
 	len = strlen(essid);
 	memcpy(local->essid, essid,
 	       len > MAX_SSID_LEN ? MAX_SSID_LEN : len);
 	local->essid[MAX_SSID_LEN] = '\0';
 	i = GET_INT_PARM(iw_mode, card_idx);
 	if ((i >= IW_MODE_ADHOC && i <= IW_MODE_REPEAT) ||
 	    i == IW_MODE_MONITOR) {
 		local->iw_mode = i;
 	} else {
 		printk(KERN_WARNING ""prism2: Unknown iw_mode %d; using ""
 		       ""IW_MODE_MASTER\n"", i);
 		local->iw_mode = IW_MODE_MASTER;
 	}
 	local->channel = GET_INT_PARM(channel, card_idx);
 	local->beacon_int = GET_INT_PARM(beacon_int, card_idx);
 	local->dtim_period = GET_INT_PARM(dtim_period, card_idx);
 	local->wds_max_connections = 16;
 	local->tx_control = HFA384X_TX_CTRL_FLAGS;
 	local->manual_retry_count = -1;
 	local->rts_threshold = 2347;
 	local->fragm_threshold = 2346;
 	local->rssi_to_dBm = 100; 
 	local->auth_algs = PRISM2_AUTH_OPEN | PRISM2_AUTH_SHARED_KEY;
 	local->sram_type = -1;
 	local->scan_channel_mask = 0xffff;
 	local->monitor_type = PRISM2_MONITOR_RADIOTAP;
 	
 	INIT_WORK(&local->reset_queue, handle_reset_queue);
 	INIT_WORK(&local->set_multicast_list_queue,
 		  hostap_set_multicast_list_queue);
 	INIT_WORK(&local->set_tim_queue, handle_set_tim_queue);
 	INIT_LIST_HEAD(&local->set_tim_list);
 	spin_lock_init(&local->set_tim_lock);
 	INIT_WORK(&local->comms_qual_update, handle_comms_qual_update);
 	
 #define HOSTAP_TASKLET_INIT(q, f, d) \
 do { memset((q), 0, sizeof(*(q))); (q)->func = (f); (q)->data = (d); } \
 while (0)
 	HOSTAP_TASKLET_INIT(&local->bap_tasklet, hostap_bap_tasklet,
 			    (unsigned long) local);
 	HOSTAP_TASKLET_INIT(&local->info_tasklet, hostap_info_tasklet,
 			    (unsigned long) local);
 	hostap_info_init(local);
 	HOSTAP_TASKLET_INIT(&local->rx_tasklet,
 			    hostap_rx_tasklet, (unsigned long) local);
 	skb_queue_head_init(&local->rx_list);
 	HOSTAP_TASKLET_INIT(&local->sta_tx_exc_tasklet,
 			    hostap_sta_tx_exc_tasklet, (unsigned long) local);
 	skb_queue_head_init(&local->sta_tx_exc_list);
 	INIT_LIST_HEAD(&local->cmd_queue);
 	init_waitqueue_head(&local->hostscan_wq);
 	lib80211_crypt_info_init(&local->crypt_info, dev->name, &local->lock);
-	init_timer(&local->passive_scan_timer);
-	local->passive_scan_timer.data = (unsigned long) local;
-	local->passive_scan_timer.function = hostap_passive_scan;
-	init_timer(&local->tick_timer);
-	local->tick_timer.data = (unsigned long) local;
-	local->tick_timer.function = hostap_tick_timer;
+	setup_timer(&local->passive_scan_timer, hostap_passive_scan,
+		    (unsigned long)local);
+	setup_timer(&local->tick_timer, hostap_tick_timer,
+		    (unsigned long)local);
 	local->tick_timer.expires = jiffies + 2 * HZ;
 	add_timer(&local->tick_timer);
 	INIT_LIST_HEAD(&local->bss_list);
 	hostap_setup_dev(dev, local, HOSTAP_INTERFACE_MASTER);
 	dev->type = ARPHRD_IEEE80211;
 	dev->header_ops = &hostap_80211_ops;
 	rtnl_lock();
 	ret = dev_alloc_name(dev, ""wifi%d"");
 	SET_NETDEV_DEV(dev, sdev);
 	if (ret >= 0)
 		ret = register_netdevice(dev);
 	prism2_set_lockdep_class(dev);
 	rtnl_unlock();
 	if (ret < 0) {
 		printk(KERN_WARNING ""%s: register netdevice failed!\n"",
 		       dev_info);
 		goto fail;
 	}
 	printk(KERN_INFO ""%s: Registered netdevice %s\n"", dev_info, dev->name);
 	hostap_init_data(local);
 	return dev;
  fail:
 	free_netdev(dev);
 	return NULL;
 }",149,1069
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_dasd_dasd_alloc_block,20,18," struct dasd_block *dasd_alloc_block(void)
 {
 	struct dasd_block *block;
 	block = kzalloc(sizeof(*block), GFP_ATOMIC);
 	if (!block)
 		return ERR_PTR(-ENOMEM);
 	
 	atomic_set(&block->open_count, -1);
 	atomic_set(&block->tasklet_scheduled, 0);
 	tasklet_init(&block->tasklet,
 		     (void (*)(unsigned long)) dasd_block_tasklet,
 		     (unsigned long) block);
 	INIT_LIST_HEAD(&block->ccw_queue);
 	spin_lock_init(&block->queue_lock);
-	init_timer(&block->timer);
-	block->timer.function = dasd_block_timeout;
-	block->timer.data = (unsigned long) block;
+	setup_timer(&block->timer, dasd_block_timeout, (unsigned long)block);
 	spin_lock_init(&block->profile.lock);
 	return block;
 }",21,162
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_common_pcibios_enable_timers,13,11," void pcibios_enable_timers(struct pci_channel *hose)
 {
 	if (hose->err_irq) {
-		init_timer(&hose->err_timer);
-		hose->err_timer.data = (unsigned long)hose;
-		hose->err_timer.function = pcibios_enable_err;
+		setup_timer(&hose->err_timer, pcibios_enable_err,
+			    (unsigned long)hose);
 	}
 	if (hose->serr_irq) {
-		init_timer(&hose->serr_timer);
-		hose->serr_timer.data = (unsigned long)hose;
-		hose->serr_timer.function = pcibios_enable_serr;
+		setup_timer(&hose->serr_timer, pcibios_enable_serr,
+			    (unsigned long)hose);
 	}
 }",17,124
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_s5p_mfc_s5p_mfc_probe,125,124," static int s5p_mfc_probe(struct platform_device *pdev)
 {
 	struct s5p_mfc_dev *dev;
 	struct video_device *vfd;
 	struct resource *res;
 	int ret;
 	pr_debug(""%s++\n"", __func__);
 	dev = devm_kzalloc(&pdev->dev, sizeof(*dev), GFP_KERNEL);
 	if (!dev)
 		return -ENOMEM;
 	spin_lock_init(&dev->irqlock);
 	spin_lock_init(&dev->condlock);
 	dev->plat_dev = pdev;
 	if (!dev->plat_dev) {
 		dev_err(&pdev->dev, ""No platform data specified\n"");
 		return -ENODEV;
 	}
 	dev->variant = of_device_get_match_data(&pdev->dev);
 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	dev->regs_base = devm_ioremap_resource(&pdev->dev, res);
 	if (IS_ERR(dev->regs_base))
 		return PTR_ERR(dev->regs_base);
 	res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
 	if (!res) {
 		dev_err(&pdev->dev, ""failed to get irq resource\n"");
 		return -ENOENT;
 	}
 	dev->irq = res->start;
 	ret = devm_request_irq(&pdev->dev, dev->irq, s5p_mfc_irq,
 					0, pdev->name, dev);
 	if (ret) {
 		dev_err(&pdev->dev, ""Failed to install irq (%d)\n"", ret);
 		return ret;
 	}
 	ret = s5p_mfc_configure_dma_memory(dev);
 	if (ret < 0) {
 		dev_err(&pdev->dev, ""failed to configure DMA memory\n"");
 		return ret;
 	}
 	ret = s5p_mfc_init_pm(dev);
 	if (ret < 0) {
 		dev_err(&pdev->dev, ""failed to get mfc clock source\n"");
 		goto err_dma;
 	}
 	mutex_init(&dev->mfc_mutex);
 	init_waitqueue_head(&dev->queue);
 	dev->hw_lock = 0;
 	INIT_WORK(&dev->watchdog_work, s5p_mfc_watchdog_worker);
 	atomic_set(&dev->watchdog_cnt, 0);
-	init_timer(&dev->watchdog_timer);
-	dev->watchdog_timer.data = (unsigned long)dev;
-	dev->watchdog_timer.function = s5p_mfc_watchdog;
+	setup_timer(&dev->watchdog_timer, s5p_mfc_watchdog,
+		    (unsigned long)dev);
 	ret = v4l2_device_register(&pdev->dev, &dev->v4l2_dev);
 	if (ret)
 		goto err_v4l2_dev_reg;
 	
 	vfd = video_device_alloc();
 	if (!vfd) {
 		v4l2_err(&dev->v4l2_dev, ""Failed to allocate video device\n"");
 		ret = -ENOMEM;
 		goto err_dec_alloc;
 	}
 	vfd->fops	= &s5p_mfc_fops;
 	vfd->ioctl_ops	= get_dec_v4l2_ioctl_ops();
 	vfd->release	= video_device_release;
 	vfd->lock	= &dev->mfc_mutex;
 	vfd->v4l2_dev	= &dev->v4l2_dev;
 	vfd->vfl_dir	= VFL_DIR_M2M;
 	snprintf(vfd->name, sizeof(vfd->name), ""%s"", S5P_MFC_DEC_NAME);
 	dev->vfd_dec	= vfd;
 	video_set_drvdata(vfd, dev);
 	
 	vfd = video_device_alloc();
 	if (!vfd) {
 		v4l2_err(&dev->v4l2_dev, ""Failed to allocate video device\n"");
 		ret = -ENOMEM;
 		goto err_enc_alloc;
 	}
 	vfd->fops	= &s5p_mfc_fops;
 	vfd->ioctl_ops	= get_enc_v4l2_ioctl_ops();
 	vfd->release	= video_device_release;
 	vfd->lock	= &dev->mfc_mutex;
 	vfd->v4l2_dev	= &dev->v4l2_dev;
 	vfd->vfl_dir	= VFL_DIR_M2M;
 	snprintf(vfd->name, sizeof(vfd->name), ""%s"", S5P_MFC_ENC_NAME);
 	dev->vfd_enc	= vfd;
 	video_set_drvdata(vfd, dev);
 	platform_set_drvdata(pdev, dev);
 	
 	s5p_mfc_init_hw_ops(dev);
 	s5p_mfc_init_hw_cmds(dev);
 	s5p_mfc_init_regs(dev);
 	
 	ret = video_register_device(dev->vfd_dec, VFL_TYPE_GRABBER, 0);
 	if (ret) {
 		v4l2_err(&dev->v4l2_dev, ""Failed to register video device\n"");
 		goto err_dec_reg;
 	}
 	v4l2_info(&dev->v4l2_dev,
 		  ""decoder registered as /dev/video%d\n"", dev->vfd_dec->num);
 	ret = video_register_device(dev->vfd_enc, VFL_TYPE_GRABBER, 0);
 	if (ret) {
 		v4l2_err(&dev->v4l2_dev, ""Failed to register video device\n"");
 		goto err_enc_reg;
 	}
 	v4l2_info(&dev->v4l2_dev,
 		  ""encoder registered as /dev/video%d\n"", dev->vfd_enc->num);
 	pr_debug(""%s--\n"", __func__);
 	return 0;
 
 err_enc_reg:
 	video_unregister_device(dev->vfd_dec);
 err_dec_reg:
 	video_device_release(dev->vfd_enc);
 err_enc_alloc:
 	video_device_release(dev->vfd_dec);
 err_dec_alloc:
 	v4l2_device_unregister(&dev->v4l2_dev);
 err_v4l2_dev_reg:
 	s5p_mfc_final_pm(dev);
 err_dma:
 	s5p_mfc_unconfigure_dma_memory(dev);
 	pr_debug(""%s-- with error\n"", __func__);
 	return ret;
 }",127,865
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_clocksource_clocksource_start_watchdog,10,9," static inline void clocksource_start_watchdog(void)
 {
 	if (watchdog_running || !watchdog || list_empty(&watchdog_list))
 		return;
-	init_timer(&watchdog_timer);
-	watchdog_timer.function = clocksource_watchdog;
+	setup_timer(&watchdog_timer, clocksource_watchdog, 0UL);
 	watchdog_timer.expires = jiffies + WATCHDOG_INTERVAL;
 	add_timer_on(&watchdog_timer, cpumask_first(cpu_online_mask));
 	watchdog_running = 1;
 }",11,72
kees_timer1,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/kees_timer1/1508184939_2017-10-16_b9eaf1872222_omap_udc_omap_ep_setup,118,116," static unsigned
 omap_ep_setup(char *name, u8 addr, u8 type,
 		unsigned buf, unsigned maxp, int dbuf)
 {
 	struct omap_ep	*ep;
 	u16		epn_rxtx = 0;
 	
 	ep = &udc->ep[addr & 0xf];
 	if (addr & USB_DIR_IN)
 		ep += 16;
 	
 	BUG_ON(ep->name[0]);
 	
 	if (type == USB_ENDPOINT_XFER_ISOC) {
 		switch (maxp) {
 		case 8:
 			epn_rxtx = 0 << 12;
 			break;
 		case 16:
 			epn_rxtx = 1 << 12;
 			break;
 		case 32:
 			epn_rxtx = 2 << 12;
 			break;
 		case 64:
 			epn_rxtx = 3 << 12;
 			break;
 		case 128:
 			epn_rxtx = 4 << 12;
 			break;
 		case 256:
 			epn_rxtx = 5 << 12;
 			break;
 		case 512:
 			epn_rxtx = 6 << 12;
 			break;
 		default:
 			BUG();
 		}
 		epn_rxtx |= UDC_EPN_RX_ISO;
 		dbuf = 1;
 	} else {
 		
 		if (!use_dma || cpu_is_omap15xx())
 			dbuf = 0;
 		switch (maxp) {
 		case 8:
 			epn_rxtx = 0 << 12;
 			break;
 		case 16:
 			epn_rxtx = 1 << 12;
 			break;
 		case 32:
 			epn_rxtx = 2 << 12;
 			break;
 		case 64:
 			epn_rxtx = 3 << 12;
 			break;
 		default:
 			BUG();
 		}
 		if (dbuf && addr)
 			epn_rxtx |= UDC_EPN_RX_DB;
-		init_timer(&ep->timer);
-		ep->timer.function = pio_out_timer;
-		ep->timer.data = (unsigned long) ep;
+		setup_timer(&ep->timer, pio_out_timer, (unsigned long)ep);
 	}
 	if (addr)
 		epn_rxtx |= UDC_EPN_RX_VALID;
 	BUG_ON(buf & 0x07);
 	epn_rxtx |= buf >> 3;
 	DBG(""%s addr %02x rxtx %04x maxp %d%s buf %d\n"",
 		name, addr, epn_rxtx, maxp, dbuf ? ""x2"" : """", buf);
 	if (addr & USB_DIR_IN)
 		omap_writew(epn_rxtx, UDC_EP_TX(addr & 0xf));
 	else
 		omap_writew(epn_rxtx, UDC_EP_RX(addr));
 	
 	buf += maxp;
 	if (dbuf)
 		buf += maxp;
 	BUG_ON(buf > 2048);
 	
 	BUG_ON(strlen(name) >= sizeof ep->name);
 	strlcpy(ep->name, name, sizeof ep->name);
 	INIT_LIST_HEAD(&ep->queue);
 	INIT_LIST_HEAD(&ep->iso);
 	ep->bEndpointAddress = addr;
 	ep->bmAttributes = type;
 	ep->double_buf = dbuf;
 	ep->udc = udc;
 	switch (type) {
 	case USB_ENDPOINT_XFER_CONTROL:
 		ep->ep.caps.type_control = true;
 		ep->ep.caps.dir_in = true;
 		ep->ep.caps.dir_out = true;
 		break;
 	case USB_ENDPOINT_XFER_ISOC:
 		ep->ep.caps.type_iso = true;
 		break;
 	case USB_ENDPOINT_XFER_BULK:
 		ep->ep.caps.type_bulk = true;
 		break;
 	case USB_ENDPOINT_XFER_INT:
 		ep->ep.caps.type_int = true;
 		break;
 	};
 	if (addr & USB_DIR_IN)
 		ep->ep.caps.dir_in = true;
 	else
 		ep->ep.caps.dir_out = true;
 	ep->ep.name = ep->name;
 	ep->ep.ops = &omap_ep_ops;
 	ep->maxpacket = maxp;
 	usb_ep_set_maxpacket_limit(&ep->ep, ep->maxpacket);
 	list_add_tail(&ep->ep.ep_list, &udc->gadget.ep_list);
 	return buf;
 }",119,636
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dma_pool_alloc-52/1518634820_2018-02-15_8d35a9dc4244_ql4_mbx_qla4xxx_set_chap,47,46," int qla4xxx_set_chap(struct scsi_qla_host *ha, char *username, char *password,
 		     uint16_t idx, int bidi)
 {
 	int ret = 0;
 	int rval = QLA_ERROR;
 	uint32_t offset = 0;
 	struct ql4_chap_table *chap_table;
 	uint32_t chap_size = 0;
 	dma_addr_t chap_dma;
-	chap_table = dma_pool_alloc(ha->chap_dma_pool, GFP_KERNEL, &chap_dma);
+	chap_table = dma_pool_zalloc(ha->chap_dma_pool, GFP_KERNEL, &chap_dma);
 	if (chap_table == NULL) {
 		ret =  -ENOMEM;
 		goto exit_set_chap;
 	}
-	memset(chap_table, 0, sizeof(struct ql4_chap_table));
 	if (bidi)
 		chap_table->flags |= BIT_6; 
 	else
 		chap_table->flags |= BIT_7; 
 	chap_table->secret_len = strlen(password);
 	strncpy(chap_table->secret, password, MAX_CHAP_SECRET_LEN - 1);
 	strncpy(chap_table->name, username, MAX_CHAP_NAME_LEN - 1);
 	chap_table->cookie = __constant_cpu_to_le16(CHAP_VALID_COOKIE);
 	if (is_qla40XX(ha)) {
 		chap_size = MAX_CHAP_ENTRIES_40XX * sizeof(*chap_table);
 		offset = FLASH_CHAP_OFFSET;
 	} else { 
 		chap_size = ha->hw.flt_chap_size / 2;
 		offset = FLASH_RAW_ACCESS_ADDR + (ha->hw.flt_region_chap << 2);
 		if (ha->port_num == 1)
 			offset += chap_size;
 	}
 	offset += (idx * sizeof(struct ql4_chap_table));
 	rval = qla4xxx_set_flash(ha, chap_dma, offset,
 				sizeof(struct ql4_chap_table),
 				FLASH_OPT_RMW_COMMIT);
 	if (rval == QLA_SUCCESS && ha->chap_list) {
 		
 		memcpy((struct ql4_chap_table *)ha->chap_list + idx,
 		       chap_table, sizeof(struct ql4_chap_table));
 	}
 	dma_pool_free(ha->chap_dma_pool, chap_table, chap_dma);
 	if (rval != QLA_SUCCESS)
 		ret =  -EINVAL;
 exit_set_chap:
 	return ret;
 }",48,327
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dma_pool_alloc-52/1518632953_2018-02-14_ddd05979f89c_bdc_ep_ep_bd_list_alloc,60,59," static int ep_bd_list_alloc(struct bdc_ep *ep)
 {
 	struct bd_table *prev_table = NULL;
 	int index, num_tabs, bd_p_tab;
 	struct bdc *bdc = ep->bdc;
 	struct bd_table *bd_table;
 	dma_addr_t dma;
 	if (usb_endpoint_xfer_isoc(ep->desc))
 		num_tabs = NUM_TABLES_ISOCH;
 	else
 		num_tabs = NUM_TABLES;
 	bd_p_tab = NUM_BDS_PER_TABLE;
 	
 	dev_dbg(bdc->dev,
 		""%s ep:%p num_tabs:%d\n"",
 		__func__, ep, num_tabs);
 	
 	ep->bd_list.bd_table_array = kzalloc(
 					num_tabs * sizeof(struct bd_table *),
 					GFP_ATOMIC);
 	if (!ep->bd_list.bd_table_array)
 		return -ENOMEM;
 	
 	for (index = 0; index < num_tabs; index++) {
 		
 		bd_table = kzalloc(sizeof(struct bd_table), GFP_ATOMIC);
 		if (!bd_table)
 			goto fail;
-		bd_table->start_bd = dma_pool_alloc(bdc->bd_table_pool,
+		bd_table->start_bd = dma_pool_zalloc(bdc->bd_table_pool,
 							GFP_ATOMIC,
 							&dma);
 		if (!bd_table->start_bd) {
 			kfree(bd_table);
 			goto fail;
 		}
 		bd_table->dma = dma;
 		dev_dbg(bdc->dev,
 			""index:%d start_bd:%p dma=%08llx prev_table:%p\n"",
 			index, bd_table->start_bd,
 			(unsigned long long)bd_table->dma, prev_table);
 		ep->bd_list.bd_table_array[index] = bd_table;
-		memset(bd_table->start_bd, 0, bd_p_tab * sizeof(struct bdc_bd));
 		if (prev_table)
 			chain_table(prev_table, bd_table, bd_p_tab);
 		prev_table = bd_table;
 	}
 	chain_table(prev_table, ep->bd_list.bd_table_array[0], bd_p_tab);
 	
 	ep->bd_list.num_tabs = num_tabs;
 	ep->bd_list.max_bdi  = (num_tabs * bd_p_tab) - 1;
 	ep->bd_list.num_tabs = num_tabs;
 	ep->bd_list.num_bds_table = bd_p_tab;
 	ep->bd_list.eqp_bdi = 0;
 	ep->bd_list.hwd_bdi = 0;
 	return 0;
 fail:
 	
 	ep_bd_list_free(ep, num_tabs);
 	return -ENOMEM;
 }",61,364
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dma_pool_alloc-52/1518639038_2018-02-15_501017f6d4e1_qla_bsg_qla84xx_updatefw,93,92," static int
 qla84xx_updatefw(struct bsg_job *bsg_job)
 {
 	struct fc_bsg_request *bsg_request = bsg_job->request;
 	struct fc_bsg_reply *bsg_reply = bsg_job->reply;
 	struct Scsi_Host *host = fc_bsg_to_shost(bsg_job);
 	scsi_qla_host_t *vha = shost_priv(host);
 	struct qla_hw_data *ha = vha->hw;
 	struct verify_chip_entry_84xx *mn = NULL;
 	dma_addr_t mn_dma, fw_dma;
 	void *fw_buf = NULL;
 	int rval = 0;
 	uint32_t sg_cnt;
 	uint32_t data_len;
 	uint16_t options;
 	uint32_t flag;
 	uint32_t fw_ver;
 	if (!IS_QLA84XX(ha)) {
 		ql_dbg(ql_dbg_user, vha, 0x7032,
 		    ""Not 84xx, exiting.\n"");
 		return -EINVAL;
 	}
 	sg_cnt = dma_map_sg(&ha->pdev->dev, bsg_job->request_payload.sg_list,
 		bsg_job->request_payload.sg_cnt, DMA_TO_DEVICE);
 	if (!sg_cnt) {
 		ql_log(ql_log_warn, vha, 0x7033,
 		    ""dma_map_sg returned %d for request.\n"", sg_cnt);
 		return -ENOMEM;
 	}
 	if (sg_cnt != bsg_job->request_payload.sg_cnt) {
 		ql_log(ql_log_warn, vha, 0x7034,
 		    ""DMA mapping resulted in different sg counts, ""
 		    ""request_sg_cnt: %x dma_request_sg_cnt: %x.\n"",
 		    bsg_job->request_payload.sg_cnt, sg_cnt);
 		rval = -EAGAIN;
 		goto done_unmap_sg;
 	}
 	data_len = bsg_job->request_payload.payload_len;
 	fw_buf = dma_alloc_coherent(&ha->pdev->dev, data_len,
 		&fw_dma, GFP_KERNEL);
 	if (!fw_buf) {
 		ql_log(ql_log_warn, vha, 0x7035,
 		    ""DMA alloc failed for fw_buf.\n"");
 		rval = -ENOMEM;
 		goto done_unmap_sg;
 	}
 	sg_copy_to_buffer(bsg_job->request_payload.sg_list,
 		bsg_job->request_payload.sg_cnt, fw_buf, data_len);
-	mn = dma_pool_alloc(ha->s_dma_pool, GFP_KERNEL, &mn_dma);
+	mn = dma_pool_zalloc(ha->s_dma_pool, GFP_KERNEL, &mn_dma);
 	if (!mn) {
 		ql_log(ql_log_warn, vha, 0x7036,
 		    ""DMA alloc failed for fw buffer.\n"");
 		rval = -ENOMEM;
 		goto done_free_fw_buf;
 	}
 	flag = bsg_request->rqst_data.h_vendor.vendor_cmd[1];
 	fw_ver = le32_to_cpu(*((uint32_t *)((uint32_t *)fw_buf + 2)));
-	memset(mn, 0, sizeof(struct access_chip_84xx));
 	mn->entry_type = VERIFY_CHIP_IOCB_TYPE;
 	mn->entry_count = 1;
 	options = VCO_FORCE_UPDATE | VCO_END_OF_DATA;
 	if (flag == A84_ISSUE_UPDATE_DIAGFW_CMD)
 		options |= VCO_DIAG_FW;
 	mn->options = cpu_to_le16(options);
 	mn->fw_ver =  cpu_to_le32(fw_ver);
 	mn->fw_size =  cpu_to_le32(data_len);
 	mn->fw_seq_size =  cpu_to_le32(data_len);
 	mn->dseg_address[0] = cpu_to_le32(LSD(fw_dma));
 	mn->dseg_address[1] = cpu_to_le32(MSD(fw_dma));
 	mn->dseg_length = cpu_to_le32(data_len);
 	mn->data_seg_cnt = cpu_to_le16(1);
 	rval = qla2x00_issue_iocb_timeout(vha, mn, mn_dma, 0, 120);
 	if (rval) {
 		ql_log(ql_log_warn, vha, 0x7037,
 		    ""Vendor request 84xx updatefw failed.\n"");
 		rval = (DID_ERROR << 16);
 	} else {
 		ql_dbg(ql_dbg_user, vha, 0x7038,
 		    ""Vendor request 84xx updatefw completed.\n"");
 		bsg_job->reply_len = sizeof(struct fc_bsg_reply);
 		bsg_reply->result = DID_OK;
 	}
 	dma_pool_free(ha->s_dma_pool, mn, mn_dma);
 done_free_fw_buf:
 	dma_free_coherent(&ha->pdev->dev, data_len, fw_buf, fw_dma);
 done_unmap_sg:
 	dma_unmap_sg(&ha->pdev->dev, bsg_job->request_payload.sg_list,
 		bsg_job->request_payload.sg_cnt, DMA_TO_DEVICE);
 	if (!rval)
 		bsg_job_done(bsg_job, bsg_reply->result,
 			       bsg_reply->reply_payload_rcv_len);
 	return rval;
 }",94,668
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dma_pool_alloc-52/1518711906_2018-02-15_61b142afb2e2_megaraid_sas_base_megasas_create_frame_pool,55,54," static int megasas_create_frame_pool(struct megasas_instance *instance)
 {
 	int i;
 	u16 max_cmd;
 	u32 sge_sz;
 	u32 frame_count;
 	struct megasas_cmd *cmd;
 	max_cmd = instance->max_mfi_cmds;
 	
 	sge_sz = (IS_DMA64) ? sizeof(struct megasas_sge64) :
 	    sizeof(struct megasas_sge32);
 	if (instance->flag_ieee)
 		sge_sz = sizeof(struct megasas_sge_skinny);
 	
 	frame_count = (instance->adapter_type == MFI_SERIES) ?
 			(15 + 1) : (3 + 1);
 	instance->mfi_frame_size = MEGAMFI_FRAME_SIZE * frame_count;
 	
 	instance->frame_dma_pool = dma_pool_create(""megasas frame pool"",
 					&instance->pdev->dev,
 					instance->mfi_frame_size, 256, 0);
 	if (!instance->frame_dma_pool) {
 		dev_printk(KERN_DEBUG, &instance->pdev->dev, ""failed to setup frame pool\n"");
 		return -ENOMEM;
 	}
 	instance->sense_dma_pool = dma_pool_create(""megasas sense pool"",
 						   &instance->pdev->dev, 128,
 						   4, 0);
 	if (!instance->sense_dma_pool) {
 		dev_printk(KERN_DEBUG, &instance->pdev->dev, ""failed to setup sense pool\n"");
 		dma_pool_destroy(instance->frame_dma_pool);
 		instance->frame_dma_pool = NULL;
 		return -ENOMEM;
 	}
 	
 	for (i = 0; i < max_cmd; i++) {
 		cmd = instance->cmd_list[i];
-		cmd->frame = dma_pool_alloc(instance->frame_dma_pool,
+		cmd->frame = dma_pool_zalloc(instance->frame_dma_pool,
 					    GFP_KERNEL, &cmd->frame_phys_addr);
 		cmd->sense = dma_pool_alloc(instance->sense_dma_pool,
 					    GFP_KERNEL, &cmd->sense_phys_addr);
 		
 		if (!cmd->frame || !cmd->sense) {
 			dev_printk(KERN_DEBUG, &instance->pdev->dev, ""dma_pool_alloc failed\n"");
 			megasas_teardown_frame_pool(instance);
 			return -ENOMEM;
 		}
-		memset(cmd->frame, 0, instance->mfi_frame_size);
 		cmd->frame->io.context = cpu_to_le32(cmd->index);
 		cmd->frame->io.pad_0 = 0;
 		if ((instance->adapter_type == MFI_SERIES) && reset_devices)
 			cmd->frame->hdr.cmd = MFI_CMD_INVALID;
 	}
 	return 0;
 }",56,396
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dma_pool_alloc-52/1518634820_2018-02-15_8d35a9dc4244_ql4_mbx_qla4xxx_get_chap,40,39," int qla4xxx_get_chap(struct scsi_qla_host *ha, char *username, char *password,
 		     uint16_t idx)
 {
 	int ret = 0;
 	int rval = QLA_ERROR;
 	uint32_t offset = 0, chap_size;
 	struct ql4_chap_table *chap_table;
 	dma_addr_t chap_dma;
-	chap_table = dma_pool_alloc(ha->chap_dma_pool, GFP_KERNEL, &chap_dma);
+	chap_table = dma_pool_zalloc(ha->chap_dma_pool, GFP_KERNEL, &chap_dma);
 	if (chap_table == NULL)
 		return -ENOMEM;
 	chap_size = sizeof(struct ql4_chap_table);
-	memset(chap_table, 0, chap_size);
 	if (is_qla40XX(ha))
 		offset = FLASH_CHAP_OFFSET | (idx * chap_size);
 	else {
 		offset = FLASH_RAW_ACCESS_ADDR + (ha->hw.flt_region_chap << 2);
 		
 		if (ha->port_num == 1)
 			offset += (ha->hw.flt_chap_size / 2);
 		offset += (idx * chap_size);
 	}
 	rval = qla4xxx_get_flash(ha, chap_dma, offset, chap_size);
 	if (rval != QLA_SUCCESS) {
 		ret = -EINVAL;
 		goto exit_get_chap;
 	}
 	DEBUG2(ql4_printk(KERN_INFO, ha, ""Chap Cookie: x%x\n"",
 		__le16_to_cpu(chap_table->cookie)));
 	if (__le16_to_cpu(chap_table->cookie) != CHAP_VALID_COOKIE) {
 		ql4_printk(KERN_ERR, ha, ""No valid chap entry found\n"");
 		goto exit_get_chap;
 	}
 	strlcpy(password, chap_table->secret, QL4_CHAP_MAX_SECRET_LEN);
 	strlcpy(username, chap_table->name, QL4_CHAP_MAX_NAME_LEN);
 	chap_table->cookie = __constant_cpu_to_le16(CHAP_VALID_COOKIE);
 exit_get_chap:
 	dma_pool_free(ha->chap_dma_pool, chap_table, chap_dma);
 	return ret;
 }",41,292
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dma_pool_alloc-52/1520514717_2018-03-08_8b1bb6dcba76_ipr_ipr_alloc_cmd_blks,89,88," static int ipr_alloc_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)
 {
 	struct ipr_cmnd *ipr_cmd;
 	struct ipr_ioarcb *ioarcb;
 	dma_addr_t dma_addr;
 	int i, entries_each_hrrq, hrrq_id = 0;
 	ioa_cfg->ipr_cmd_pool = dma_pool_create(IPR_NAME, &ioa_cfg->pdev->dev,
 						sizeof(struct ipr_cmnd), 512, 0);
 	if (!ioa_cfg->ipr_cmd_pool)
 		return -ENOMEM;
 	ioa_cfg->ipr_cmnd_list = kcalloc(IPR_NUM_CMD_BLKS, sizeof(struct ipr_cmnd *), GFP_KERNEL);
 	ioa_cfg->ipr_cmnd_list_dma = kcalloc(IPR_NUM_CMD_BLKS, sizeof(dma_addr_t), GFP_KERNEL);
 	if (!ioa_cfg->ipr_cmnd_list || !ioa_cfg->ipr_cmnd_list_dma) {
 		ipr_free_cmd_blks(ioa_cfg);
 		return -ENOMEM;
 	}
 	for (i = 0; i < ioa_cfg->hrrq_num; i++) {
 		if (ioa_cfg->hrrq_num > 1) {
 			if (i == 0) {
 				entries_each_hrrq = IPR_NUM_INTERNAL_CMD_BLKS;
 				ioa_cfg->hrrq[i].min_cmd_id = 0;
 				ioa_cfg->hrrq[i].max_cmd_id =
 					(entries_each_hrrq - 1);
 			} else {
 				entries_each_hrrq =
 					IPR_NUM_BASE_CMD_BLKS/
 					(ioa_cfg->hrrq_num - 1);
 				ioa_cfg->hrrq[i].min_cmd_id =
 					IPR_NUM_INTERNAL_CMD_BLKS +
 					(i - 1) * entries_each_hrrq;
 				ioa_cfg->hrrq[i].max_cmd_id =
 					(IPR_NUM_INTERNAL_CMD_BLKS +
 					i * entries_each_hrrq - 1);
 			}
 		} else {
 			entries_each_hrrq = IPR_NUM_CMD_BLKS;
 			ioa_cfg->hrrq[i].min_cmd_id = 0;
 			ioa_cfg->hrrq[i].max_cmd_id = (entries_each_hrrq - 1);
 		}
 		ioa_cfg->hrrq[i].size = entries_each_hrrq;
 	}
 	BUG_ON(ioa_cfg->hrrq_num == 0);
 	i = IPR_NUM_CMD_BLKS -
 		ioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].max_cmd_id - 1;
 	if (i > 0) {
 		ioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].size += i;
 		ioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].max_cmd_id += i;
 	}
 	for (i = 0; i < IPR_NUM_CMD_BLKS; i++) {
-		ipr_cmd = dma_pool_alloc(ioa_cfg->ipr_cmd_pool, GFP_KERNEL, &dma_addr);
+		ipr_cmd = dma_pool_zalloc(ioa_cfg->ipr_cmd_pool, GFP_KERNEL, &dma_addr);
 		if (!ipr_cmd) {
 			ipr_free_cmd_blks(ioa_cfg);
 			return -ENOMEM;
 		}
-		memset(ipr_cmd, 0, sizeof(*ipr_cmd));
 		ioa_cfg->ipr_cmnd_list[i] = ipr_cmd;
 		ioa_cfg->ipr_cmnd_list_dma[i] = dma_addr;
 		ioarcb = &ipr_cmd->ioarcb;
 		ipr_cmd->dma_addr = dma_addr;
 		if (ioa_cfg->sis64)
 			ioarcb->a.ioarcb_host_pci_addr64 = cpu_to_be64(dma_addr);
 		else
 			ioarcb->a.ioarcb_host_pci_addr = cpu_to_be32(dma_addr);
 		ioarcb->host_response_handle = cpu_to_be32(i << 2);
 		if (ioa_cfg->sis64) {
 			ioarcb->u.sis64_addr_data.data_ioadl_addr =
 				cpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, i.ioadl64));
 			ioarcb->u.sis64_addr_data.ioasa_host_pci_addr =
 				cpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, s.ioasa64));
 		} else {
 			ioarcb->write_ioadl_addr =
 				cpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, i.ioadl));
 			ioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;
 			ioarcb->ioasa_host_pci_addr =
 				cpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, s.ioasa));
 		}
 		ioarcb->ioasa_len = cpu_to_be16(sizeof(struct ipr_ioasa));
 		ipr_cmd->cmd_index = i;
 		ipr_cmd->ioa_cfg = ioa_cfg;
 		ipr_cmd->sense_buffer_dma = dma_addr +
 			offsetof(struct ipr_cmnd, sense_buffer);
 		ipr_cmd->ioarcb.cmd_pkt.hrrq_id = hrrq_id;
 		ipr_cmd->hrrq = &ioa_cfg->hrrq[hrrq_id];
 		list_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);
 		if (i >= ioa_cfg->hrrq[hrrq_id].max_cmd_id)
 			hrrq_id++;
 	}
 	return 0;
 }",90,699
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dma_pool_alloc-52/1518634820_2018-02-15_8d35a9dc4244_ql4_os_qla4xxx_delete_chap,69,68," static int qla4xxx_delete_chap(struct Scsi_Host *shost, uint16_t chap_tbl_idx)
 {
 	struct scsi_qla_host *ha = to_qla_host(shost);
 	struct ql4_chap_table *chap_table;
 	dma_addr_t chap_dma;
 	int max_chap_entries = 0;
 	uint32_t offset = 0;
 	uint32_t chap_size;
 	int ret = 0;
-	chap_table = dma_pool_alloc(ha->chap_dma_pool, GFP_KERNEL, &chap_dma);
+	chap_table = dma_pool_zalloc(ha->chap_dma_pool, GFP_KERNEL, &chap_dma);
 	if (chap_table == NULL)
 		return -ENOMEM;
-	memset(chap_table, 0, sizeof(struct ql4_chap_table));
 	if (is_qla80XX(ha))
 		max_chap_entries = (ha->hw.flt_chap_size / 2) /
 				   sizeof(struct ql4_chap_table);
 	else
 		max_chap_entries = MAX_CHAP_ENTRIES_40XX;
 	if (chap_tbl_idx > max_chap_entries) {
 		ret = -EINVAL;
 		goto exit_delete_chap;
 	}
 	
 	ret = qla4xxx_is_chap_active(shost, chap_tbl_idx);
 	if (ret) {
 		ql4_printk(KERN_INFO, ha, ""CHAP entry %d is in use, cannot ""
 			   ""delete from flash\n"", chap_tbl_idx);
 		ret = -EBUSY;
 		goto exit_delete_chap;
 	}
 	chap_size = sizeof(struct ql4_chap_table);
 	if (is_qla40XX(ha))
 		offset = FLASH_CHAP_OFFSET | (chap_tbl_idx * chap_size);
 	else {
 		offset = FLASH_RAW_ACCESS_ADDR + (ha->hw.flt_region_chap << 2);
 		
 		if (ha->port_num == 1)
 			offset += (ha->hw.flt_chap_size / 2);
 		offset += (chap_tbl_idx * chap_size);
 	}
 	ret = qla4xxx_get_flash(ha, chap_dma, offset, chap_size);
 	if (ret != QLA_SUCCESS) {
 		ret = -EINVAL;
 		goto exit_delete_chap;
 	}
 	DEBUG2(ql4_printk(KERN_INFO, ha, ""Chap Cookie: x%x\n"",
 			  __le16_to_cpu(chap_table->cookie)));
 	if (__le16_to_cpu(chap_table->cookie) != CHAP_VALID_COOKIE) {
 		ql4_printk(KERN_ERR, ha, ""No valid chap entry found\n"");
 		goto exit_delete_chap;
 	}
 	chap_table->cookie = __constant_cpu_to_le16(0xFFFF);
 	offset = FLASH_CHAP_OFFSET |
 			(chap_tbl_idx * sizeof(struct ql4_chap_table));
 	ret = qla4xxx_set_flash(ha, chap_dma, offset, chap_size,
 				FLASH_OPT_RMW_COMMIT);
 	if (ret == QLA_SUCCESS && ha->chap_list) {
 		mutex_lock(&ha->chap_sem);
 		
 		memcpy((struct ql4_chap_table *)ha->chap_list + chap_tbl_idx,
 			chap_table, sizeof(struct ql4_chap_table));
 		mutex_unlock(&ha->chap_sem);
 	}
 	if (ret != QLA_SUCCESS)
 		ret =  -EINVAL;
 exit_delete_chap:
 	dma_pool_free(ha->chap_dma_pool, chap_table, chap_dma);
 	return ret;
 }",70,454
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dma_pool_alloc-52/1518860473_2018-02-17_6de84023d31b_bcm-flexrm-mailbox_flexrm_startup,120,119," static int flexrm_startup(struct mbox_chan *chan)
 {
 	u64 d;
 	u32 val, off;
 	int ret = 0;
 	dma_addr_t next_addr;
 	struct flexrm_ring *ring = chan->con_priv;
 	
 	ring->bd_base = dma_pool_alloc(ring->mbox->bd_pool,
 				       GFP_KERNEL, &ring->bd_dma_base);
 	if (!ring->bd_base) {
 		dev_err(ring->mbox->dev,
 			""can't allocate BD memory for ring%d\n"",
 			ring->num);
 		ret = -ENOMEM;
 		goto fail;
 	}
 	
 	for (off = 0; off < RING_BD_SIZE; off += RING_DESC_SIZE) {
 		next_addr = off + RING_DESC_SIZE;
 		if (next_addr == RING_BD_SIZE)
 			next_addr = 0;
 		next_addr += ring->bd_dma_base;
 		if (RING_BD_ALIGN_CHECK(next_addr))
 			d = flexrm_next_table_desc(RING_BD_TOGGLE_VALID(off),
 						    next_addr);
 		else
 			d = flexrm_null_desc(RING_BD_TOGGLE_INVALID(off));
 		flexrm_write_desc(ring->bd_base + off, d);
 	}
 	
-	ring->cmpl_base = dma_pool_alloc(ring->mbox->cmpl_pool,
+	ring->cmpl_base = dma_pool_zalloc(ring->mbox->cmpl_pool,
 					 GFP_KERNEL, &ring->cmpl_dma_base);
 	if (!ring->cmpl_base) {
 		dev_err(ring->mbox->dev,
 			""can't allocate completion memory for ring%d\n"",
 			ring->num);
 		ret = -ENOMEM;
 		goto fail_free_bd_memory;
 	}
-	memset(ring->cmpl_base, 0, RING_CMPL_SIZE);
 	
 	if (ring->irq == UINT_MAX) {
 		dev_err(ring->mbox->dev,
 			""ring%d IRQ not available\n"", ring->num);
 		ret = -ENODEV;
 		goto fail_free_cmpl_memory;
 	}
 	ret = request_threaded_irq(ring->irq,
 				   flexrm_irq_event,
 				   flexrm_irq_thread,
 				   0, dev_name(ring->mbox->dev), ring);
 	if (ret) {
 		dev_err(ring->mbox->dev,
 			""failed to request ring%d IRQ\n"", ring->num);
 		goto fail_free_cmpl_memory;
 	}
 	ring->irq_requested = true;
 	
 	ring->irq_aff_hint = CPU_MASK_NONE;
 	val = ring->mbox->num_rings;
 	val = (num_online_cpus() < val) ? val / num_online_cpus() : 1;
 	cpumask_set_cpu((ring->num / val) % num_online_cpus(),
 			&ring->irq_aff_hint);
 	ret = irq_set_affinity_hint(ring->irq, &ring->irq_aff_hint);
 	if (ret) {
 		dev_err(ring->mbox->dev,
 			""failed to set IRQ affinity hint for ring%d\n"",
 			ring->num);
 		goto fail_free_irq;
 	}
 	
 	writel_relaxed(0x0, ring->regs + RING_CONTROL);
 	
 	val = BD_START_ADDR_VALUE(ring->bd_dma_base);
 	writel_relaxed(val, ring->regs + RING_BD_START_ADDR);
 	
 	ring->bd_write_offset =
 			readl_relaxed(ring->regs + RING_BD_WRITE_PTR);
 	ring->bd_write_offset *= RING_DESC_SIZE;
 	
 	val = CMPL_START_ADDR_VALUE(ring->cmpl_dma_base);
 	writel_relaxed(val, ring->regs + RING_CMPL_START_ADDR);
 	
 	ring->cmpl_read_offset =
 			readl_relaxed(ring->regs + RING_CMPL_WRITE_PTR);
 	ring->cmpl_read_offset *= RING_DESC_SIZE;
 	
 	readl_relaxed(ring->regs + RING_NUM_REQ_RECV_LS);
 	readl_relaxed(ring->regs + RING_NUM_REQ_RECV_MS);
 	readl_relaxed(ring->regs + RING_NUM_REQ_TRANS_LS);
 	readl_relaxed(ring->regs + RING_NUM_REQ_TRANS_MS);
 	readl_relaxed(ring->regs + RING_NUM_REQ_OUTSTAND);
 	
 	val = 0;
 	val |= (ring->msi_timer_val << MSI_TIMER_VAL_SHIFT);
 	val |= BIT(MSI_ENABLE_SHIFT);
 	val |= (ring->msi_count_threshold & MSI_COUNT_MASK) << MSI_COUNT_SHIFT;
 	writel_relaxed(val, ring->regs + RING_MSI_CONTROL);
 	
 	val = BIT(CONTROL_ACTIVE_SHIFT);
 	writel_relaxed(val, ring->regs + RING_CONTROL);
 	
 	atomic_set(&ring->msg_send_count, 0);
 	atomic_set(&ring->msg_cmpl_count, 0);
 	return 0;
 fail_free_irq:
 	free_irq(ring->irq, ring);
 	ring->irq_requested = false;
 fail_free_cmpl_memory:
 	dma_pool_free(ring->mbox->cmpl_pool,
 		      ring->cmpl_base, ring->cmpl_dma_base);
 	ring->cmpl_base = NULL;
 fail_free_bd_memory:
 	dma_pool_free(ring->mbox->bd_pool,
 		      ring->bd_base, ring->bd_dma_base);
 	ring->bd_base = NULL;
 fail:
 	return ret;
 }",121,731
dma_pool_alloc-52,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dma_pool_alloc-52/1518639038_2018-02-15_501017f6d4e1_qla_iocb_qla82xx_start_scsi,278,277," int
 qla82xx_start_scsi(srb_t *sp)
 {
 	int		nseg;
 	unsigned long   flags;
 	struct scsi_cmnd *cmd;
 	uint32_t	*clr_ptr;
 	uint32_t        index;
 	uint32_t	handle;
 	uint16_t	cnt;
 	uint16_t	req_cnt;
 	uint16_t	tot_dsds;
 	struct device_reg_82xx __iomem *reg;
 	uint32_t dbval;
 	uint32_t *fcp_dl;
 	uint8_t additional_cdb_len;
 	struct ct6_dsd *ctx;
 	struct scsi_qla_host *vha = sp->vha;
 	struct qla_hw_data *ha = vha->hw;
 	struct req_que *req = NULL;
 	struct rsp_que *rsp = NULL;
 	
 	reg = &ha->iobase->isp82;
 	cmd = GET_CMD_SP(sp);
 	req = vha->req;
 	rsp = ha->rsp_q_map[0];
 	
 	tot_dsds = 0;
 	dbval = 0x04 | (ha->portnum << 5);
 	
 	if (vha->marker_needed != 0) {
 		if (qla2x00_marker(vha, req,
 			rsp, 0, 0, MK_SYNC_ALL) != QLA_SUCCESS) {
 			ql_log(ql_log_warn, vha, 0x300c,
 			    ""qla2x00_marker failed for cmd=%p.\n"", cmd);
 			return QLA_FUNCTION_FAILED;
 		}
 		vha->marker_needed = 0;
 	}
 	
 	spin_lock_irqsave(&ha->hardware_lock, flags);
 	
 	handle = req->current_outstanding_cmd;
 	for (index = 1; index < req->num_outstanding_cmds; index++) {
 		handle++;
 		if (handle == req->num_outstanding_cmds)
 			handle = 1;
 		if (!req->outstanding_cmds[handle])
 			break;
 	}
 	if (index == req->num_outstanding_cmds)
 		goto queuing_error;
 	
 	if (scsi_sg_count(cmd)) {
 		nseg = dma_map_sg(&ha->pdev->dev, scsi_sglist(cmd),
 		    scsi_sg_count(cmd), cmd->sc_data_direction);
 		if (unlikely(!nseg))
 			goto queuing_error;
 	} else
 		nseg = 0;
 	tot_dsds = nseg;
 	if (tot_dsds > ql2xshiftctondsd) {
 		struct cmd_type_6 *cmd_pkt;
 		uint16_t more_dsd_lists = 0;
 		struct dsd_dma *dsd_ptr;
 		uint16_t i;
 		more_dsd_lists = qla24xx_calc_dsd_lists(tot_dsds);
 		if ((more_dsd_lists + ha->gbl_dsd_inuse) >= NUM_DSD_CHAIN) {
 			ql_dbg(ql_dbg_io, vha, 0x300d,
 			    ""Num of DSD list %d is than %d for cmd=%p.\n"",
 			    more_dsd_lists + ha->gbl_dsd_inuse, NUM_DSD_CHAIN,
 			    cmd);
 			goto queuing_error;
 		}
 		if (more_dsd_lists <= ha->gbl_dsd_avail)
 			goto sufficient_dsds;
 		else
 			more_dsd_lists -= ha->gbl_dsd_avail;
 		for (i = 0; i < more_dsd_lists; i++) {
 			dsd_ptr = kzalloc(sizeof(struct dsd_dma), GFP_ATOMIC);
 			if (!dsd_ptr) {
 				ql_log(ql_log_fatal, vha, 0x300e,
 				    ""Failed to allocate memory for dsd_dma ""
 				    ""for cmd=%p.\n"", cmd);
 				goto queuing_error;
 			}
 			dsd_ptr->dsd_addr = dma_pool_alloc(ha->dl_dma_pool,
 				GFP_ATOMIC, &dsd_ptr->dsd_list_dma);
 			if (!dsd_ptr->dsd_addr) {
 				kfree(dsd_ptr);
 				ql_log(ql_log_fatal, vha, 0x300f,
 				    ""Failed to allocate memory for dsd_addr ""
 				    ""for cmd=%p.\n"", cmd);
 				goto queuing_error;
 			}
 			list_add_tail(&dsd_ptr->list, &ha->gbl_dsd_list);
 			ha->gbl_dsd_avail++;
 		}
 sufficient_dsds:
 		req_cnt = 1;
 		if (req->cnt < (req_cnt + 2)) {
 			cnt = (uint16_t)RD_REG_DWORD_RELAXED(
 				&reg->req_q_out[0]);
 			if (req->ring_index < cnt)
 				req->cnt = cnt - req->ring_index;
 			else
 				req->cnt = req->length -
 					(req->ring_index - cnt);
 			if (req->cnt < (req_cnt + 2))
 				goto queuing_error;
 		}
 		ctx = sp->u.scmd.ctx =
 		    mempool_alloc(ha->ctx_mempool, GFP_ATOMIC);
 		if (!ctx) {
 			ql_log(ql_log_fatal, vha, 0x3010,
 			    ""Failed to allocate ctx for cmd=%p.\n"", cmd);
 			goto queuing_error;
 		}
 		memset(ctx, 0, sizeof(struct ct6_dsd));
-		ctx->fcp_cmnd = dma_pool_alloc(ha->fcp_cmnd_dma_pool,
+		ctx->fcp_cmnd = dma_pool_zalloc(ha->fcp_cmnd_dma_pool,
 			GFP_ATOMIC, &ctx->fcp_cmnd_dma);
 		if (!ctx->fcp_cmnd) {
 			ql_log(ql_log_fatal, vha, 0x3011,
 			    ""Failed to allocate fcp_cmnd for cmd=%p.\n"", cmd);
 			goto queuing_error;
 		}
 		
 		INIT_LIST_HEAD(&ctx->dsd_list);
 		ctx->dsd_use_cnt = 0;
 		if (cmd->cmd_len > 16) {
 			additional_cdb_len = cmd->cmd_len - 16;
 			if ((cmd->cmd_len % 4) != 0) {
 				
 				ql_log(ql_log_warn, vha, 0x3012,
 				    ""scsi cmd len %d not multiple of 4 ""
 				    ""for cmd=%p.\n"", cmd->cmd_len, cmd);
 				goto queuing_error_fcp_cmnd;
 			}
 			ctx->fcp_cmnd_len = 12 + cmd->cmd_len + 4;
 		} else {
 			additional_cdb_len = 0;
 			ctx->fcp_cmnd_len = 12 + 16 + 4;
 		}
 		cmd_pkt = (struct cmd_type_6 *)req->ring_ptr;
 		cmd_pkt->handle = MAKE_HANDLE(req->id, handle);
 		
 		
 		clr_ptr = (uint32_t *)cmd_pkt + 2;
 		memset(clr_ptr, 0, REQUEST_ENTRY_SIZE - 8);
 		cmd_pkt->dseg_count = cpu_to_le16(tot_dsds);
 		
 		cmd_pkt->nport_handle = cpu_to_le16(sp->fcport->loop_id);
 		cmd_pkt->port_id[0] = sp->fcport->d_id.b.al_pa;
 		cmd_pkt->port_id[1] = sp->fcport->d_id.b.area;
 		cmd_pkt->port_id[2] = sp->fcport->d_id.b.domain;
 		cmd_pkt->vp_index = sp->vha->vp_idx;
 		
 		if (qla24xx_build_scsi_type_6_iocbs(sp, cmd_pkt, tot_dsds))
 			goto queuing_error_fcp_cmnd;
 		int_to_scsilun(cmd->device->lun, &cmd_pkt->lun);
 		host_to_fcp_swap((uint8_t *)&cmd_pkt->lun, sizeof(cmd_pkt->lun));
 		
-		memset(ctx->fcp_cmnd, 0, sizeof(struct fcp_cmnd));
 		int_to_scsilun(cmd->device->lun, &ctx->fcp_cmnd->lun);
 		ctx->fcp_cmnd->additional_cdb_len = additional_cdb_len;
 		if (cmd->sc_data_direction == DMA_TO_DEVICE)
 			ctx->fcp_cmnd->additional_cdb_len |= 1;
 		else if (cmd->sc_data_direction == DMA_FROM_DEVICE)
 			ctx->fcp_cmnd->additional_cdb_len |= 2;
 		
 		if (ha->flags.fcp_prio_enabled)
 			ctx->fcp_cmnd->task_attribute |=
 			    sp->fcport->fcp_prio << 3;
 		memcpy(ctx->fcp_cmnd->cdb, cmd->cmnd, cmd->cmd_len);
 		fcp_dl = (uint32_t *)(ctx->fcp_cmnd->cdb + 16 +
 		    additional_cdb_len);
 		*fcp_dl = htonl((uint32_t)scsi_bufflen(cmd));
 		cmd_pkt->fcp_cmnd_dseg_len = cpu_to_le16(ctx->fcp_cmnd_len);
 		cmd_pkt->fcp_cmnd_dseg_address[0] =
 		    cpu_to_le32(LSD(ctx->fcp_cmnd_dma));
 		cmd_pkt->fcp_cmnd_dseg_address[1] =
 		    cpu_to_le32(MSD(ctx->fcp_cmnd_dma));
 		sp->flags |= SRB_FCP_CMND_DMA_VALID;
 		cmd_pkt->byte_count = cpu_to_le32((uint32_t)scsi_bufflen(cmd));
 		
 		cmd_pkt->entry_count = (uint8_t)req_cnt;
 		
 		cmd_pkt->entry_status = (uint8_t) rsp->id;
 	} else {
 		struct cmd_type_7 *cmd_pkt;
 		req_cnt = qla24xx_calc_iocbs(vha, tot_dsds);
 		if (req->cnt < (req_cnt + 2)) {
 			cnt = (uint16_t)RD_REG_DWORD_RELAXED(
 			    &reg->req_q_out[0]);
 			if (req->ring_index < cnt)
 				req->cnt = cnt - req->ring_index;
 			else
 				req->cnt = req->length -
 					(req->ring_index - cnt);
 		}
 		if (req->cnt < (req_cnt + 2))
 			goto queuing_error;
 		cmd_pkt = (struct cmd_type_7 *)req->ring_ptr;
 		cmd_pkt->handle = MAKE_HANDLE(req->id, handle);
 		
 		
 		clr_ptr = (uint32_t *)cmd_pkt + 2;
 		memset(clr_ptr, 0, REQUEST_ENTRY_SIZE - 8);
 		cmd_pkt->dseg_count = cpu_to_le16(tot_dsds);
 		
 		cmd_pkt->nport_handle = cpu_to_le16(sp->fcport->loop_id);
 		cmd_pkt->port_id[0] = sp->fcport->d_id.b.al_pa;
 		cmd_pkt->port_id[1] = sp->fcport->d_id.b.area;
 		cmd_pkt->port_id[2] = sp->fcport->d_id.b.domain;
 		cmd_pkt->vp_index = sp->vha->vp_idx;
 		int_to_scsilun(cmd->device->lun, &cmd_pkt->lun);
 		host_to_fcp_swap((uint8_t *)&cmd_pkt->lun,
 		    sizeof(cmd_pkt->lun));
 		
 		if (ha->flags.fcp_prio_enabled)
 			cmd_pkt->task |= sp->fcport->fcp_prio << 3;
 		
 		memcpy(cmd_pkt->fcp_cdb, cmd->cmnd, cmd->cmd_len);
 		host_to_fcp_swap(cmd_pkt->fcp_cdb, sizeof(cmd_pkt->fcp_cdb));
 		cmd_pkt->byte_count = cpu_to_le32((uint32_t)scsi_bufflen(cmd));
 		
 		qla24xx_build_scsi_iocbs(sp, cmd_pkt, tot_dsds, req);
 		
 		cmd_pkt->entry_count = (uint8_t)req_cnt;
 		
 		cmd_pkt->entry_status = (uint8_t) rsp->id;
 	}
 	
 	req->current_outstanding_cmd = handle;
 	req->outstanding_cmds[handle] = sp;
 	sp->handle = handle;
 	cmd->host_scribble = (unsigned char *)(unsigned long)handle;
 	req->cnt -= req_cnt;
 	wmb();
 	
 	req->ring_index++;
 	if (req->ring_index == req->length) {
 		req->ring_index = 0;
 		req->ring_ptr = req->ring;
 	} else
 		req->ring_ptr++;
 	sp->flags |= SRB_DMA_VALID;
 	
 	
 	dbval = dbval | (req->id << 8) | (req->ring_index << 16);
 	if (ql2xdbwr)
 		qla82xx_wr_32(ha, (uintptr_t __force)ha->nxdb_wr_ptr, dbval);
 	else {
 		WRT_REG_DWORD(ha->nxdb_wr_ptr, dbval);
 		wmb();
 		while (RD_REG_DWORD(ha->nxdb_rd_ptr) != dbval) {
 			WRT_REG_DWORD(ha->nxdb_wr_ptr, dbval);
 			wmb();
 		}
 	}
 	
 	if (vha->flags.process_response_queue &&
 	    rsp->ring_ptr->signature != RESPONSE_PROCESSED)
 		qla24xx_process_response_queue(vha, rsp);
 	spin_unlock_irqrestore(&ha->hardware_lock, flags);
 	return QLA_SUCCESS;
 queuing_error_fcp_cmnd:
 	dma_pool_free(ha->fcp_cmnd_dma_pool, ctx->fcp_cmnd, ctx->fcp_cmnd_dma);
 queuing_error:
 	if (tot_dsds)
 		scsi_dma_unmap(cmd);
 	if (sp->u.scmd.ctx) {
 		mempool_free(sp->u.scmd.ctx, ha->ctx_mempool);
 		sp->u.scmd.ctx = NULL;
 	}
 	spin_unlock_irqrestore(&ha->hardware_lock, flags);
 	return QLA_FUNCTION_FAILED;
 }",279,1889
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/free_bootmem-77/1540937361_2018-10-30_2013288f7238_xhci-dbc_xdbc_free_ring,8,8," static void __init xdbc_free_ring(struct xdbc_ring *ring)
 {
 	struct xdbc_segment *seg = ring->segment;
 	if (!seg)
 		return;
-	free_bootmem(seg->dma, PAGE_SIZE);
+	memblock_free(seg->dma, PAGE_SIZE);
 	ring->segment = NULL;
 }",9,54
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/free_bootmem-77/1540937361_2018-10-30_2013288f7238_tce_64_free_tce_table,9,9," void __init free_tce_table(void *tbl)
 {
 	unsigned int size;
 	if (!tbl)
 		return;
 	size = table_size_to_number_of_entries(specified_table_size);
 	size *= TCE_ENTRY_SIZE;
-	free_bootmem(__pa(tbl), size);
+	memblock_free(__pa(tbl), size);
 }",10,54
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/free_bootmem-77/1540937361_2018-10-30_2013288f7238_init_free_memmap,12,12," static inline void free_memmap(unsigned long start_pfn, unsigned long end_pfn) {
   struct page *start_pg, *end_pg;
   unsigned long pg, pgend;
   
   start_pg = pfn_to_page(start_pfn - 1) + 1;
   end_pg = pfn_to_page(end_pfn);
   
   pg = PAGE_ALIGN(__pa(start_pg));
   pgend = __pa(end_pg) & PAGE_MASK;
   
-  if (pg < pgend) free_bootmem(pg, pgend - pg);
+  if (pg < pgend) memblock_free(pg, pgend - pg);
 }",13,100
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_smp_64_pcpu_free_bootmem,1,1,"-static void __init pcpu_free_bootmem(void *ptr, size_t size) { free_bootmem(__pa(ptr), size); }
+static void __init pcpu_free_bootmem(void *ptr, size_t size) { memblock_free(__pa(ptr), size); }",2,52
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_setup_percpu_pcpu_fc_free,1,1,"-static void __init pcpu_fc_free(void *ptr, size_t size) { free_bootmem(__pa(ptr), size); }
+static void __init pcpu_fc_free(void *ptr, size_t size) { memblock_free(__pa(ptr), size); }",2,52
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_xhci-dbc_xdbc_init,35,35," static int __init xdbc_init(void) {
   unsigned long flags;
   void __iomem *base;
   int ret = 0;
   u32 offset;
   if (!(xdbc.flags & XDBC_FLAGS_INITIALIZED)) return 0;
   
   if (early_xdbc_console.index == -1 || (early_xdbc_console.flags & CON_BOOT)) {
     xdbc_trace(""hardware not used anymore\n"");
     goto free_and_quit;
   }
   base = ioremap_nocache(xdbc.xhci_start, xdbc.xhci_length);
   if (!base) {
     xdbc_trace(""failed to remap the io address\n"");
     ret = -ENOMEM;
     goto free_and_quit;
   }
   raw_spin_lock_irqsave(&xdbc.lock, flags);
   early_iounmap(xdbc.xhci_base, xdbc.xhci_length);
   xdbc.xhci_base = base;
   offset = xhci_find_next_ext_cap(xdbc.xhci_base, 0, XHCI_EXT_CAPS_DEBUG);
   xdbc.xdbc_reg = (struct xdbc_regs __iomem *)(xdbc.xhci_base + offset);
   raw_spin_unlock_irqrestore(&xdbc.lock, flags);
   kthread_run(xdbc_scrub_function, NULL, ""%s"", ""xdbc"");
   return 0;
 free_and_quit:
   xdbc_free_ring(&xdbc.evt_ring);
   xdbc_free_ring(&xdbc.out_ring);
   xdbc_free_ring(&xdbc.in_ring);
-  free_bootmem(xdbc.table_dma, PAGE_SIZE);
-  free_bootmem(xdbc.out_dma, PAGE_SIZE);
+  memblock_free(xdbc.table_dma, PAGE_SIZE);
+  memblock_free(xdbc.out_dma, PAGE_SIZE);
   writel(0, &xdbc.xdbc_reg->control);
   early_iounmap(xdbc.xhci_base, xdbc.xhci_length);
   return ret;
 }",37,283
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_setup_64_pcpu_fc_free,1,1,"-static void __init pcpu_fc_free(void *ptr, size_t size) { free_bootmem(__pa(ptr), size); }
+static void __init pcpu_fc_free(void *ptr, size_t size) { memblock_free(__pa(ptr), size); }",2,52
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_smu_smu_init,75,75," int __init smu_init(void) {
   struct device_node *np;
   const u32 *data;
   int ret = 0;
   np = of_find_node_by_type(NULL, ""smu"");
   if (np == NULL) return -ENODEV;
   printk(KERN_INFO ""SMU: Driver %s %s\n"", VERSION, AUTHOR);
   
   smu_cmdbuf_abs = memblock_alloc_base(4096, 4096, 0x80000000UL);
   if (smu_cmdbuf_abs == 0) {
     printk(KERN_ERR ""SMU: Command buffer allocation failed !\n"");
     ret = -EINVAL;
     goto fail_np;
   }
   smu = memblock_alloc(sizeof(struct smu_device), 0);
   spin_lock_init(&smu->lock);
   INIT_LIST_HEAD(&smu->cmd_list);
   INIT_LIST_HEAD(&smu->cmd_i2c_list);
   smu->of_node = np;
   smu->db_irq = 0;
   smu->msg_irq = 0;
   
   smu->cmd_buf_abs = (u32)smu_cmdbuf_abs;
   smu->cmd_buf = __va(smu_cmdbuf_abs);
   smu->db_node = of_find_node_by_name(NULL, ""smu-doorbell"");
   if (smu->db_node == NULL) {
     printk(KERN_ERR ""SMU: Can't find doorbell GPIO !\n"");
     ret = -ENXIO;
     goto fail_bootmem;
   }
   data = of_get_property(smu->db_node, ""reg"", NULL);
   if (data == NULL) {
     printk(KERN_ERR ""SMU: Can't find doorbell GPIO address !\n"");
     ret = -ENXIO;
     goto fail_db_node;
   }
   
   smu->doorbell = *data;
   if (smu->doorbell < 0x50) smu->doorbell += 0x50;
   
   do {
     smu->msg_node = of_find_node_by_name(NULL, ""smu-interrupt"");
     if (smu->msg_node == NULL) break;
     data = of_get_property(smu->msg_node, ""reg"", NULL);
     if (data == NULL) {
       of_node_put(smu->msg_node);
       smu->msg_node = NULL;
       break;
     }
     smu->msg = *data;
     if (smu->msg < 0x50) smu->msg += 0x50;
   } while (0);
   
   smu->db_buf = ioremap(0x8000860c, 0x1000);
   if (smu->db_buf == NULL) {
     printk(KERN_ERR ""SMU: Can't map doorbell buffer pointer !\n"");
     ret = -ENXIO;
     goto fail_msg_node;
   }
   
   smu->broken_nap = pmac_get_uninorth_variant() < 4;
   if (smu->broken_nap) printk(KERN_INFO ""SMU: using NAP mode workaround\n"");
   sys_ctrler = SYS_CTRLER_SMU;
   return 0;
 fail_msg_node:
   of_node_put(smu->msg_node);
 fail_db_node:
   of_node_put(smu->db_node);
 fail_bootmem:
-  free_bootmem(__pa(smu), sizeof(struct smu_device));
+  memblock_free(__pa(smu), sizeof(struct smu_device));
   smu = NULL;
 fail_np:
   of_node_put(np);
   return ret;
 }",76,491
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_mm_init_free_memmap,12,12," static inline void free_memmap(unsigned long start_pfn, unsigned long end_pfn) {
   struct page *start_pg, *end_pg;
   unsigned long pg, pgend;
   
   start_pg = pfn_to_page(start_pfn - 1) + 1;
   end_pg = pfn_to_page(end_pfn - 1) + 1;
   
   pg = (unsigned long)PAGE_ALIGN(__pa(start_pg));
   pgend = (unsigned long)__pa(end_pg) & PAGE_MASK;
   
-  if (pg < pgend) free_bootmem(pg, pgend - pg);
+  if (pg < pgend) memblock_free(pg, pgend - pg);
 }",13,112
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_p2m_free_p2m_page,7,7," static void __ref free_p2m_page(void *p) {
   if (unlikely(!slab_is_available())) {
-    free_bootmem((unsigned long)p, PAGE_SIZE);
+    memblock_free((unsigned long)p, PAGE_SIZE);
     return;
   }
   free_page((unsigned long)p);
 }",8,58
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_mem_mem_init,15,15," void __init mem_init(void) {
   
   memset(empty_zero_page, 0, PAGE_SIZE);
   
   brk_end = (unsigned long)UML_ROUND_UP(sbrk(0));
   map_memory(brk_end, __pa(brk_end), uml_reserved - brk_end, 1, 1, 0);
-  free_bootmem(__pa(brk_end), uml_reserved - brk_end);
+  memblock_free(__pa(brk_end), uml_reserved - brk_end);
   uml_reserved = brk_end;
   
   free_all_bootmem();
   max_low_pfn = totalram_pages;
   max_pfn = totalram_pages;
   mem_init_print_info(NULL);
   kmalloc_ok = 1;
 }",16,102
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_swiotlb-xen_xen_swiotlb_init,65,65," int __ref xen_swiotlb_init(int verbose, bool early) {
   unsigned long bytes, order;
   int rc = -ENOMEM;
   enum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;
   unsigned int repeat = 3;
   xen_io_tlb_nslabs = swiotlb_nr_tbl();
 retry:
   bytes = xen_set_nslabs(xen_io_tlb_nslabs);
   order = get_order(xen_io_tlb_nslabs << IO_TLB_SHIFT);
   
   if (early)
     xen_io_tlb_start = memblock_alloc(PAGE_ALIGN(bytes), PAGE_SIZE);
   else {
 #define SLABS_PER_PAGE (1 << (PAGE_SHIFT - IO_TLB_SHIFT))
 #define IO_TLB_MIN_SLABS ((1 << 20) >> IO_TLB_SHIFT)
     while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
       xen_io_tlb_start = (void *)xen_get_swiotlb_free_pages(order);
       if (xen_io_tlb_start) break;
       order--;
     }
     if (order != get_order(bytes)) {
       pr_warn(""Warning: only able to allocate %ld MB for software IO TLB\n"", (PAGE_SIZE << order) >> 20);
       xen_io_tlb_nslabs = SLABS_PER_PAGE << order;
       bytes = xen_io_tlb_nslabs << IO_TLB_SHIFT;
     }
   }
   if (!xen_io_tlb_start) {
     m_ret = XEN_SWIOTLB_ENOMEM;
     goto error;
   }
   xen_io_tlb_end = xen_io_tlb_start + bytes;
   
   rc = xen_swiotlb_fixup(xen_io_tlb_start, bytes, xen_io_tlb_nslabs);
   if (rc) {
     if (early)
-      free_bootmem(__pa(xen_io_tlb_start), PAGE_ALIGN(bytes));
+      memblock_free(__pa(xen_io_tlb_start), PAGE_ALIGN(bytes));
     else {
       free_pages((unsigned long)xen_io_tlb_start, order);
       xen_io_tlb_start = NULL;
     }
     m_ret = XEN_SWIOTLB_EFIXUP;
     goto error;
   }
   start_dma_addr = xen_virt_to_bus(xen_io_tlb_start);
   if (early) {
     if (swiotlb_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs, verbose)) panic(""Cannot allocate SWIOTLB buffer"");
     rc = 0;
   } else
     rc = swiotlb_late_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs);
   if (!rc) swiotlb_set_max_segment(PAGE_SIZE);
   return rc;
 error:
   if (repeat--) {
     xen_io_tlb_nslabs = max(1024UL, 
                             (xen_io_tlb_nslabs >> 1));
     pr_info(""Lowering to %luMB\n"", (xen_io_tlb_nslabs << IO_TLB_SHIFT) >> 20);
     goto retry;
   }
   pr_err(""%s (rc:%d)\n"", xen_swiotlb_error(m_ret), rc);
   if (early)
     panic(""%s (rc:%d)"", xen_swiotlb_error(m_ret), rc);
   else
     free_pages((unsigned long)xen_io_tlb_start, order);
   return rc;
 }",66,398
free_bootmem-77,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/free_bootmem-77/1540937361_2018-10-30_2013288f7238_xhci-dbc_early_xdbc_setup_hardware,18,18," int __init early_xdbc_setup_hardware(void) {
   int ret;
   if (!xdbc.xdbc_reg) return -ENODEV;
   xdbc_bios_handoff();
   raw_spin_lock_init(&xdbc.lock);
   ret = xdbc_early_setup();
   if (ret) {
     pr_notice(""failed to setup the connection to host\n"");
     xdbc_free_ring(&xdbc.evt_ring);
     xdbc_free_ring(&xdbc.out_ring);
     xdbc_free_ring(&xdbc.in_ring);
-    if (xdbc.table_dma) free_bootmem(xdbc.table_dma, PAGE_SIZE);
-    if (xdbc.out_dma) free_bootmem(xdbc.out_dma, PAGE_SIZE);
+    if (xdbc.table_dma) memblock_free(xdbc.table_dma, PAGE_SIZE);
+    if (xdbc.out_dma) memblock_free(xdbc.out_dma, PAGE_SIZE);
     xdbc.table_base = NULL;
     xdbc.out_buf = NULL;
   }
   return ret;
 }",20,161
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcaction/1534706529_2018-08-19_244cd96adb5f_en_tc_parse_tc_fdb_actions,102,101," static int parse_tc_fdb_actions(struct mlx5e_priv *priv, struct tcf_exts *exts,
 				struct mlx5e_tc_flow_parse_attr *parse_attr,
 				struct mlx5e_tc_flow *flow)
 {
 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 	struct ip_tunnel_info *info = NULL;
 	const struct tc_action *a;
-	LIST_HEAD(actions);
+	int i;
 	bool encap = false;
 	u32 action = 0;
 	int err;
 	if (!tcf_exts_has_actions(exts))
 		return -EINVAL;
 	attr->in_rep = rpriv->rep;
 	attr->in_mdev = priv->mdev;
-	tcf_exts_to_list(exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	tcf_exts_for_each_action(i, a, exts) {
 		if (is_tcf_gact_shot(a)) {
 			action |= MLX5_FLOW_CONTEXT_ACTION_DROP |
 				  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			continue;
 		}
 		if (is_tcf_pedit(a)) {
 			err = parse_tc_pedit_action(priv, a, MLX5_FLOW_NAMESPACE_FDB,
 						    parse_attr);
 			if (err)
 				return err;
 			action |= MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;
 			attr->mirror_count = attr->out_count;
 			continue;
 		}
 		if (is_tcf_csum(a)) {
 			if (csum_offload_supported(priv, action,
 						   tcf_csum_update_flags(a)))
 				continue;
 			return -EOPNOTSUPP;
 		}
 		if (is_tcf_mirred_egress_redirect(a) || is_tcf_mirred_egress_mirror(a)) {
 			struct mlx5e_priv *out_priv;
 			struct net_device *out_dev;
 			out_dev = tcf_mirred_dev(a);
 			if (attr->out_count >= MLX5_MAX_FLOW_FWD_VPORTS) {
 				pr_err(""can't support more than %d output ports, can't offload forwarding\n"",
 				       attr->out_count);
 				return -EOPNOTSUPP;
 			}
 			if (switchdev_port_same_parent_id(priv->netdev,
 							  out_dev) ||
 			    is_merged_eswitch_dev(priv, out_dev)) {
 				action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 					  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 				out_priv = netdev_priv(out_dev);
 				rpriv = out_priv->ppriv;
 				attr->out_rep[attr->out_count] = rpriv->rep;
 				attr->out_mdev[attr->out_count++] = out_priv->mdev;
 			} else if (encap) {
 				parse_attr->mirred_ifindex = out_dev->ifindex;
 				parse_attr->tun_info = *info;
 				attr->parse_attr = parse_attr;
 				action |= MLX5_FLOW_CONTEXT_ACTION_ENCAP |
 					  MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 					  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 				
 			} else {
 				pr_err(""devices %s %s not on same switch HW, can't offload forwarding\n"",
 				       priv->netdev->name, out_dev->name);
 				return -EINVAL;
 			}
 			continue;
 		}
 		if (is_tcf_tunnel_set(a)) {
 			info = tcf_tunnel_info(a);
 			if (info)
 				encap = true;
 			else
 				return -EOPNOTSUPP;
 			attr->mirror_count = attr->out_count;
 			continue;
 		}
 		if (is_tcf_vlan(a)) {
 			err = parse_tc_vlan_action(priv, a, attr, &action);
 			if (err)
 				return err;
 			attr->mirror_count = attr->out_count;
 			continue;
 		}
 		if (is_tcf_tunnel_release(a)) {
 			action |= MLX5_FLOW_CONTEXT_ACTION_DECAP;
 			continue;
 		}
 		return -EINVAL;
 	}
 	attr->action = action;
 	if (!actions_match_supported(priv, exts, parse_attr, flow))
 		return -EOPNOTSUPP;
 	if (attr->out_count > 1 && !mlx5_esw_has_fwd_fdb(priv->mdev)) {
 		netdev_warn_once(priv->netdev, ""current firmware doesn't support split rule for port mirroring\n"");
 		return -EOPNOTSUPP;
 	}
 	return 0;
 }",104,574
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcaction/1534706529_2018-08-19_244cd96adb5f_spectrum_flower_mlxsw_sp_flower_parse_actions,88,87," static int mlxsw_sp_flower_parse_actions(struct mlxsw_sp *mlxsw_sp,
 					 struct mlxsw_sp_acl_block *block,
 					 struct mlxsw_sp_acl_rule_info *rulei,
 					 struct tcf_exts *exts,
 					 struct netlink_ext_ack *extack)
 {
 	const struct tc_action *a;
-	LIST_HEAD(actions);
+	int i;
 	int err;
 	if (!tcf_exts_has_actions(exts))
 		return 0;
 	
 	err = mlxsw_sp_acl_rulei_act_count(mlxsw_sp, rulei, extack);
 	if (err)
 		return err;
-	tcf_exts_to_list(exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	tcf_exts_for_each_action(i, a, exts) {
 		if (is_tcf_gact_ok(a)) {
 			err = mlxsw_sp_acl_rulei_act_terminate(rulei);
 			if (err) {
 				NL_SET_ERR_MSG_MOD(extack, ""Cannot append terminate action"");
 				return err;
 			}
 		} else if (is_tcf_gact_shot(a)) {
 			err = mlxsw_sp_acl_rulei_act_drop(rulei);
 			if (err) {
 				NL_SET_ERR_MSG_MOD(extack, ""Cannot append drop action"");
 				return err;
 			}
 		} else if (is_tcf_gact_trap(a)) {
 			err = mlxsw_sp_acl_rulei_act_trap(rulei);
 			if (err) {
 				NL_SET_ERR_MSG_MOD(extack, ""Cannot append trap action"");
 				return err;
 			}
 		} else if (is_tcf_gact_goto_chain(a)) {
 			u32 chain_index = tcf_gact_goto_chain_index(a);
 			struct mlxsw_sp_acl_ruleset *ruleset;
 			u16 group_id;
 			ruleset = mlxsw_sp_acl_ruleset_lookup(mlxsw_sp, block,
 							      chain_index,
 							      MLXSW_SP_ACL_PROFILE_FLOWER);
 			if (IS_ERR(ruleset))
 				return PTR_ERR(ruleset);
 			group_id = mlxsw_sp_acl_ruleset_group_id(ruleset);
 			err = mlxsw_sp_acl_rulei_act_jump(rulei, group_id);
 			if (err) {
 				NL_SET_ERR_MSG_MOD(extack, ""Cannot append jump action"");
 				return err;
 			}
 		} else if (is_tcf_mirred_egress_redirect(a)) {
 			struct net_device *out_dev;
 			struct mlxsw_sp_fid *fid;
 			u16 fid_index;
 			fid = mlxsw_sp_acl_dummy_fid(mlxsw_sp);
 			fid_index = mlxsw_sp_fid_index(fid);
 			err = mlxsw_sp_acl_rulei_act_fid_set(mlxsw_sp, rulei,
 							     fid_index, extack);
 			if (err)
 				return err;
 			out_dev = tcf_mirred_dev(a);
 			err = mlxsw_sp_acl_rulei_act_fwd(mlxsw_sp, rulei,
 							 out_dev, extack);
 			if (err)
 				return err;
 		} else if (is_tcf_mirred_egress_mirror(a)) {
 			struct net_device *out_dev = tcf_mirred_dev(a);
 			err = mlxsw_sp_acl_rulei_act_mirror(mlxsw_sp, rulei,
 							    block, out_dev,
 							    extack);
 			if (err)
 				return err;
 		} else if (is_tcf_vlan(a)) {
 			u16 proto = be16_to_cpu(tcf_vlan_push_proto(a));
 			u32 action = tcf_vlan_action(a);
 			u8 prio = tcf_vlan_push_prio(a);
 			u16 vid = tcf_vlan_push_vid(a);
 			return mlxsw_sp_acl_rulei_act_vlan(mlxsw_sp, rulei,
 							   action, vid,
 							   proto, prio, extack);
 		} else {
 			NL_SET_ERR_MSG_MOD(extack, ""Unsupported action"");
 			dev_err(mlxsw_sp->bus_info->dev, ""Unsupported action\n"");
 			return -EOPNOTSUPP;
 		}
 	}
 	return 0;
 }",90,523
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/tcaction/1534706529_2018-08-19_244cd96adb5f_stmmac_tc_tc_fill_actions,30,29," static int tc_fill_actions(struct stmmac_tc_entry *entry,
 			   struct stmmac_tc_entry *frag,
 			   struct tc_cls_u32_offload *cls)
 {
 	struct stmmac_tc_entry *action_entry = entry;
 	const struct tc_action *act;
 	struct tcf_exts *exts;
-	LIST_HEAD(actions);
+	int i;
 	exts = cls->knode.exts;
 	if (!tcf_exts_has_actions(exts))
 		return -EINVAL;
 	if (frag)
 		action_entry = frag;
-	tcf_exts_to_list(exts, &actions);
-	list_for_each_entry(act, &actions, list) {
+	tcf_exts_for_each_action(i, act, exts) {
 		
 		if (is_tcf_gact_ok(act)) {
 			action_entry->val.af = 1;
 			break;
 		}
 		
 		if (is_tcf_gact_shot(act)) {
 			action_entry->val.rf = 1;
 			break;
 		}
 		
 		return -EINVAL;
 	}
 	return 0;
 }",32,154
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcaction/1534706529_2018-08-19_244cd96adb5f_en_tc_parse_tc_nic_actions,69,68," static int parse_tc_nic_actions(struct mlx5e_priv *priv, struct tcf_exts *exts,
 				struct mlx5e_tc_flow_parse_attr *parse_attr,
 				struct mlx5e_tc_flow *flow)
 {
 	struct mlx5_nic_flow_attr *attr = flow->nic_attr;
 	const struct tc_action *a;
-	LIST_HEAD(actions);
+	int i;
 	u32 action = 0;
 	int err;
 	if (!tcf_exts_has_actions(exts))
 		return -EINVAL;
 	attr->flow_tag = MLX5_FS_DEFAULT_FLOW_TAG;
-	tcf_exts_to_list(exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	tcf_exts_for_each_action(i, a, exts) {
 		if (is_tcf_gact_shot(a)) {
 			action |= MLX5_FLOW_CONTEXT_ACTION_DROP;
 			if (MLX5_CAP_FLOWTABLE(priv->mdev,
 					       flow_table_properties_nic_receive.flow_counter))
 				action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			continue;
 		}
 		if (is_tcf_pedit(a)) {
 			err = parse_tc_pedit_action(priv, a, MLX5_FLOW_NAMESPACE_KERNEL,
 						    parse_attr);
 			if (err)
 				return err;
 			action |= MLX5_FLOW_CONTEXT_ACTION_MOD_HDR |
 				  MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
 			continue;
 		}
 		if (is_tcf_csum(a)) {
 			if (csum_offload_supported(priv, action,
 						   tcf_csum_update_flags(a)))
 				continue;
 			return -EOPNOTSUPP;
 		}
 		if (is_tcf_mirred_egress_redirect(a)) {
 			struct net_device *peer_dev = tcf_mirred_dev(a);
 			if (priv->netdev->netdev_ops == peer_dev->netdev_ops &&
 			    same_hw_devs(priv, netdev_priv(peer_dev))) {
 				parse_attr->mirred_ifindex = peer_dev->ifindex;
 				flow->flags |= MLX5E_TC_FLOW_HAIRPIN;
 				action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 					  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			} else {
 				netdev_warn(priv->netdev, ""device %s not on same HW, can't offload\n"",
 					    peer_dev->name);
 				return -EINVAL;
 			}
 			continue;
 		}
 		if (is_tcf_skbedit_mark(a)) {
 			u32 mark = tcf_skbedit_mark(a);
 			if (mark & ~MLX5E_TC_FLOW_ID_MASK) {
 				netdev_warn(priv->netdev, ""Bad flow mark - only 16 bit is supported: 0x%x\n"",
 					    mark);
 				return -EINVAL;
 			}
 			attr->flow_tag = mark;
 			action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
 			continue;
 		}
 		return -EINVAL;
 	}
 	attr->action = action;
 	if (!actions_match_supported(priv, exts, parse_attr, flow))
 		return -EOPNOTSUPP;
 	return 0;
 }",71,382
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcaction/1534706529_2018-08-19_244cd96adb5f_en_tc_modify_header_match_supported,39,38," static bool modify_header_match_supported(struct mlx5_flow_spec *spec,
 					  struct tcf_exts *exts)
 {
 	const struct tc_action *a;
 	bool modify_ip_header;
-	LIST_HEAD(actions);
+	int i;
 	u8 htype, ip_proto;
 	void *headers_v;
 	u16 ethertype;
 	int nkeys, i;
 	headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value, outer_headers);
 	ethertype = MLX5_GET(fte_match_set_lyr_2_4, headers_v, ethertype);
 	
 	if (ethertype != ETH_P_IP && ethertype != ETH_P_IPV6)
 		goto out_ok;
 	modify_ip_header = false;
-	tcf_exts_to_list(exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	tcf_exts_for_each_action(i, a, exts) {
 		if (!is_tcf_pedit(a))
 			continue;
 		nkeys = tcf_pedit_nkeys(a);
 		for (i = 0; i < nkeys; i++) {
 			htype = tcf_pedit_htype(a, i);
 			if (htype == TCA_PEDIT_KEY_EX_HDR_TYPE_IP4 ||
 			    htype == TCA_PEDIT_KEY_EX_HDR_TYPE_IP6) {
 				modify_ip_header = true;
 				break;
 			}
 		}
 	}
 	ip_proto = MLX5_GET(fte_match_set_lyr_2_4, headers_v, ip_protocol);
 	if (modify_ip_header && ip_proto != IPPROTO_TCP &&
 	    ip_proto != IPPROTO_UDP && ip_proto != IPPROTO_ICMP) {
 		pr_info(""can't offload re-write of ip proto %d\n"", ip_proto);
 		return false;
 	}
 out_ok:
 	return true;
 }",41,231
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcaction/1534706529_2018-08-19_244cd96adb5f_cxgb4_tc_flower_cxgb4_validate_flow_actions,71,69," static int cxgb4_validate_flow_actions(struct net_device *dev,
 				       struct tc_cls_flower_offload *cls)
 {
 	const struct tc_action *a;
 	bool act_redir = false;
 	bool act_pedit = false;
 	bool act_vlan = false;
-	LIST_HEAD(actions);
-	tcf_exts_to_list(cls->exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	int i;tcf_exts_for_each_action(i, a, cls->exts) {
 		if (is_tcf_gact_ok(a)) {
 			
 		} else if (is_tcf_gact_shot(a)) {
 			
 		} else if (is_tcf_mirred_egress_redirect(a)) {
 			struct adapter *adap = netdev2adap(dev);
 			struct net_device *n_dev, *target_dev;
 			unsigned int i;
 			bool found = false;
 			target_dev = tcf_mirred_dev(a);
 			for_each_port(adap, i) {
 				n_dev = adap->port[i];
 				if (target_dev == n_dev) {
 					found = true;
 					break;
 				}
 			}
 			
 			if (!found) {
 				netdev_err(dev, ""%s: Out port invalid\n"",
 					   __func__);
 				return -EINVAL;
 			}
 			act_redir = true;
 		} else if (is_tcf_vlan(a)) {
 			u16 proto = be16_to_cpu(tcf_vlan_push_proto(a));
 			u32 vlan_action = tcf_vlan_action(a);
 			switch (vlan_action) {
 			case TCA_VLAN_ACT_POP:
 				break;
 			case TCA_VLAN_ACT_PUSH:
 			case TCA_VLAN_ACT_MODIFY:
 				if (proto != ETH_P_8021Q) {
 					netdev_err(dev, ""%s: Unsupported vlan proto\n"",
 						   __func__);
 					return -EOPNOTSUPP;
 				}
 				break;
 			default:
 				netdev_err(dev, ""%s: Unsupported vlan action\n"",
 					   __func__);
 				return -EOPNOTSUPP;
 			}
 			act_vlan = true;
 		} else if (is_tcf_pedit(a)) {
 			bool pedit_valid = valid_pedit_action(dev, a);
 			if (!pedit_valid)
 				return -EOPNOTSUPP;
 			act_pedit = true;
 		} else {
 			netdev_err(dev, ""%s: Unsupported action\n"", __func__);
 			return -EOPNOTSUPP;
 		}
 	}
 	if ((act_pedit || act_vlan) && !act_redir) {
 		netdev_err(dev, ""%s: pedit/vlan rewrite invalid without egress redirect\n"",
 			   __func__);
 		return -EINVAL;
 	}
 	return 0;
 }",72,382
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcaction/1534706529_2018-08-19_244cd96adb5f_cxgb4_tc_u32_fill_action_fields,48,47," static int fill_action_fields(struct adapter *adap,
 			      struct ch_filter_specification *fs,
 			      struct tc_cls_u32_offload *cls)
 {
 	unsigned int num_actions = 0;
 	const struct tc_action *a;
 	struct tcf_exts *exts;
-	LIST_HEAD(actions);
+	int i;
 	exts = cls->knode.exts;
 	if (!tcf_exts_has_actions(exts))
 		return -EINVAL;
-	tcf_exts_to_list(exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	tcf_exts_for_each_action(i, a, exts) {
 		
 		if (num_actions)
 			return -EINVAL;
 		
 		if (is_tcf_gact_shot(a)) {
 			fs->action = FILTER_DROP;
 			num_actions++;
 			continue;
 		}
 		
 		if (is_tcf_mirred_egress_redirect(a)) {
 			struct net_device *n_dev, *target_dev;
 			bool found = false;
 			unsigned int i;
 			target_dev = tcf_mirred_dev(a);
 			for_each_port(adap, i) {
 				n_dev = adap->port[i];
 				if (target_dev == n_dev) {
 					fs->action = FILTER_SWITCH;
 					fs->eport = i;
 					found = true;
 					break;
 				}
 			}
 			
 			if (!found)
 				return -EINVAL;
 			num_actions++;
 			continue;
 		}
 		
 		return -EINVAL;
 	}
 	return 0;
 }",50,226
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcaction/1534706529_2018-08-19_244cd96adb5f_cxgb4_tc_flower_cxgb4_process_flow_actions,53,51," static void cxgb4_process_flow_actions(struct net_device *in,
 				       struct tc_cls_flower_offload *cls,
 				       struct ch_filter_specification *fs)
 {
 	const struct tc_action *a;
-	LIST_HEAD(actions);
-	tcf_exts_to_list(cls->exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	int i;tcf_exts_for_each_action(i, a, cls->exts) {
 		if (is_tcf_gact_ok(a)) {
 			fs->action = FILTER_PASS;
 		} else if (is_tcf_gact_shot(a)) {
 			fs->action = FILTER_DROP;
 		} else if (is_tcf_mirred_egress_redirect(a)) {
 			struct net_device *out = tcf_mirred_dev(a);
 			struct port_info *pi = netdev_priv(out);
 			fs->action = FILTER_SWITCH;
 			fs->eport = pi->port_id;
 		} else if (is_tcf_vlan(a)) {
 			u32 vlan_action = tcf_vlan_action(a);
 			u8 prio = tcf_vlan_push_prio(a);
 			u16 vid = tcf_vlan_push_vid(a);
 			u16 vlan_tci = (prio << VLAN_PRIO_SHIFT) | vid;
 			switch (vlan_action) {
 			case TCA_VLAN_ACT_POP:
 				fs->newvlan |= VLAN_REMOVE;
 				break;
 			case TCA_VLAN_ACT_PUSH:
 				fs->newvlan |= VLAN_INSERT;
 				fs->vlan = vlan_tci;
 				break;
 			case TCA_VLAN_ACT_MODIFY:
 				fs->newvlan |= VLAN_REWRITE;
 				fs->vlan = vlan_tci;
 				break;
 			default:
 				break;
 			}
 		} else if (is_tcf_pedit(a)) {
 			u32 mask, val, offset;
 			int nkeys, i;
 			u8 htype;
 			nkeys = tcf_pedit_nkeys(a);
 			for (i = 0; i < nkeys; i++) {
 				htype = tcf_pedit_htype(a, i);
 				mask = tcf_pedit_mask(a, i);
 				val = tcf_pedit_val(a, i);
 				offset = tcf_pedit_offset(a, i);
 				process_pedit_field(fs, val, mask, offset,
 						    htype);
 			}
 		}
 	}
 }",54,343
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcaction/1534706529_2018-08-19_244cd96adb5f_qede_filter_qede_parse_actions,21,20," static int qede_parse_actions(struct qede_dev *edev,
 			      struct tcf_exts *exts)
 {
 	int rc = -EINVAL, num_act = 0;
 	const struct tc_action *a;
 	bool is_drop = false;
-	LIST_HEAD(actions);
+	int i;
 	if (!tcf_exts_has_actions(exts)) {
 		DP_NOTICE(edev, ""No tc actions received\n"");
 		return rc;
 	}
-	tcf_exts_to_list(exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	tcf_exts_for_each_action(i, a, exts) {
 		num_act++;
 		if (is_tcf_gact_shot(a))
 			is_drop = true;
 	}
 	if (num_act == 1 && is_drop)
 		return 0;
 	return rc;
 }",23,130
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcaction/1534706529_2018-08-19_244cd96adb5f_bnxt_tc_bnxt_tc_parse_actions,57,56," static int bnxt_tc_parse_actions(struct bnxt *bp,
 				 struct bnxt_tc_actions *actions,
 				 struct tcf_exts *tc_exts)
 {
 	const struct tc_action *tc_act;
-	LIST_HEAD(tc_actions);
+	int i;
 	int rc;
 	if (!tcf_exts_has_actions(tc_exts)) {
 		netdev_info(bp->dev, ""no actions"");
 		return -EINVAL;
 	}
-	tcf_exts_to_list(tc_exts, &tc_actions);
-	list_for_each_entry(tc_act, &tc_actions, list) {
+	tcf_exts_for_each_action(i, tc_act, tc_exts) {
 		
 		if (is_tcf_gact_shot(tc_act)) {
 			actions->flags |= BNXT_TC_ACTION_FLAG_DROP;
 			return 0; 
 		}
 		
 		if (is_tcf_mirred_egress_redirect(tc_act)) {
 			rc = bnxt_tc_parse_redir(bp, actions, tc_act);
 			if (rc)
 				return rc;
 			continue;
 		}
 		
 		if (is_tcf_vlan(tc_act)) {
 			bnxt_tc_parse_vlan(bp, actions, tc_act);
 			continue;
 		}
 		
 		if (is_tcf_tunnel_set(tc_act)) {
 			rc = bnxt_tc_parse_tunnel_set(bp, actions, tc_act);
 			if (rc)
 				return rc;
 			continue;
 		}
 		
 		if (is_tcf_tunnel_release(tc_act)) {
 			actions->flags |= BNXT_TC_ACTION_FLAG_TUNNEL_DECAP;
 			continue;
 		}
 	}
 	if (actions->flags & BNXT_TC_ACTION_FLAG_FWD) {
 		if (actions->flags & BNXT_TC_ACTION_FLAG_TUNNEL_ENCAP) {
 			
 			actions->dst_fid = bp->pf.fw_fid;
 		} else {
 			
 			actions->dst_fid =
 				bnxt_flow_get_dst_fid(bp, actions->dst_dev);
 			if (actions->dst_fid == BNXT_FID_INVALID)
 				return -EINVAL;
 		}
 	}
 	return 0;
 }",59,271
tcaction,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/tcaction/1534706529_2018-08-19_244cd96adb5f_action_nfp_flower_compile_action,32,31," int nfp_flower_compile_action(struct nfp_app *app,
 			      struct tc_cls_flower_offload *flow,
 			      struct net_device *netdev,
 			      struct nfp_fl_payload *nfp_flow)
 {
 	int act_len, act_cnt, err, tun_out_cnt, out_cnt;
 	enum nfp_flower_tun_type tun_type;
 	const struct tc_action *a;
 	u32 csum_updated = 0;
-	LIST_HEAD(actions);
+	int i;
 	memset(nfp_flow->action_data, 0, NFP_FL_MAX_A_SIZ);
 	nfp_flow->meta.act_len = 0;
 	tun_type = NFP_FL_TUNNEL_NONE;
 	act_len = 0;
 	act_cnt = 0;
 	tun_out_cnt = 0;
 	out_cnt = 0;
-	tcf_exts_to_list(flow->exts, &actions);
-	list_for_each_entry(a, &actions, list) {
+	tcf_exts_for_each_action(i, a, flow->exts) {
 		err = nfp_flower_loop_action(app, a, flow, nfp_flow, &act_len,
 					     netdev, &tun_type, &tun_out_cnt,
 					     &out_cnt, &csum_updated);
 		if (err)
 			return err;
 		act_cnt++;
 	}
 	
 	if (act_cnt > 1)
 		nfp_flow->meta.shortcut = cpu_to_be32(NFP_FL_SC_ACT_NULL);
 	nfp_flow->meta.act_len = act_len;
 	return 0;
 }",34,204
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_analysis_ccw,52,53," static struct dasd_ccw_req *
 dasd_eckd_analysis_ccw(struct dasd_device *device)
 {
 	struct dasd_eckd_private *private = device->private;
 	struct eckd_count *count_data;
 	struct LO_eckd_data *LO_data;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	int cplength, datasize;
 	int i;
 	cplength = 8;
 	datasize = sizeof(struct DE_eckd_data) + 2*sizeof(struct LO_eckd_data);
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize, device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize,
+				   device, NULL);
 	if (IS_ERR(cqr))
 		return cqr;
 	ccw = cqr->cpaddr;
 	
 	define_extent(ccw++, cqr->data, 0, 2,
 		      DASD_ECKD_CCW_READ_COUNT, device, 0);
 	LO_data = cqr->data + sizeof(struct DE_eckd_data);
 	
 	ccw[-1].flags |= CCW_FLAG_CC;
 	locate_record(ccw++, LO_data++, 0, 0, 4,
 		      DASD_ECKD_CCW_READ_COUNT, device, 0);
 	count_data = private->count_area;
 	for (i = 0; i < 4; i++) {
 		ccw[-1].flags |= CCW_FLAG_CC;
 		ccw->cmd_code = DASD_ECKD_CCW_READ_COUNT;
 		ccw->flags = 0;
 		ccw->count = 8;
 		ccw->cda = (__u32)(addr_t) count_data;
 		ccw++;
 		count_data++;
 	}
 	
 	ccw[-1].flags |= CCW_FLAG_CC;
 	locate_record(ccw++, LO_data++, 2, 0, 1,
 		      DASD_ECKD_CCW_READ_COUNT, device, 0);
 	
 	ccw[-1].flags |= CCW_FLAG_CC;
 	ccw->cmd_code = DASD_ECKD_CCW_READ_COUNT;
 	ccw->flags = 0;
 	ccw->count = 8;
 	ccw->cda = (__u32)(addr_t) count_data;
 	cqr->block = NULL;
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->retries = 255;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",54,355
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_steal_lock,44,44," static int
 dasd_eckd_steal_lock(struct dasd_device *device)
 {
 	struct dasd_ccw_req *cqr;
 	int rc;
 	struct ccw1 *ccw;
 	int useglobal;
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 	useglobal = 0;
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1, 32, device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1, 32, device, NULL);
 	if (IS_ERR(cqr)) {
 		mutex_lock(&dasd_reserve_mutex);
 		useglobal = 1;
 		cqr = &dasd_reserve_req->cqr;
 		memset(cqr, 0, sizeof(*cqr));
 		memset(&dasd_reserve_req->ccw, 0,
 		       sizeof(dasd_reserve_req->ccw));
 		cqr->cpaddr = &dasd_reserve_req->ccw;
 		cqr->data = &dasd_reserve_req->data;
 		cqr->magic = DASD_ECKD_MAGIC;
 	}
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_SLCK;
 	ccw->flags |= CCW_FLAG_SLI;
 	ccw->count = 32;
 	ccw->cda = (__u32)(addr_t) cqr->data;
 	cqr->startdev = device;
 	cqr->memdev = device;
 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
 	set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->retries = 2;	
 	cqr->expires = 2 * HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	rc = dasd_sleep_on_immediatly(cqr);
 	if (!rc)
 		set_bit(DASD_FLAG_IS_RESERVED, &device->flags);
 	if (useglobal)
 		mutex_unlock(&dasd_reserve_mutex);
 	else
 		dasd_sfree_request(cqr, cqr->memdev);
 	return rc;
 }",45,296
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_diag_dasd_diag_build_cp,70,71," static struct dasd_ccw_req *dasd_diag_build_cp(struct dasd_device *memdev,
 					       struct dasd_block *block,
 					       struct request *req)
 {
 	struct dasd_ccw_req *cqr;
 	struct dasd_diag_req *dreq;
 	struct dasd_diag_bio *dbio;
 	struct req_iterator iter;
 	struct bio_vec bv;
 	char *dst;
 	unsigned int count, datasize;
 	sector_t recid, first_rec, last_rec;
 	unsigned int blksize, off;
 	unsigned char rw_cmd;
 	if (rq_data_dir(req) == READ)
 		rw_cmd = MDSK_READ_REQ;
 	else if (rq_data_dir(req) == WRITE)
 		rw_cmd = MDSK_WRITE_REQ;
 	else
 		return ERR_PTR(-EINVAL);
 	blksize = block->bp_block;
 	
 	first_rec = blk_rq_pos(req) >> block->s2b_shift;
 	last_rec =
 		(blk_rq_pos(req) + blk_rq_sectors(req) - 1) >> block->s2b_shift;
 	
 	count = 0;
 	rq_for_each_segment(bv, req, iter) {
 		if (bv.bv_len & (blksize - 1))
 			
 			return ERR_PTR(-EINVAL);
 		count += bv.bv_len >> (block->s2b_shift + 9);
 	}
 	
 	if (count != last_rec - first_rec + 1)
 		return ERR_PTR(-EINVAL);
 	
 	datasize = sizeof(struct dasd_diag_req) +
 		count*sizeof(struct dasd_diag_bio);
-	cqr = dasd_smalloc_request(DASD_DIAG_MAGIC, 0, datasize, memdev);
+	cqr = dasd_smalloc_request(DASD_DIAG_MAGIC, 0, datasize, memdev,
+				   blk_mq_rq_to_pdu(req));
 	if (IS_ERR(cqr))
 		return cqr;
 	dreq = (struct dasd_diag_req *) cqr->data;
 	dreq->block_count = count;
 	dbio = dreq->bio;
 	recid = first_rec;
 	rq_for_each_segment(bv, req, iter) {
 		dst = page_address(bv.bv_page) + bv.bv_offset;
 		for (off = 0; off < bv.bv_len; off += blksize) {
 			memset(dbio, 0, sizeof (struct dasd_diag_bio));
 			dbio->type = rw_cmd;
 			dbio->block_number = recid + 1;
 			dbio->buffer = dst;
 			dbio++;
 			dst += blksize;
 			recid++;
 		}
 	}
 	cqr->retries = memdev->default_retries;
 	cqr->buildclk = get_tod_clock();
 	if (blk_noretry_request(req) ||
 	    block->base->features & DASD_FEATURE_FAILFAST)
 		set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->startdev = memdev;
 	cqr->memdev = memdev;
 	cqr->block = block;
 	cqr->expires = memdev->default_expires * HZ;
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",72,460
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_fba_dasd_fba_build_cp_discard,88,89," static struct dasd_ccw_req *dasd_fba_build_cp_discard(
 						struct dasd_device *memdev,
 						struct dasd_block *block,
 						struct request *req)
 {
 	struct LO_fba_data *LO_data;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	sector_t wz_stop = 0, d_stop = 0;
 	sector_t first_rec, last_rec;
 	unsigned int blksize = block->bp_block;
 	unsigned int blocks_per_page;
 	int wz_count = 0;
 	int d_count = 0;
 	int cur_pos = 0; 
 	int count = 0;
 	int cplength;
 	int datasize;
 	int nr_ccws;
 	first_rec = blk_rq_pos(req) >> block->s2b_shift;
 	last_rec =
 		(blk_rq_pos(req) + blk_rq_sectors(req) - 1) >> block->s2b_shift;
 	count = last_rec - first_rec + 1;
 	blocks_per_page = BLOCKS_PER_PAGE(blksize);
 	nr_ccws = count_ccws(first_rec, last_rec, blocks_per_page);
 	
 	cplength = 1 + 2 * nr_ccws;
 	datasize = sizeof(struct DE_fba_data) +
 		nr_ccws * (sizeof(struct LO_fba_data) + sizeof(struct ccw1));
-	cqr = dasd_smalloc_request(DASD_FBA_MAGIC, cplength, datasize, memdev);
+	cqr = dasd_smalloc_request(DASD_FBA_MAGIC, cplength, datasize, memdev,
+				   blk_mq_rq_to_pdu(req));
 	if (IS_ERR(cqr))
 		return cqr;
 	ccw = cqr->cpaddr;
 	define_extent(ccw++, cqr->data, WRITE, blksize, first_rec, count);
 	LO_data = cqr->data + sizeof(struct DE_fba_data);
 	
 	if (first_rec % blocks_per_page != 0) {
 		wz_stop = first_rec + blocks_per_page -
 			(first_rec % blocks_per_page) - 1;
 		if (wz_stop > last_rec)
 			wz_stop = last_rec;
 		wz_count = wz_stop - first_rec + 1;
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, LO_data++, WRITE, cur_pos, wz_count);
 		ccw[-1].flags |= CCW_FLAG_CC;
 		ccw_write_zero(ccw++, wz_count * blksize);
 		cur_pos = wz_count;
 	}
 	
 	if (last_rec - (first_rec + cur_pos) + 1 >= blocks_per_page) {
 		
 		if ((last_rec - blocks_per_page + 1) % blocks_per_page != 0)
 			d_stop = last_rec - ((last_rec - blocks_per_page + 1) %
 					     blocks_per_page);
 		else
 			d_stop = last_rec;
 		d_count = d_stop - (first_rec + cur_pos) + 1;
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, LO_data++, WRITE, cur_pos, d_count);
 		ccw[-1].flags |= CCW_FLAG_CC;
 		ccw_write_no_data(ccw++);
 		cur_pos += d_count;
 	}
 	
 	if (cur_pos == 0 || first_rec + cur_pos - 1 < last_rec) {
 		if (d_stop != 0)
 			wz_count = last_rec - d_stop;
 		else if (wz_stop != 0)
 			wz_count = last_rec - wz_stop;
 		else
 			wz_count = count;
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, LO_data++, WRITE, cur_pos, wz_count);
 		ccw[-1].flags |= CCW_FLAG_CC;
 		ccw_write_zero(ccw++, wz_count * blksize);
 	}
 	if (blk_noretry_request(req) ||
 	    block->base->features & DASD_FEATURE_FAILFAST)
 		set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->startdev = memdev;
 	cqr->memdev = memdev;
 	cqr->block = block;
 	cqr->expires = memdev->default_expires * HZ;	
 	cqr->retries = memdev->default_retries;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",90,624
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_symm_io,86,86," static int dasd_symm_io(struct dasd_device *device, void __user *argp)
 {
 	struct dasd_symmio_parms usrparm;
 	char *psf_data, *rssd_result;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	char psf0, psf1;
 	int rc;
 	if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RAWIO))
 		return -EACCES;
 	psf0 = psf1 = 0;
 	
 	rc = -EFAULT;
 	if (copy_from_user(&usrparm, argp, sizeof(usrparm)))
 		goto out;
 	if (is_compat_task()) {
 		
 		rc = -EINVAL;
 		if ((usrparm.psf_data >> 32) != 0)
 			goto out;
 		if ((usrparm.rssd_result >> 32) != 0)
 			goto out;
 		usrparm.psf_data &= 0x7fffffffULL;
 		usrparm.rssd_result &= 0x7fffffffULL;
 	}
 	
 	psf_data = kzalloc(usrparm.psf_data_len, GFP_KERNEL | GFP_DMA);
 	rssd_result = kzalloc(usrparm.rssd_result_len, GFP_KERNEL | GFP_DMA);
 	if (!psf_data || !rssd_result) {
 		rc = -ENOMEM;
 		goto out_free;
 	}
 	
 	rc = -EFAULT;
 	if (copy_from_user(psf_data,
 			   (void __user *)(unsigned long) usrparm.psf_data,
 			   usrparm.psf_data_len))
 		goto out_free;
 	psf0 = psf_data[0];
 	psf1 = psf_data[1];
 	
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 2 , 0, device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 2, 0, device, NULL);
 	if (IS_ERR(cqr)) {
 		DBF_DEV_EVENT(DBF_WARNING, device, ""%s"",
 			""Could not allocate initialization request"");
 		rc = PTR_ERR(cqr);
 		goto out_free;
 	}
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->retries = 3;
 	cqr->expires = 10 * HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	
 	ccw = cqr->cpaddr;
 	
 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
 	ccw->count = usrparm.psf_data_len;
 	ccw->flags |= CCW_FLAG_CC;
 	ccw->cda = (__u32)(addr_t) psf_data;
 	ccw++;
 	
 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
 	ccw->count = usrparm.rssd_result_len;
 	ccw->flags = CCW_FLAG_SLI ;
 	ccw->cda = (__u32)(addr_t) rssd_result;
 	rc = dasd_sleep_on(cqr);
 	if (rc)
 		goto out_sfree;
 	rc = -EFAULT;
 	if (copy_to_user((void __user *)(unsigned long) usrparm.rssd_result,
 			   rssd_result, usrparm.rssd_result_len))
 		goto out_sfree;
 	rc = 0;
 out_sfree:
 	dasd_sfree_request(cqr, cqr->memdev);
 out_free:
 	kfree(rssd_result);
 	kfree(psf_data);
 out:
 	DBF_DEV_EVENT(DBF_WARNING, device,
 		      ""Symmetrix ioctl (0x%02x 0x%02x): rc=%d"",
 		      (int) psf0, (int) psf1, rc);
 	return rc;
 }",87,515
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/inference/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_fba_dasd_fba_build_cp_regular,121,122," static struct dasd_ccw_req *dasd_fba_build_cp_regular(
 						struct dasd_device *memdev,
 						struct dasd_block *block,
 						struct request *req)
 {
 	struct dasd_fba_private *private = block->base->private;
 	unsigned long *idaws;
 	struct LO_fba_data *LO_data;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	struct req_iterator iter;
 	struct bio_vec bv;
 	char *dst;
 	int count, cidaw, cplength, datasize;
 	sector_t recid, first_rec, last_rec;
 	unsigned int blksize, off;
 	unsigned char cmd;
 	if (rq_data_dir(req) == READ) {
 		cmd = DASD_FBA_CCW_READ;
 	} else if (rq_data_dir(req) == WRITE) {
 		cmd = DASD_FBA_CCW_WRITE;
 	} else
 		return ERR_PTR(-EINVAL);
 	blksize = block->bp_block;
 	
 	first_rec = blk_rq_pos(req) >> block->s2b_shift;
 	last_rec =
 		(blk_rq_pos(req) + blk_rq_sectors(req) - 1) >> block->s2b_shift;
 	
 	count = 0;
 	cidaw = 0;
 	rq_for_each_segment(bv, req, iter) {
 		if (bv.bv_len & (blksize - 1))
 			
 			return ERR_PTR(-EINVAL);
 		count += bv.bv_len >> (block->s2b_shift + 9);
 		if (idal_is_needed (page_address(bv.bv_page), bv.bv_len))
 			cidaw += bv.bv_len / blksize;
 	}
 	
 	if (count != last_rec - first_rec + 1)
 		return ERR_PTR(-EINVAL);
 	
 	cplength = 2 + count;
 	
 	datasize = sizeof(struct DE_fba_data) + sizeof(struct LO_fba_data) +
 		cidaw * sizeof(unsigned long);
 	
 	if (private->rdc_data.mode.bits.data_chain == 0) {
 		cplength += count - 1;
 		datasize += (count - 1)*sizeof(struct LO_fba_data);
 	}
 	
-	cqr = dasd_smalloc_request(DASD_FBA_MAGIC, cplength, datasize, memdev);
+	cqr = dasd_smalloc_request(DASD_FBA_MAGIC, cplength, datasize, memdev,
+				   blk_mq_rq_to_pdu(req));
 	if (IS_ERR(cqr))
 		return cqr;
 	ccw = cqr->cpaddr;
 	
 	define_extent(ccw++, cqr->data, rq_data_dir(req),
 		      block->bp_block, blk_rq_pos(req), blk_rq_sectors(req));
 	
 	idaws = (unsigned long *) (cqr->data + sizeof(struct DE_fba_data));
 	LO_data = (struct LO_fba_data *) (idaws + cidaw);
 	
 	if (private->rdc_data.mode.bits.data_chain != 0) {
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, LO_data++, rq_data_dir(req), 0, count);
 	}
 	recid = first_rec;
 	rq_for_each_segment(bv, req, iter) {
 		dst = page_address(bv.bv_page) + bv.bv_offset;
 		if (dasd_page_cache) {
 			char *copy = kmem_cache_alloc(dasd_page_cache,
 						      GFP_DMA | __GFP_NOWARN);
 			if (copy && rq_data_dir(req) == WRITE)
 				memcpy(copy + bv.bv_offset, dst, bv.bv_len);
 			if (copy)
 				dst = copy + bv.bv_offset;
 		}
 		for (off = 0; off < bv.bv_len; off += blksize) {
 			
 			if (private->rdc_data.mode.bits.data_chain == 0) {
 				ccw[-1].flags |= CCW_FLAG_CC;
 				locate_record(ccw, LO_data++,
 					      rq_data_dir(req),
 					      recid - first_rec, 1);
 				ccw->flags = CCW_FLAG_CC;
 				ccw++;
 			} else {
 				if (recid > first_rec)
 					ccw[-1].flags |= CCW_FLAG_DC;
 				else
 					ccw[-1].flags |= CCW_FLAG_CC;
 			}
 			ccw->cmd_code = cmd;
 			ccw->count = block->bp_block;
 			if (idal_is_needed(dst, blksize)) {
 				ccw->cda = (__u32)(addr_t) idaws;
 				ccw->flags = CCW_FLAG_IDA;
 				idaws = idal_create_words(idaws, dst, blksize);
 			} else {
 				ccw->cda = (__u32)(addr_t) dst;
 				ccw->flags = 0;
 			}
 			ccw++;
 			dst += blksize;
 			recid++;
 		}
 	}
 	if (blk_noretry_request(req) ||
 	    block->base->features & DASD_FEATURE_FAILFAST)
 		set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->startdev = memdev;
 	cqr->memdev = memdev;
 	cqr->block = block;
 	cqr->expires = memdev->default_expires * HZ;	
 	cqr->retries = memdev->default_retries;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",123,826
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_reserve,44,44," static int
 dasd_eckd_reserve(struct dasd_device *device)
 {
 	struct dasd_ccw_req *cqr;
 	int rc;
 	struct ccw1 *ccw;
 	int useglobal;
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 	useglobal = 0;
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1, 32, device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1, 32, device, NULL);
 	if (IS_ERR(cqr)) {
 		mutex_lock(&dasd_reserve_mutex);
 		useglobal = 1;
 		cqr = &dasd_reserve_req->cqr;
 		memset(cqr, 0, sizeof(*cqr));
 		memset(&dasd_reserve_req->ccw, 0,
 		       sizeof(dasd_reserve_req->ccw));
 		cqr->cpaddr = &dasd_reserve_req->ccw;
 		cqr->data = &dasd_reserve_req->data;
 		cqr->magic = DASD_ECKD_MAGIC;
 	}
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_RESERVE;
 	ccw->flags |= CCW_FLAG_SLI;
 	ccw->count = 32;
 	ccw->cda = (__u32)(addr_t) cqr->data;
 	cqr->startdev = device;
 	cqr->memdev = device;
 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
 	set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->retries = 2;	
 	cqr->expires = 2 * HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	rc = dasd_sleep_on_immediatly(cqr);
 	if (!rc)
 		set_bit(DASD_FLAG_IS_RESERVED, &device->flags);
 	if (useglobal)
 		mutex_unlock(&dasd_reserve_mutex);
 	else
 		dasd_sfree_request(cqr, cqr->memdev);
 	return rc;
 }",45,296
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_read_features,55,54," static int dasd_eckd_read_features(struct dasd_device *device)
 {
 	struct dasd_eckd_private *private = device->private;
 	struct dasd_psf_prssd_data *prssdp;
 	struct dasd_rssd_features *features;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	int rc;
 	memset(&private->features, 0, sizeof(struct dasd_rssd_features));
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 	+ 1 ,
-				   (sizeof(struct dasd_psf_prssd_data) +
-				    sizeof(struct dasd_rssd_features)),
-				   device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 + 1,
+				   (sizeof(struct dasd_psf_prssd_data) + sizeof(struct dasd_rssd_features)),
+				   device, NULL);
 	if (IS_ERR(cqr)) {
 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, ""%s"", ""Could not ""
 				""allocate initialization request"");
 		return PTR_ERR(cqr);
 	}
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->block = NULL;
 	cqr->retries = 256;
 	cqr->expires = 10 * HZ;
 	
 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
 	prssdp->order = PSF_ORDER_PRSSD;
 	prssdp->suborder = 0x41;	
 	
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
 	ccw->count = sizeof(struct dasd_psf_prssd_data);
 	ccw->flags |= CCW_FLAG_CC;
 	ccw->cda = (__u32)(addr_t) prssdp;
 	
 	features = (struct dasd_rssd_features *) (prssdp + 1);
 	memset(features, 0, sizeof(struct dasd_rssd_features));
 	ccw++;
 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
 	ccw->count = sizeof(struct dasd_rssd_features);
 	ccw->cda = (__u32)(addr_t) features;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	rc = dasd_sleep_on(cqr);
 	if (rc == 0) {
 		prssdp = (struct dasd_psf_prssd_data *) cqr->data;
 		features = (struct dasd_rssd_features *) (prssdp + 1);
 		memcpy(&private->features, features,
 		       sizeof(struct dasd_rssd_features));
 	} else
 		dev_warn(&device->cdev->dev, ""Reading device feature codes""
 			 "" failed with rc=%d\n"", rc);
 	dasd_sfree_request(cqr, cqr->memdev);
 	return rc;
 }",58,424
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_read_message_buffer,68,67," static int dasd_eckd_read_message_buffer(struct dasd_device *device,
 					 struct dasd_rssd_messages *messages,
 					 __u8 lpum)
 {
 	struct dasd_rssd_messages *message_buf;
 	struct dasd_psf_prssd_data *prssdp;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	int rc;
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 	+ 1 ,
-				   (sizeof(struct dasd_psf_prssd_data) +
-				    sizeof(struct dasd_rssd_messages)),
-				   device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 + 1,
+				   (sizeof(struct dasd_psf_prssd_data) + sizeof(struct dasd_rssd_messages)),
+				   device, NULL);
 	if (IS_ERR(cqr)) {
 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, ""%s"",
 				""Could not allocate read message buffer request"");
 		return PTR_ERR(cqr);
 	}
 	cqr->lpm = lpum;
 retry:
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->block = NULL;
 	cqr->expires = 10 * HZ;
 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
 	
 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
 	cqr->retries = 256;
 	
 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
 	prssdp->order = PSF_ORDER_PRSSD;
 	prssdp->suborder = 0x03;	
 	
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
 	ccw->count = sizeof(struct dasd_psf_prssd_data);
 	ccw->flags |= CCW_FLAG_CC;
 	ccw->flags |= CCW_FLAG_SLI;
 	ccw->cda = (__u32)(addr_t) prssdp;
 	
 	message_buf = (struct dasd_rssd_messages *) (prssdp + 1);
 	memset(message_buf, 0, sizeof(struct dasd_rssd_messages));
 	ccw++;
 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
 	ccw->count = sizeof(struct dasd_rssd_messages);
 	ccw->flags |= CCW_FLAG_SLI;
 	ccw->cda = (__u32)(addr_t) message_buf;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	rc = dasd_sleep_on_immediatly(cqr);
 	if (rc == 0) {
 		prssdp = (struct dasd_psf_prssd_data *) cqr->data;
 		message_buf = (struct dasd_rssd_messages *)
 			(prssdp + 1);
 		memcpy(messages, message_buf,
 		       sizeof(struct dasd_rssd_messages));
 	} else if (cqr->lpm) {
 		
 		cqr->lpm = 0;
 		goto retry;
 	} else
 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
 				""Reading messages failed with rc=%d\n""
 				, rc);
 	dasd_sfree_request(cqr, cqr->memdev);
 	return rc;
 }",71,455
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_read_conf_lpm,44,42," static int dasd_eckd_read_conf_lpm(struct dasd_device *device,
 				   void **rcd_buffer,
 				   int *rcd_buffer_size, __u8 lpm)
 {
 	struct ciw *ciw;
 	char *rcd_buf = NULL;
 	int ret;
 	struct dasd_ccw_req *cqr;
 	
 	ciw = ccw_device_get_ciw(device->cdev, CIW_TYPE_RCD);
 	if (!ciw || ciw->cmd != DASD_ECKD_CCW_RCD) {
 		ret = -EOPNOTSUPP;
 		goto out_error;
 	}
 	rcd_buf = kzalloc(DASD_ECKD_RCD_DATA_SIZE, GFP_KERNEL | GFP_DMA);
 	if (!rcd_buf) {
 		ret = -ENOMEM;
 		goto out_error;
 	}
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 ,
-				   0, 
-				   device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1, 0, device, NULL);
 	if (IS_ERR(cqr)) {
 		DBF_DEV_EVENT(DBF_WARNING, device, ""%s"",
 			      ""Could not allocate RCD request"");
 		ret = -ENOMEM;
 		goto out_error;
 	}
 	dasd_eckd_fill_rcd_cqr(device, cqr, rcd_buf, lpm);
 	cqr->callback = read_conf_cb;
 	ret = dasd_sleep_on(cqr);
 	
 	dasd_sfree_request(cqr, cqr->memdev);
 	if (ret)
 		goto out_error;
 	*rcd_buffer_size = DASD_ECKD_RCD_DATA_SIZE;
 	*rcd_buffer = rcd_buf;
 	return 0;
 out_error:
 	kfree(rcd_buf);
 	*rcd_buffer = NULL;
 	*rcd_buffer_size = 0;
 	return ret;
 }",45,237
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_build_cp_cmd_single,176,176," static struct dasd_ccw_req *dasd_eckd_build_cp_cmd_single(
 					       struct dasd_device *startdev,
 					       struct dasd_block *block,
 					       struct request *req,
 					       sector_t first_rec,
 					       sector_t last_rec,
 					       sector_t first_trk,
 					       sector_t last_trk,
 					       unsigned int first_offs,
 					       unsigned int last_offs,
 					       unsigned int blk_per_trk,
 					       unsigned int blksize)
 {
 	struct dasd_eckd_private *private;
 	unsigned long *idaws;
 	struct LO_eckd_data *LO_data;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	struct req_iterator iter;
 	struct bio_vec bv;
 	char *dst;
 	unsigned int off;
 	int count, cidaw, cplength, datasize;
 	sector_t recid;
 	unsigned char cmd, rcmd;
 	int use_prefix;
 	struct dasd_device *basedev;
 	basedev = block->base;
 	private = basedev->private;
 	if (rq_data_dir(req) == READ)
 		cmd = DASD_ECKD_CCW_READ_MT;
 	else if (rq_data_dir(req) == WRITE)
 		cmd = DASD_ECKD_CCW_WRITE_MT;
 	else
 		return ERR_PTR(-EINVAL);
 	
 	count = 0;
 	cidaw = 0;
 	rq_for_each_segment(bv, req, iter) {
 		if (bv.bv_len & (blksize - 1))
 			
 			return ERR_PTR(-EINVAL);
 		count += bv.bv_len >> (block->s2b_shift + 9);
 		if (idal_is_needed (page_address(bv.bv_page), bv.bv_len))
 			cidaw += bv.bv_len >> (block->s2b_shift + 9);
 	}
 	
 	if (count != last_rec - first_rec + 1)
 		return ERR_PTR(-EINVAL);
 	
 	use_prefix = private->features.feature[8] & 0x01;
 	if (use_prefix) {
 		
 		cplength = 2 + count;
 		
 		datasize = sizeof(struct PFX_eckd_data) +
 			sizeof(struct LO_eckd_data) +
 			cidaw * sizeof(unsigned long);
 	} else {
 		
 		cplength = 2 + count;
 		
 		datasize = sizeof(struct DE_eckd_data) +
 			sizeof(struct LO_eckd_data) +
 			cidaw * sizeof(unsigned long);
 	}
 	
 	if (private->uses_cdl && first_rec < 2*blk_per_trk) {
 		if (last_rec >= 2*blk_per_trk)
 			count = 2*blk_per_trk - first_rec;
 		cplength += count;
 		datasize += count*sizeof(struct LO_eckd_data);
 	}
 	
 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize,
-				   startdev);
+				   startdev, blk_mq_rq_to_pdu(req));
 	if (IS_ERR(cqr))
 		return cqr;
 	ccw = cqr->cpaddr;
 	
 	if (use_prefix) {
 		if (prefix(ccw++, cqr->data, first_trk,
 			   last_trk, cmd, basedev, startdev) == -EAGAIN) {
 			
 			dasd_sfree_request(cqr, startdev);
 			return ERR_PTR(-EAGAIN);
 		}
 		idaws = (unsigned long *) (cqr->data +
 					   sizeof(struct PFX_eckd_data));
 	} else {
 		if (define_extent(ccw++, cqr->data, first_trk,
 				  last_trk, cmd, basedev, 0) == -EAGAIN) {
 			
 			dasd_sfree_request(cqr, startdev);
 			return ERR_PTR(-EAGAIN);
 		}
 		idaws = (unsigned long *) (cqr->data +
 					   sizeof(struct DE_eckd_data));
 	}
 	
 	LO_data = (struct LO_eckd_data *) (idaws + cidaw);
 	recid = first_rec;
 	if (private->uses_cdl == 0 || recid > 2*blk_per_trk) {
 		
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, LO_data++, first_trk, first_offs + 1,
 			      last_rec - recid + 1, cmd, basedev, blksize);
 	}
 	rq_for_each_segment(bv, req, iter) {
 		dst = page_address(bv.bv_page) + bv.bv_offset;
 		if (dasd_page_cache) {
 			char *copy = kmem_cache_alloc(dasd_page_cache,
 						      GFP_DMA | __GFP_NOWARN);
 			if (copy && rq_data_dir(req) == WRITE)
 				memcpy(copy + bv.bv_offset, dst, bv.bv_len);
 			if (copy)
 				dst = copy + bv.bv_offset;
 		}
 		for (off = 0; off < bv.bv_len; off += blksize) {
 			sector_t trkid = recid;
 			unsigned int recoffs = sector_div(trkid, blk_per_trk);
 			rcmd = cmd;
 			count = blksize;
 			
 			if (private->uses_cdl && recid < 2*blk_per_trk) {
 				if (dasd_eckd_cdl_special(blk_per_trk, recid)){
 					rcmd |= 0x8;
 					count = dasd_eckd_cdl_reclen(recid);
 					if (count < blksize &&
 					    rq_data_dir(req) == READ)
 						memset(dst + count, 0xe5,
 						       blksize - count);
 				}
 				ccw[-1].flags |= CCW_FLAG_CC;
 				locate_record(ccw++, LO_data++,
 					      trkid, recoffs + 1,
 					      1, rcmd, basedev, count);
 			}
 			
 			if (private->uses_cdl && recid == 2*blk_per_trk) {
 				ccw[-1].flags |= CCW_FLAG_CC;
 				locate_record(ccw++, LO_data++,
 					      trkid, recoffs + 1,
 					      last_rec - recid + 1,
 					      cmd, basedev, count);
 			}
 			
 			ccw[-1].flags |= CCW_FLAG_CC;
 			ccw->cmd_code = rcmd;
 			ccw->count = count;
 			if (idal_is_needed(dst, blksize)) {
 				ccw->cda = (__u32)(addr_t) idaws;
 				ccw->flags = CCW_FLAG_IDA;
 				idaws = idal_create_words(idaws, dst, blksize);
 			} else {
 				ccw->cda = (__u32)(addr_t) dst;
 				ccw->flags = 0;
 			}
 			ccw++;
 			dst += blksize;
 			recid++;
 		}
 	}
 	if (blk_noretry_request(req) ||
 	    block->base->features & DASD_FEATURE_FAILFAST)
 		set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->startdev = startdev;
 	cqr->memdev = startdev;
 	cqr->block = block;
 	cqr->expires = startdev->default_expires * HZ;	
 	cqr->lpm = dasd_path_get_ppm(startdev);
 	cqr->retries = startdev->default_retries;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",177,1069
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_build_check,70,70," static struct dasd_ccw_req *
 dasd_eckd_build_check(struct dasd_device *base, struct format_data_t *fdata,
 		      int enable_pav, struct eckd_count *fmt_buffer, int rpt)
 {
 	struct dasd_eckd_private *start_priv;
 	struct dasd_eckd_private *base_priv;
 	struct dasd_device *startdev = NULL;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	void *data;
 	int cplength, datasize;
 	int use_prefix;
 	int count;
 	int i;
 	if (enable_pav)
 		startdev = dasd_alias_get_start_dev(base);
 	if (!startdev)
 		startdev = base;
 	start_priv = startdev->private;
 	base_priv = base->private;
 	count = rpt * (fdata->stop_unit - fdata->start_unit + 1);
 	use_prefix = base_priv->features.feature[8] & 0x01;
 	if (use_prefix) {
 		cplength = 1;
 		datasize = sizeof(struct PFX_eckd_data);
 	} else {
 		cplength = 2;
 		datasize = sizeof(struct DE_eckd_data) +
 			sizeof(struct LO_eckd_data);
 	}
 	cplength += count;
 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize,
-				  startdev);
+				   startdev, NULL);
 	if (IS_ERR(cqr))
 		return cqr;
 	start_priv->count++;
 	data = cqr->data;
 	ccw = cqr->cpaddr;
 	if (use_prefix) {
 		prefix_LRE(ccw++, data, fdata->start_unit, fdata->stop_unit,
 			   DASD_ECKD_CCW_READ_COUNT, base, startdev, 1, 0,
 			   count, 0, 0);
 	} else {
 		define_extent(ccw++, data, fdata->start_unit, fdata->stop_unit,
 			      DASD_ECKD_CCW_READ_COUNT, startdev, 0);
 		data += sizeof(struct DE_eckd_data);
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, data, fdata->start_unit, 0, count,
 			      DASD_ECKD_CCW_READ_COUNT, base, 0);
 	}
 	for (i = 0; i < count; i++) {
 		ccw[-1].flags |= CCW_FLAG_CC;
 		ccw->cmd_code = DASD_ECKD_CCW_READ_COUNT;
 		ccw->flags = CCW_FLAG_SLI;
 		ccw->count = 8;
 		ccw->cda = (__u32)(addr_t) fmt_buffer;
 		ccw++;
 		fmt_buffer++;
 	}
 	cqr->startdev = startdev;
 	cqr->memdev = startdev;
 	cqr->basedev = base;
 	cqr->retries = DASD_RETRIES;
 	cqr->expires = startdev->default_expires * HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	
 	set_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags);
 	return cqr;
 }",71,448
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_psf_cuir_response,41,41," static int
 dasd_eckd_psf_cuir_response(struct dasd_device *device, int response,
 			    __u32 message_id, __u8 lpum)
 {
 	struct dasd_psf_cuir_response *psf_cuir;
 	int pos = pathmask_to_pos(lpum);
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	int rc;
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1  ,
-				  sizeof(struct dasd_psf_cuir_response),
-				  device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1,
+				   sizeof(struct dasd_psf_cuir_response),
+				   device, NULL);
 	if (IS_ERR(cqr)) {
 		DBF_DEV_EVENT(DBF_WARNING, device, ""%s"",
 			   ""Could not allocate PSF-CUIR request"");
 		return PTR_ERR(cqr);
 	}
 	psf_cuir = (struct dasd_psf_cuir_response *)cqr->data;
 	psf_cuir->order = PSF_ORDER_CUIR_RESPONSE;
 	psf_cuir->cc = response;
 	psf_cuir->chpid = device->path[pos].chpid;
 	psf_cuir->message_id = message_id;
 	psf_cuir->cssid = device->path[pos].cssid;
 	psf_cuir->ssid = device->path[pos].ssid;
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
 	ccw->cda = (__u32)(addr_t)psf_cuir;
 	ccw->flags = CCW_FLAG_SLI;
 	ccw->count = sizeof(struct dasd_psf_cuir_response);
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->block = NULL;
 	cqr->retries = 256;
 	cqr->expires = 10*HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
 	rc = dasd_sleep_on(cqr);
 	dasd_sfree_request(cqr, cqr->memdev);
 	return rc;
 }",44,301
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_build_check_tcw,64,65," static struct dasd_ccw_req *
 dasd_eckd_build_check_tcw(struct dasd_device *base, struct format_data_t *fdata,
 			  int enable_pav, struct eckd_count *fmt_buffer,
 			  int rpt)
 {
 	struct dasd_eckd_private *start_priv;
 	struct dasd_device *startdev = NULL;
 	struct tidaw *last_tidaw = NULL;
 	struct dasd_ccw_req *cqr;
 	struct itcw *itcw;
 	int itcw_size;
 	int count;
 	int rc;
 	int i;
 	if (enable_pav)
 		startdev = dasd_alias_get_start_dev(base);
 	if (!startdev)
 		startdev = base;
 	start_priv = startdev->private;
 	count = rpt * (fdata->stop_unit - fdata->start_unit + 1);
 	
 	itcw_size = itcw_calc_size(0, count, 0);
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 0, itcw_size, startdev);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 0, itcw_size, startdev,
+				   NULL);
 	if (IS_ERR(cqr))
 		return cqr;
 	start_priv->count++;
 	itcw = itcw_init(cqr->data, itcw_size, ITCW_OP_READ, 0, count, 0);
 	if (IS_ERR(itcw)) {
 		rc = -EINVAL;
 		goto out_err;
 	}
 	cqr->cpaddr = itcw_get_tcw(itcw);
 	rc = prepare_itcw(itcw, fdata->start_unit, fdata->stop_unit,
 			  DASD_ECKD_CCW_READ_COUNT_MT, base, startdev, 0, count,
 			  sizeof(struct eckd_count),
 			  count * sizeof(struct eckd_count), 0, rpt);
 	if (rc)
 		goto out_err;
 	for (i = 0; i < count; i++) {
 		last_tidaw = itcw_add_tidaw(itcw, 0, fmt_buffer++,
 					    sizeof(struct eckd_count));
 		if (IS_ERR(last_tidaw)) {
 			rc = -EINVAL;
 			goto out_err;
 		}
 	}
 	last_tidaw->flags |= TIDAW_FLAGS_LAST;
 	itcw_finalize(itcw);
 	cqr->cpmode = 1;
 	cqr->startdev = startdev;
 	cqr->memdev = startdev;
 	cqr->basedev = base;
 	cqr->retries = startdev->default_retries;
 	cqr->expires = startdev->default_expires * HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	
 	set_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags);
 	set_bit(DASD_CQR_SUPPRESS_IL, &cqr->flags);
 	return cqr;
 out_err:
 	dasd_sfree_request(cqr, startdev);
 	return ERR_PTR(rc);
 }",66,419
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_build_psf_ssc,34,34," static struct dasd_ccw_req *dasd_eckd_build_psf_ssc(struct dasd_device *device,
 						    int enable_pav)
 {
 	struct dasd_ccw_req *cqr;
 	struct dasd_psf_ssc_data *psf_ssc_data;
 	struct ccw1 *ccw;
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1  ,
-				  sizeof(struct dasd_psf_ssc_data),
-				  device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1,
+				   sizeof(struct dasd_psf_ssc_data), device,
+				   NULL);
 	if (IS_ERR(cqr)) {
 		DBF_DEV_EVENT(DBF_WARNING, device, ""%s"",
 			   ""Could not allocate PSF-SSC request"");
 		return cqr;
 	}
 	psf_ssc_data = (struct dasd_psf_ssc_data *)cqr->data;
 	psf_ssc_data->order = PSF_ORDER_SSC;
 	psf_ssc_data->suborder = 0xc0;
 	if (enable_pav) {
 		psf_ssc_data->suborder |= 0x08;
 		psf_ssc_data->reserved[0] = 0x88;
 	}
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
 	ccw->cda = (__u32)(addr_t)psf_ssc_data;
 	ccw->count = 66;
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->block = NULL;
 	cqr->retries = 256;
 	cqr->expires = 10*HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",37,223
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_build_cp_tpm_track,144,145," static struct dasd_ccw_req *dasd_eckd_build_cp_tpm_track(
 					       struct dasd_device *startdev,
 					       struct dasd_block *block,
 					       struct request *req,
 					       sector_t first_rec,
 					       sector_t last_rec,
 					       sector_t first_trk,
 					       sector_t last_trk,
 					       unsigned int first_offs,
 					       unsigned int last_offs,
 					       unsigned int blk_per_trk,
 					       unsigned int blksize)
 {
 	struct dasd_ccw_req *cqr;
 	struct req_iterator iter;
 	struct bio_vec bv;
 	char *dst;
 	unsigned int trkcount, ctidaw;
 	unsigned char cmd;
 	struct dasd_device *basedev;
 	unsigned int tlf;
 	struct itcw *itcw;
 	struct tidaw *last_tidaw = NULL;
 	int itcw_op;
 	size_t itcw_size;
 	u8 tidaw_flags;
 	unsigned int seg_len, part_len, len_to_track_end;
 	unsigned char new_track;
 	sector_t recid, trkid;
 	unsigned int offs;
 	unsigned int count, count_to_trk_end;
 	int ret;
 	basedev = block->base;
 	if (rq_data_dir(req) == READ) {
 		cmd = DASD_ECKD_CCW_READ_TRACK_DATA;
 		itcw_op = ITCW_OP_READ;
 	} else if (rq_data_dir(req) == WRITE) {
 		cmd = DASD_ECKD_CCW_WRITE_TRACK_DATA;
 		itcw_op = ITCW_OP_WRITE;
 	} else
 		return ERR_PTR(-EINVAL);
 	
 	trkcount = last_trk - first_trk + 1;
 	ctidaw = 0;
 	rq_for_each_segment(bv, req, iter) {
 		++ctidaw;
 	}
 	if (rq_data_dir(req) == WRITE)
 		ctidaw += (last_trk - first_trk);
 	
 	itcw_size = itcw_calc_size(0, ctidaw, 0);
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 0, itcw_size, startdev);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 0, itcw_size, startdev,
+				   blk_mq_rq_to_pdu(req));
 	if (IS_ERR(cqr))
 		return cqr;
 	
 	if (first_trk == last_trk)
 		tlf = last_offs - first_offs + 1;
 	else
 		tlf = last_offs + 1;
 	tlf *= blksize;
 	itcw = itcw_init(cqr->data, itcw_size, itcw_op, 0, ctidaw, 0);
 	if (IS_ERR(itcw)) {
 		ret = -EINVAL;
 		goto out_error;
 	}
 	cqr->cpaddr = itcw_get_tcw(itcw);
 	if (prepare_itcw(itcw, first_trk, last_trk,
 			 cmd, basedev, startdev,
 			 first_offs + 1,
 			 trkcount, blksize,
 			 (last_rec - first_rec + 1) * blksize,
 			 tlf, blk_per_trk) == -EAGAIN) {
 		
 		ret = -EAGAIN;
 		goto out_error;
 	}
 	len_to_track_end = 0;
 	
 	if (rq_data_dir(req) == WRITE) {
 		new_track = 1;
 		recid = first_rec;
 		rq_for_each_segment(bv, req, iter) {
 			dst = page_address(bv.bv_page) + bv.bv_offset;
 			seg_len = bv.bv_len;
 			while (seg_len) {
 				if (new_track) {
 					trkid = recid;
 					offs = sector_div(trkid, blk_per_trk);
 					count_to_trk_end = blk_per_trk - offs;
 					count = min((last_rec - recid + 1),
 						    (sector_t)count_to_trk_end);
 					len_to_track_end = count * blksize;
 					recid += count;
 					new_track = 0;
 				}
 				part_len = min(seg_len, len_to_track_end);
 				seg_len -= part_len;
 				len_to_track_end -= part_len;
 				
 				if (!len_to_track_end) {
 					new_track = 1;
 					tidaw_flags = TIDAW_FLAGS_INSERT_CBC;
 				} else
 					tidaw_flags = 0;
 				last_tidaw = itcw_add_tidaw(itcw, tidaw_flags,
 							    dst, part_len);
 				if (IS_ERR(last_tidaw)) {
 					ret = -EINVAL;
 					goto out_error;
 				}
 				dst += part_len;
 			}
 		}
 	} else {
 		rq_for_each_segment(bv, req, iter) {
 			dst = page_address(bv.bv_page) + bv.bv_offset;
 			last_tidaw = itcw_add_tidaw(itcw, 0x00,
 						    dst, bv.bv_len);
 			if (IS_ERR(last_tidaw)) {
 				ret = -EINVAL;
 				goto out_error;
 			}
 		}
 	}
 	last_tidaw->flags |= TIDAW_FLAGS_LAST;
 	last_tidaw->flags &= ~TIDAW_FLAGS_INSERT_CBC;
 	itcw_finalize(itcw);
 	if (blk_noretry_request(req) ||
 	    block->base->features & DASD_FEATURE_FAILFAST)
 		set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->cpmode = 1;
 	cqr->startdev = startdev;
 	cqr->memdev = startdev;
 	cqr->block = block;
 	cqr->expires = startdev->default_expires * HZ;	
 	cqr->lpm = dasd_path_get_ppm(startdev);
 	cqr->retries = startdev->default_retries;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 out_error:
 	dasd_sfree_request(cqr, startdev);
 	return ERR_PTR(ret);
 }",146,783
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_dasd_generic_build_rdc,36,36," static struct dasd_ccw_req *dasd_generic_build_rdc(struct dasd_device *device,
 						   void *rdc_buffer,
 						   int rdc_buffer_size,
 						   int magic)
 {
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	unsigned long *idaw;
-	cqr = dasd_smalloc_request(magic, 1 , rdc_buffer_size, device);
+	cqr = dasd_smalloc_request(magic, 1, rdc_buffer_size, device, NULL);
 	if (IS_ERR(cqr)) {
 		
 		dev_err(&device->cdev->dev,
 			 ""An error occurred in the DASD device driver, ""
 			 ""reason=%s\n"", ""13"");
 		return cqr;
 	}
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = CCW_CMD_RDC;
 	if (idal_is_needed(rdc_buffer, rdc_buffer_size)) {
 		idaw = (unsigned long *) (cqr->data);
 		ccw->cda = (__u32)(addr_t) idaw;
 		ccw->flags = CCW_FLAG_IDA;
 		idaw = idal_create_words(idaw, rdc_buffer, rdc_buffer_size);
 	} else {
 		ccw->cda = (__u32)(addr_t) rdc_buffer;
 		ccw->flags = 0;
 	}
 	ccw->count = rdc_buffer_size;
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->expires = 10*HZ;
 	cqr->retries = 256;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",37,237
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_release,44,44," static int
 dasd_eckd_release(struct dasd_device *device)
 {
 	struct dasd_ccw_req *cqr;
 	int rc;
 	struct ccw1 *ccw;
 	int useglobal;
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 	useglobal = 0;
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1, 32, device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1, 32, device, NULL);
 	if (IS_ERR(cqr)) {
 		mutex_lock(&dasd_reserve_mutex);
 		useglobal = 1;
 		cqr = &dasd_reserve_req->cqr;
 		memset(cqr, 0, sizeof(*cqr));
 		memset(&dasd_reserve_req->ccw, 0,
 		       sizeof(dasd_reserve_req->ccw));
 		cqr->cpaddr = &dasd_reserve_req->ccw;
 		cqr->data = &dasd_reserve_req->data;
 		cqr->magic = DASD_ECKD_MAGIC;
 	}
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_RELEASE;
 	ccw->flags |= CCW_FLAG_SLI;
 	ccw->count = 32;
 	ccw->cda = (__u32)(addr_t) cqr->data;
 	cqr->startdev = device;
 	cqr->memdev = device;
 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
 	set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->retries = 2;	
 	cqr->expires = 2 * HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	rc = dasd_sleep_on_immediatly(cqr);
 	if (!rc)
 		clear_bit(DASD_FLAG_IS_RESERVED, &device->flags);
 	if (useglobal)
 		mutex_unlock(&dasd_reserve_mutex);
 	else
 		dasd_sfree_request(cqr, cqr->memdev);
 	return rc;
 }",45,296
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_build_format,237,237," static struct dasd_ccw_req *
 dasd_eckd_build_format(struct dasd_device *base,
 		       struct format_data_t *fdata,
 		       int enable_pav)
 {
 	struct dasd_eckd_private *base_priv;
 	struct dasd_eckd_private *start_priv;
 	struct dasd_device *startdev = NULL;
 	struct dasd_ccw_req *fcp;
 	struct eckd_count *ect;
 	struct ch_t address;
 	struct ccw1 *ccw;
 	void *data;
 	int rpt;
 	int cplength, datasize;
 	int i, j;
 	int intensity = 0;
 	int r0_perm;
 	int nr_tracks;
 	int use_prefix;
 	if (enable_pav)
 		startdev = dasd_alias_get_start_dev(base);
 	if (!startdev)
 		startdev = base;
 	start_priv = startdev->private;
 	base_priv = base->private;
 	rpt = recs_per_track(&base_priv->rdc_data, 0, fdata->blksize);
 	nr_tracks = fdata->stop_unit - fdata->start_unit + 1;
 	
 	if (fdata->intensity & 0x10) {
 		r0_perm = 0;
 		intensity = fdata->intensity & ~0x10;
 	} else {
 		r0_perm = 1;
 		intensity = fdata->intensity;
 	}
 	use_prefix = base_priv->features.feature[8] & 0x01;
 	switch (intensity) {
 	case 0x00:	
 	case 0x08:	
 		cplength = 2 + (rpt*nr_tracks);
 		if (use_prefix)
 			datasize = sizeof(struct PFX_eckd_data) +
 				sizeof(struct LO_eckd_data) +
 				rpt * nr_tracks * sizeof(struct eckd_count);
 		else
 			datasize = sizeof(struct DE_eckd_data) +
 				sizeof(struct LO_eckd_data) +
 				rpt * nr_tracks * sizeof(struct eckd_count);
 		break;
 	case 0x01:	
 	case 0x09:	
 		cplength = 2 + rpt * nr_tracks;
 		if (use_prefix)
 			datasize = sizeof(struct PFX_eckd_data) +
 				sizeof(struct LO_eckd_data) +
 				sizeof(struct eckd_count) +
 				rpt * nr_tracks * sizeof(struct eckd_count);
 		else
 			datasize = sizeof(struct DE_eckd_data) +
 				sizeof(struct LO_eckd_data) +
 				sizeof(struct eckd_count) +
 				rpt * nr_tracks * sizeof(struct eckd_count);
 		break;
 	case 0x04:	
 	case 0x0c:	
 		cplength = 3;
 		if (use_prefix)
 			datasize = sizeof(struct PFX_eckd_data) +
 				sizeof(struct LO_eckd_data) +
 				sizeof(struct eckd_count);
 		else
 			datasize = sizeof(struct DE_eckd_data) +
 				sizeof(struct LO_eckd_data) +
 				sizeof(struct eckd_count);
 		break;
 	default:
 		dev_warn(&startdev->cdev->dev,
 			 ""An I/O control call used incorrect flags 0x%x\n"",
 			 fdata->intensity);
 		return ERR_PTR(-EINVAL);
 	}
 	
-	fcp = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength,
-				   datasize, startdev);
+	fcp = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize,
+				   startdev, NULL);
 	if (IS_ERR(fcp))
 		return fcp;
 	start_priv->count++;
 	data = fcp->data;
 	ccw = fcp->cpaddr;
 	switch (intensity & ~0x08) {
 	case 0x00: 
 		if (use_prefix) {
 			prefix(ccw++, (struct PFX_eckd_data *) data,
 			       fdata->start_unit, fdata->stop_unit,
 			       DASD_ECKD_CCW_WRITE_CKD, base, startdev);
 			
 			if (r0_perm)
 				((struct PFX_eckd_data *)data)
 					->define_extent.ga_extended |= 0x04;
 			data += sizeof(struct PFX_eckd_data);
 		} else {
 			define_extent(ccw++, (struct DE_eckd_data *) data,
 				      fdata->start_unit, fdata->stop_unit,
 				      DASD_ECKD_CCW_WRITE_CKD, startdev, 0);
 			
 			if (r0_perm)
 				((struct DE_eckd_data *) data)
 					->ga_extended |= 0x04;
 			data += sizeof(struct DE_eckd_data);
 		}
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, (struct LO_eckd_data *) data,
 			      fdata->start_unit, 0, rpt*nr_tracks,
 			      DASD_ECKD_CCW_WRITE_CKD, base,
 			      fdata->blksize);
 		data += sizeof(struct LO_eckd_data);
 		break;
 	case 0x01: 
 		if (use_prefix) {
 			prefix(ccw++, (struct PFX_eckd_data *) data,
 			       fdata->start_unit, fdata->stop_unit,
 			       DASD_ECKD_CCW_WRITE_RECORD_ZERO,
 			       base, startdev);
 			data += sizeof(struct PFX_eckd_data);
 		} else {
 			define_extent(ccw++, (struct DE_eckd_data *) data,
 			       fdata->start_unit, fdata->stop_unit,
 			       DASD_ECKD_CCW_WRITE_RECORD_ZERO, startdev, 0);
 			data += sizeof(struct DE_eckd_data);
 		}
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, (struct LO_eckd_data *) data,
 			      fdata->start_unit, 0, rpt * nr_tracks + 1,
 			      DASD_ECKD_CCW_WRITE_RECORD_ZERO, base,
 			      base->block->bp_block);
 		data += sizeof(struct LO_eckd_data);
 		break;
 	case 0x04: 
 		if (use_prefix) {
 			prefix(ccw++, (struct PFX_eckd_data *) data,
 			       fdata->start_unit, fdata->stop_unit,
 			       DASD_ECKD_CCW_WRITE_CKD, base, startdev);
 			data += sizeof(struct PFX_eckd_data);
 		} else {
 			define_extent(ccw++, (struct DE_eckd_data *) data,
 			       fdata->start_unit, fdata->stop_unit,
 			       DASD_ECKD_CCW_WRITE_CKD, startdev, 0);
 			data += sizeof(struct DE_eckd_data);
 		}
 		ccw[-1].flags |= CCW_FLAG_CC;
 		locate_record(ccw++, (struct LO_eckd_data *) data,
 			      fdata->start_unit, 0, 1,
 			      DASD_ECKD_CCW_WRITE_CKD, base, 8);
 		data += sizeof(struct LO_eckd_data);
 		break;
 	}
 	for (j = 0; j < nr_tracks; j++) {
 		
 		set_ch_t(&address,
 			 (fdata->start_unit + j) /
 			 base_priv->rdc_data.trk_per_cyl,
 			 (fdata->start_unit + j) %
 			 base_priv->rdc_data.trk_per_cyl);
 		if (intensity & 0x01) {	
 			ect = (struct eckd_count *) data;
 			data += sizeof(struct eckd_count);
 			ect->cyl = address.cyl;
 			ect->head = address.head;
 			ect->record = 0;
 			ect->kl = 0;
 			ect->dl = 8;
 			ccw[-1].flags |= CCW_FLAG_CC;
 			ccw->cmd_code = DASD_ECKD_CCW_WRITE_RECORD_ZERO;
 			ccw->flags = CCW_FLAG_SLI;
 			ccw->count = 8;
 			ccw->cda = (__u32)(addr_t) ect;
 			ccw++;
 		}
 		if ((intensity & ~0x08) & 0x04) {	
 			ect = (struct eckd_count *) data;
 			data += sizeof(struct eckd_count);
 			ect->cyl = address.cyl;
 			ect->head = address.head;
 			ect->record = 1;
 			ect->kl = 0;
 			ect->dl = 0;
 			ccw[-1].flags |= CCW_FLAG_CC;
 			ccw->cmd_code = DASD_ECKD_CCW_WRITE_CKD;
 			ccw->flags = CCW_FLAG_SLI;
 			ccw->count = 8;
 			ccw->cda = (__u32)(addr_t) ect;
 		} else {		
 			for (i = 0; i < rpt; i++) {
 				ect = (struct eckd_count *) data;
 				data += sizeof(struct eckd_count);
 				ect->cyl = address.cyl;
 				ect->head = address.head;
 				ect->record = i + 1;
 				ect->kl = 0;
 				ect->dl = fdata->blksize;
 				
 				if ((intensity & 0x08) &&
 				    address.cyl == 0 && address.head == 0) {
 					if (i < 3) {
 						ect->kl = 4;
 						ect->dl = sizes_trk0[i] - 4;
 					}
 				}
 				if ((intensity & 0x08) &&
 				    address.cyl == 0 && address.head == 1) {
 					ect->kl = 44;
 					ect->dl = LABEL_SIZE - 44;
 				}
 				ccw[-1].flags |= CCW_FLAG_CC;
 				if (i != 0 || j == 0)
 					ccw->cmd_code =
 						DASD_ECKD_CCW_WRITE_CKD;
 				else
 					ccw->cmd_code =
 						DASD_ECKD_CCW_WRITE_CKD_MT;
 				ccw->flags = CCW_FLAG_SLI;
 				ccw->count = 8;
 				ccw->cda = (__u32)(addr_t) ect;
 				ccw++;
 			}
 		}
 	}
 	fcp->startdev = startdev;
 	fcp->memdev = startdev;
 	fcp->basedev = base;
 	fcp->retries = 256;
 	fcp->expires = startdev->default_expires * HZ;
 	fcp->buildclk = get_tod_clock();
 	fcp->status = DASD_CQR_FILLED;
 	return fcp;
 }",239,1459
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_performance,53,52," static int
 dasd_eckd_performance(struct dasd_device *device, void __user *argp)
 {
 	struct dasd_psf_prssd_data *prssdp;
 	struct dasd_rssd_perf_stats_t *stats;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	int rc;
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1   + 1 ,
-				   (sizeof(struct dasd_psf_prssd_data) +
-				    sizeof(struct dasd_rssd_perf_stats_t)),
-				   device);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 + 1,
+				   (sizeof(struct dasd_psf_prssd_data) + sizeof(struct dasd_rssd_perf_stats_t)),
+				   device, NULL);
 	if (IS_ERR(cqr)) {
 		DBF_DEV_EVENT(DBF_WARNING, device, ""%s"",
 			    ""Could not allocate initialization request"");
 		return PTR_ERR(cqr);
 	}
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->retries = 0;
 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
 	cqr->expires = 10 * HZ;
 	
 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
 	prssdp->order = PSF_ORDER_PRSSD;
 	prssdp->suborder = 0x01;	
 	prssdp->varies[1] = 0x01;	
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
 	ccw->count = sizeof(struct dasd_psf_prssd_data);
 	ccw->flags |= CCW_FLAG_CC;
 	ccw->cda = (__u32)(addr_t) prssdp;
 	
 	stats = (struct dasd_rssd_perf_stats_t *) (prssdp + 1);
 	memset(stats, 0, sizeof(struct dasd_rssd_perf_stats_t));
 	ccw++;
 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
 	ccw->count = sizeof(struct dasd_rssd_perf_stats_t);
 	ccw->cda = (__u32)(addr_t) stats;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	rc = dasd_sleep_on(cqr);
 	if (rc == 0) {
 		prssdp = (struct dasd_psf_prssd_data *) cqr->data;
 		stats = (struct dasd_rssd_perf_stats_t *) (prssdp + 1);
 		if (copy_to_user(argp, stats,
 				 sizeof(struct dasd_rssd_perf_stats_t)))
 			rc = -EFAULT;
 	}
 	dasd_sfree_request(cqr, cqr->memdev);
 	return rc;
 }",56,395
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_snid,56,57," static int dasd_eckd_snid(struct dasd_device *device,
 			  void __user *argp)
 {
 	struct dasd_ccw_req *cqr;
 	int rc;
 	struct ccw1 *ccw;
 	int useglobal;
 	struct dasd_snid_ioctl_data usrparm;
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 	if (copy_from_user(&usrparm, argp, sizeof(usrparm)))
 		return -EFAULT;
 	useglobal = 0;
 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1,
-				   sizeof(struct dasd_snid_data), device);
+				   sizeof(struct dasd_snid_data), device,
+				   NULL);
 	if (IS_ERR(cqr)) {
 		mutex_lock(&dasd_reserve_mutex);
 		useglobal = 1;
 		cqr = &dasd_reserve_req->cqr;
 		memset(cqr, 0, sizeof(*cqr));
 		memset(&dasd_reserve_req->ccw, 0,
 		       sizeof(dasd_reserve_req->ccw));
 		cqr->cpaddr = &dasd_reserve_req->ccw;
 		cqr->data = &dasd_reserve_req->data;
 		cqr->magic = DASD_ECKD_MAGIC;
 	}
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_SNID;
 	ccw->flags |= CCW_FLAG_SLI;
 	ccw->count = 12;
 	ccw->cda = (__u32)(addr_t) cqr->data;
 	cqr->startdev = device;
 	cqr->memdev = device;
 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
 	set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	set_bit(DASD_CQR_ALLOW_SLOCK, &cqr->flags);
 	cqr->retries = 5;
 	cqr->expires = 10 * HZ;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	cqr->lpm = usrparm.path_mask;
 	rc = dasd_sleep_on_immediatly(cqr);
 	
 	if (!rc && usrparm.path_mask && (cqr->lpm != usrparm.path_mask))
 		rc = -EIO;
 	if (!rc) {
 		usrparm.data = *((struct dasd_snid_data *)cqr->data);
 		if (copy_to_user(argp, &usrparm, sizeof(usrparm)))
 			rc = -EFAULT;
 	}
 	if (useglobal)
 		mutex_unlock(&dasd_reserve_mutex);
 	else
 		dasd_sfree_request(cqr, cqr->memdev);
 	return rc;
 }",58,395
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_build_cp_raw,130,130," static struct dasd_ccw_req *dasd_eckd_build_cp_raw(struct dasd_device *startdev,
 						   struct dasd_block *block,
 						   struct request *req)
 {
 	sector_t start_padding_sectors, end_sector_offset, end_padding_sectors;
 	unsigned int seg_len, len_to_track_end;
 	unsigned int cidaw, cplength, datasize;
 	sector_t first_trk, last_trk, sectors;
 	struct dasd_eckd_private *base_priv;
 	struct dasd_device *basedev;
 	struct req_iterator iter;
 	struct dasd_ccw_req *cqr;
 	unsigned int first_offs;
 	unsigned int trkcount;
 	unsigned long *idaws;
 	unsigned int size;
 	unsigned char cmd;
 	struct bio_vec bv;
 	struct ccw1 *ccw;
 	int use_prefix;
 	void *data;
 	char *dst;
 	
 	start_padding_sectors = blk_rq_pos(req) % DASD_RAW_SECTORS_PER_TRACK;
 	end_sector_offset = (blk_rq_pos(req) + blk_rq_sectors(req)) %
 		DASD_RAW_SECTORS_PER_TRACK;
 	end_padding_sectors = (DASD_RAW_SECTORS_PER_TRACK - end_sector_offset) %
 		DASD_RAW_SECTORS_PER_TRACK;
 	basedev = block->base;
 	if ((start_padding_sectors || end_padding_sectors) &&
 	    (rq_data_dir(req) == WRITE)) {
 		DBF_DEV_EVENT(DBF_ERR, basedev,
 			      ""raw write not track aligned (%lu,%lu) req %p"",
 			      start_padding_sectors, end_padding_sectors, req);
 		return ERR_PTR(-EINVAL);
 	}
 	first_trk = blk_rq_pos(req) / DASD_RAW_SECTORS_PER_TRACK;
 	last_trk = (blk_rq_pos(req) + blk_rq_sectors(req) - 1) /
 		DASD_RAW_SECTORS_PER_TRACK;
 	trkcount = last_trk - first_trk + 1;
 	first_offs = 0;
 	if (rq_data_dir(req) == READ)
 		cmd = DASD_ECKD_CCW_READ_TRACK;
 	else if (rq_data_dir(req) == WRITE)
 		cmd = DASD_ECKD_CCW_WRITE_FULL_TRACK;
 	else
 		return ERR_PTR(-EINVAL);
 	
 	cidaw = trkcount * DASD_RAW_BLOCK_PER_TRACK;
 	
 	base_priv = basedev->private;
 	use_prefix = base_priv->features.feature[8] & 0x01;
 	if (use_prefix) {
 		cplength = 1 + trkcount;
 		size = sizeof(struct PFX_eckd_data) + 2;
 	} else {
 		cplength = 2 + trkcount;
 		size = sizeof(struct DE_eckd_data) +
 			sizeof(struct LRE_eckd_data) + 2;
 	}
 	size = ALIGN(size, 8);
 	datasize = size + cidaw * sizeof(unsigned long);
 	
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength,
-				   datasize, startdev);
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize,
+				   startdev, blk_mq_rq_to_pdu(req));
 	if (IS_ERR(cqr))
 		return cqr;
 	ccw = cqr->cpaddr;
 	data = cqr->data;
 	if (use_prefix) {
 		prefix_LRE(ccw++, data, first_trk, last_trk, cmd, basedev,
 			   startdev, 1, first_offs + 1, trkcount, 0, 0);
 	} else {
 		define_extent(ccw++, data, first_trk, last_trk, cmd, basedev, 0);
 		ccw[-1].flags |= CCW_FLAG_CC;
 		data += sizeof(struct DE_eckd_data);
 		locate_record_ext(ccw++, data, first_trk, first_offs + 1,
 				  trkcount, cmd, basedev, 0, 0);
 	}
 	idaws = (unsigned long *)(cqr->data + size);
 	len_to_track_end = 0;
 	if (start_padding_sectors) {
 		ccw[-1].flags |= CCW_FLAG_CC;
 		ccw->cmd_code = cmd;
 		
 		ccw->count = 57326;
 		
 		len_to_track_end = 65536 - start_padding_sectors * 512;
 		ccw->cda = (__u32)(addr_t)idaws;
 		ccw->flags |= CCW_FLAG_IDA;
 		ccw->flags |= CCW_FLAG_SLI;
 		ccw++;
 		for (sectors = 0; sectors < start_padding_sectors; sectors += 8)
 			idaws = idal_create_words(idaws, rawpadpage, PAGE_SIZE);
 	}
 	rq_for_each_segment(bv, req, iter) {
 		dst = page_address(bv.bv_page) + bv.bv_offset;
 		seg_len = bv.bv_len;
 		if (cmd == DASD_ECKD_CCW_READ_TRACK)
 			memset(dst, 0, seg_len);
 		if (!len_to_track_end) {
 			ccw[-1].flags |= CCW_FLAG_CC;
 			ccw->cmd_code = cmd;
 			
 			ccw->count = 57326;
 			
 			len_to_track_end = 65536;
 			ccw->cda = (__u32)(addr_t)idaws;
 			ccw->flags |= CCW_FLAG_IDA;
 			ccw->flags |= CCW_FLAG_SLI;
 			ccw++;
 		}
 		len_to_track_end -= seg_len;
 		idaws = idal_create_words(idaws, dst, seg_len);
 	}
 	for (sectors = 0; sectors < end_padding_sectors; sectors += 8)
 		idaws = idal_create_words(idaws, rawpadpage, PAGE_SIZE);
 	if (blk_noretry_request(req) ||
 	    block->base->features & DASD_FEATURE_FAILFAST)
 		set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->startdev = startdev;
 	cqr->memdev = startdev;
 	cqr->block = block;
 	cqr->expires = startdev->default_expires * HZ;
 	cqr->lpm = dasd_path_get_ppm(startdev);
 	cqr->retries = startdev->default_retries;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",132,854
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_query_host_access,73,73," static int dasd_eckd_query_host_access(struct dasd_device *device,
 				       struct dasd_psf_query_host_access *data)
 {
 	struct dasd_eckd_private *private = device->private;
 	struct dasd_psf_query_host_access *host_access;
 	struct dasd_psf_prssd_data *prssdp;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	int rc;
 	
 	if (!device->block && private->lcu->pav == HYPER_PAV)
 		return -EOPNOTSUPP;
 	
 	if (!(private->features.feature[14] & 0x80))
 		return -EOPNOTSUPP;
-	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 	+ 1 ,
+	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 + 1,
 				   sizeof(struct dasd_psf_prssd_data) + 1,
-				   device);
+				   device, NULL);
 	if (IS_ERR(cqr)) {
 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, ""%s"",
 				""Could not allocate read message buffer request"");
 		return PTR_ERR(cqr);
 	}
 	host_access = kzalloc(sizeof(*host_access), GFP_KERNEL | GFP_DMA);
 	if (!host_access) {
 		dasd_sfree_request(cqr, device);
 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, ""%s"",
 				""Could not allocate host_access buffer"");
 		return -ENOMEM;
 	}
 	cqr->startdev = device;
 	cqr->memdev = device;
 	cqr->block = NULL;
 	cqr->retries = 256;
 	cqr->expires = 10 * HZ;
 	
 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
 	prssdp->order = PSF_ORDER_PRSSD;
 	prssdp->suborder = PSF_SUBORDER_QHA;	
 	
 	prssdp->lss = private->ned->ID;
 	prssdp->volume = private->ned->unit_addr;
 	
 	ccw = cqr->cpaddr;
 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
 	ccw->count = sizeof(struct dasd_psf_prssd_data);
 	ccw->flags |= CCW_FLAG_CC;
 	ccw->flags |= CCW_FLAG_SLI;
 	ccw->cda = (__u32)(addr_t) prssdp;
 	
 	ccw++;
 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
 	ccw->count = sizeof(struct dasd_psf_query_host_access);
 	ccw->flags |= CCW_FLAG_SLI;
 	ccw->cda = (__u32)(addr_t) host_access;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	
 	__set_bit(DASD_CQR_SUPPRESS_CR, &cqr->flags);
 	rc = dasd_sleep_on_interruptible(cqr);
 	if (rc == 0) {
 		*data = *host_access;
 	} else {
 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
 				""Reading host access data failed with rc=%d\n"",
 				rc);
 		rc = -EOPNOTSUPP;
 	}
 	dasd_sfree_request(cqr, cqr->memdev);
 	kfree(host_access);
 	return rc;
 }",75,466
dasd_smalloc,/media/hdd2/yusuf/SMPL-How-Far-Are-We/emse_submission_fix/k_3/test/dasd_smalloc/1528132059_2018-06-04_c5205f2ff2be_dasd_eckd_dasd_eckd_build_cp_cmd_track,141,141," static struct dasd_ccw_req *dasd_eckd_build_cp_cmd_track(
 					       struct dasd_device *startdev,
 					       struct dasd_block *block,
 					       struct request *req,
 					       sector_t first_rec,
 					       sector_t last_rec,
 					       sector_t first_trk,
 					       sector_t last_trk,
 					       unsigned int first_offs,
 					       unsigned int last_offs,
 					       unsigned int blk_per_trk,
 					       unsigned int blksize)
 {
 	unsigned long *idaws;
 	struct dasd_ccw_req *cqr;
 	struct ccw1 *ccw;
 	struct req_iterator iter;
 	struct bio_vec bv;
 	char *dst, *idaw_dst;
 	unsigned int cidaw, cplength, datasize;
 	unsigned int tlf;
 	sector_t recid;
 	unsigned char cmd;
 	struct dasd_device *basedev;
 	unsigned int trkcount, count, count_to_trk_end;
 	unsigned int idaw_len, seg_len, part_len, len_to_track_end;
 	unsigned char new_track, end_idaw;
 	sector_t trkid;
 	unsigned int recoffs;
 	basedev = block->base;
 	if (rq_data_dir(req) == READ)
 		cmd = DASD_ECKD_CCW_READ_TRACK_DATA;
 	else if (rq_data_dir(req) == WRITE)
 		cmd = DASD_ECKD_CCW_WRITE_TRACK_DATA;
 	else
 		return ERR_PTR(-EINVAL);
 	
 	cidaw = last_rec - first_rec + 1;
 	trkcount = last_trk - first_trk + 1;
 	
 	cplength = 1 + trkcount;
 	datasize = sizeof(struct PFX_eckd_data) + cidaw * sizeof(unsigned long);
 	
 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize,
-				   startdev);
+				   startdev, blk_mq_rq_to_pdu(req));
 	if (IS_ERR(cqr))
 		return cqr;
 	ccw = cqr->cpaddr;
 	
 	if (first_trk == last_trk)
 		tlf = last_offs - first_offs + 1;
 	else
 		tlf = last_offs + 1;
 	tlf *= blksize;
 	if (prefix_LRE(ccw++, cqr->data, first_trk,
 		       last_trk, cmd, basedev, startdev,
 		       1 , first_offs + 1,
 		       trkcount, blksize,
 		       tlf) == -EAGAIN) {
 		
 		dasd_sfree_request(cqr, startdev);
 		return ERR_PTR(-EAGAIN);
 	}
 	
 	idaws = (unsigned long *) (cqr->data + sizeof(struct PFX_eckd_data));
 	recid = first_rec;
 	new_track = 1;
 	end_idaw = 0;
 	len_to_track_end = 0;
 	idaw_dst = NULL;
 	idaw_len = 0;
 	rq_for_each_segment(bv, req, iter) {
 		dst = page_address(bv.bv_page) + bv.bv_offset;
 		seg_len = bv.bv_len;
 		while (seg_len) {
 			if (new_track) {
 				trkid = recid;
 				recoffs = sector_div(trkid, blk_per_trk);
 				count_to_trk_end = blk_per_trk - recoffs;
 				count = min((last_rec - recid + 1),
 					    (sector_t)count_to_trk_end);
 				len_to_track_end = count * blksize;
 				ccw[-1].flags |= CCW_FLAG_CC;
 				ccw->cmd_code = cmd;
 				ccw->count = len_to_track_end;
 				ccw->cda = (__u32)(addr_t)idaws;
 				ccw->flags = CCW_FLAG_IDA;
 				ccw++;
 				recid += count;
 				new_track = 0;
 				
 				if (!idaw_dst)
 					idaw_dst = dst;
 			}
 			
 			if (!idaw_dst) {
 				if (__pa(dst) & (IDA_BLOCK_SIZE-1)) {
 					dasd_sfree_request(cqr, startdev);
 					return ERR_PTR(-ERANGE);
 				} else
 					idaw_dst = dst;
 			}
 			if ((idaw_dst + idaw_len) != dst) {
 				dasd_sfree_request(cqr, startdev);
 				return ERR_PTR(-ERANGE);
 			}
 			part_len = min(seg_len, len_to_track_end);
 			seg_len -= part_len;
 			dst += part_len;
 			idaw_len += part_len;
 			len_to_track_end -= part_len;
 			
 			if (!(__pa(idaw_dst + idaw_len) & (IDA_BLOCK_SIZE-1)))
 				end_idaw = 1;
 			
 			if (!len_to_track_end) {
 				new_track = 1;
 				end_idaw = 1;
 			}
 			if (end_idaw) {
 				idaws = idal_create_words(idaws, idaw_dst,
 							  idaw_len);
 				idaw_dst = NULL;
 				idaw_len = 0;
 				end_idaw = 0;
 			}
 		}
 	}
 	if (blk_noretry_request(req) ||
 	    block->base->features & DASD_FEATURE_FAILFAST)
 		set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags);
 	cqr->startdev = startdev;
 	cqr->memdev = startdev;
 	cqr->block = block;
 	cqr->expires = startdev->default_expires * HZ;	
 	cqr->lpm = dasd_path_get_ppm(startdev);
 	cqr->retries = startdev->default_retries;
 	cqr->buildclk = get_tod_clock();
 	cqr->status = DASD_CQR_FILLED;
 	return cqr;
 }",142,764
